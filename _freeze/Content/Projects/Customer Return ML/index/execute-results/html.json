{
  "hash": "2288d1a961fdffefae0ede1f01c467fd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Customer Returns\"\nauthor: Brian Cervantes Alvarez\ndescription: \"Leveraging logistic regression and random forest to advance the prediction of retail item returns with increased precision.\"\nbibliography: \"bibliography.bib\"\nnocite: |\n     @*\ndate: \"3-11-2023\"\nimage: /Assets/Images/return.jpeg\nformat:\n  html:\n    toc: true\n    toc-location: right\n    html-math-method: katex\n    page-layout: article\nexecute: \n  warning: false\n  message: false\ncategories: [R, Machine Learning, Random Forest]\n---\n\n\n\n\n![](/Assets/Images/return.jpeg)\n\n# Project Summary\n\n## Objective\nThis project aimed to predict customer return behavior using a dataset with various product and customer attributes. By employing logistic regression and Random Forest models, we sought to determine the likelihood of a customer returning a purchased item.\n\n## Methodology\nOur analysis began with loading essential libraries and datasets, followed by an exploratory data analysis (EDA) to understand the distribution of returns across different attributes like product department, size, and customer state. Key features such as season, customer age, manufacturer's suggested retail price (MSRP), and price range were engineered to enrich the dataset.\n\nWe initially applied logistic regression for its suitability for binary outcomes. However, to improve our model's predictive accuracy, we transitioned to a Random Forest approach. This model yielded an AUC score of 0.625, indicating moderate predictive capability and highlighting areas for potential improvement through further feature engineering or model tuning.\n\n### Key Steps and Code Highlights\n\n1. **Preparation and EDA:**\n   - Loaded necessary R packages: `tidyverse`, `lubridate`, `caret`, and `glmnet`.\n   - Imported and glimpsed at the training and testing data.\n   - Conducted EDA to analyze returns by product department, size, and customer state.\n\n2. **Feature Engineering:**\n   - Developed a feature engineering function to create relevant variables such as `Season`, `CustomerAge`, `MSRP`, and `PriceRange`.\n   - Cleaned the dataset by transforming data types and removing irrelevant columns.\n\n3. **Modeling:**\n   - Fitted a Random Forest model with cross-validation, optimizing for the ROC metric.\n   - Predictions were made on the test dataset.\n\n4. **Final Output:**\n   - Generated a submission file containing predictions.\n\n### Improvements and Considerations\n\n- The logistic regression model served as an initial step, indicating the need for refinement.\n- The moderate AUC score from the Random Forest model suggests exploring additional feature engineering or alternative modeling techniques could enhance predictive performance.\n\n### Technical Documentation\n\n- **Data Loading and Exploration:**\n  - Essential libraries for data manipulation and modeling were loaded.\n  - Initial exploration involved visualizing returns across different attributes to identify trends.\n\n- **Feature Engineering Function:**\n  - Transformed categorical variables and engineered new features to improve model input.\n  - Simplified the dataset by excluding non-essential columns and adjusting data types.\n\n- **Random Forest Modeling:**\n  - Implemented a Random Forest model with a focus on the ROC metric for evaluation.\n  - Applied cross-validation for model training to ensure robustness.\n\n- **Prediction and Submission:**\n  - Predicted probabilities of returns on the test set.\n  - Prepared the submission file with ID and predicted probabilities.\n\n### Comments and Clarifications\n\n- **Data Cleaning and Preparation:** Made clear the purpose of data transformations and feature engineering to prepare for modeling.\n- **Model Selection and Evaluation:** Explained the choice of models and their evaluation, highlighting the transition from logistic regression to Random Forest for improved accuracy.\n- **Submission Process:** Detailed the steps for preparing the final submission, ensuring clarity on the expected output format.\n\n\n## Load the required packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)\n```\n:::\n\n\n\n\n## Load the training and test data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- read_csv(\"../../../Assets/Datasets/customerReturnTrain.csv\") \ntest <- read_csv(\"../../../Assets/Datasets/customerReturnTest.csv\") \n\nglimpse(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…\n```\n\n\n:::\n:::\n\n\n\n\n## Data Exploration\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n:::\n\n\n\n\n## Feature Engineering\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 57, 57, 57, 57, 45, 45, 45, 57, 57, 57, 59, 59, 40, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :28.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:38.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :48.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :49.81  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:61.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :78.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(train$Returned)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   No   Yes \n42035 22877 \n```\n\n\n:::\n:::\n\n\n\n\n## Fit a Random Forest Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(345)\n\n#Model using Random Forest \nctrl <- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)\nfit <- train(Returned ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 50,\n             tuneLength = 3,\n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n64912 samples\n   10 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 43274, 43275, 43275 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5551023  1.0000000  0.0000000\n  36    0.6241038  0.8325442  0.3603618\n  70    0.6245050  0.8245271  0.3713335\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 70.\n```\n\n\n:::\n:::\n\n\n\n\n## Make prediction on the test data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntestPredictions <- predict(fit, newdata = test, type = \"prob\")[,2]\n```\n:::\n\n\n\n\n## Writing the Submission File\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\n# write.csv(submission, \"submission.csv\", row.names = FALSE)\n```\n:::\n\n\n\n\n## Leftout Features that were considered\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)\n```\n:::\n\n\n\n\n## Data References\n\n\n\n\n::: {.cell}\n\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}