{
  "hash": "ef66272661dc7abfd2517482c743ab53",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Applied Multivariate Analysis: Investigating Red & White Wines\"\nauthor: \n    - \"Brian Cervantes Alvarez\"\ndate: \"12-14-2023\"\ndescription: \"Exploratory data analysis distinguishes red and white wines; MANOVA confirms mean differences. Classification models reveal Random Forest's superiority, identifying key variables.\"\nimage: redwhite.jpeg\nformat:\n  html:\n    toc: true\n    toc-location: right\n    html-math-method: katex\n    page-layout: article\nexecute: \n  warning: false\n  message: false\ncategories: \n    - R\n    - Machine Learning\n    - Support Vector Machines\n    - Logistic Regression\n    - Random Forest\n    - Classification\n    - Statistics\n---\n\n\n![](redwhite.jpeg)\n\n\n# Distinguish White Wines From Red Wines\n\n\n## Perform EDA on Red & White Wine Means:\n\nI conducted an exploratory data analysis in comparing means for the 11 chemical attributes in the red and white wines. Notably, 'totalSulfurDioxide' showed significant mean differences, followed by 'freeSulfurDioxide' and 'residualSugar.' These attributes currently stand out showing a clear difference between the means of white and red wines.\n\n\n## Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.\n\n\\begin{align*}\n&H_0: \\bar{\\mu}_{\\text{red}} = \\bar{\\mu}_{\\text{white}} \\\\\n&H_A: \\bar{\\mu}_{\\text{red}} \\neq \\bar{\\mu}_{\\text{white}}\n\\end{align*}\n\n### MANOVA Results\n\nThe MANOVA reveals a significant disparity in the means for specific chemical traits between red and white wines. Notably, Pillai's Trace $(0.86158)$ indicates a robust effect, accounting for approximately $86.128\\%$ of the variance. The p-value of $2.2 \\times 10^{-16}$ signifies significant differences in the mean vectors. Hence, the MANOVA decisively rejects the null hypothesis of no difference in means, and we have exceptionally high confidence in accepting the alternative-that there is a difference in means between each wine.\n\n## Perform Classification Modeling on Red & White Wines\n\n### Train-Test Split & Cross-Validation Set up\n\nI performed a train-test split to ensure the models can handle new, unseen data. I allocated $70\\%$ to the training data to have a larger sample for the testing data $(30\\%)$. To enhance reliability, I employed cross-validation, repeatedly splitting the data into different training and testing sets. This approach provides a more comprehensive evaluation of the model's effectiveness.\n\n### Random Forest, SVM and Logistic Models\n\nFor my models, I've selected Random Forest, Support Vector Machine, and Logistic Regression as promising candidates for effective classification. Logistic Regression is particularly beneficial in binary classification scenarios due to its simplicity and interpretability. Meanwhile, Random Forest excels in capturing complex relationships through ensemble learning, and Support Vector Machine demonstrates proficiency in handling both linear and non-linear patterns. As the results will show later, Random Forest performed the best followed by SVM. Note, the models were not tuned to use their best hyperparameters.\n\n\n### Metrics & Variable Importance\n\nThe confusion matrix for the red and white wine classification using the Random Forest model shows strong performance. The model correctly identified $474$ red wines and $1464$ white wines, with only $5$ red wines and $5$ white wines misclassified. This indicates high accuracy in both precision and recall. In comparison to the Support Vector Machine (SVM) and Logistic Regression models, the Random Forest performed better by minimizing misclassifications. The SVM model had slightly more misclassified instances ($13$ in total), while the Logistic Regression model had $17$ misclassifications. The Random Forest's performance makes it a better choice for this classifying red and white wines compared to SVM and Logistic Regression.\n\n\nThe variable importance analysis shows that \"chlorides,\" with a significance of $100%$, is the most crucial feature for distinguishing red and white wines. Additionally, \"totalSulfurDioxide\" $(96.92%)$ and \"volatileAcidity\" $(43.47%)$ also played key roles, contributing to the model's clear performance in wine classification.\n\n### Classifying a New Red Wine Drawn From The Same Population\n\nTo estimate the probability of correctly classifying a new red wine drawn from the same population, we can use the concept of recall. \n\nIn our confusion matrix:\n\n$$ \\text{Recall (for red wine)} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}} $$\n\nTo find the probability:\n\n$$ \\text{Recall (for red wine)} = \\frac{474}{474 + 5} = \\frac{474}{479}  = 0.9896$$\n\nSo, the estimated probability of correctly classifying a new red wine, drawn from the same population, is approximately  $\\frac{474}{479}$, or roughly $98.96\\%$. This suggests a very high probability of correctly identifying red wines based on the model's current performance.\n\n## K-Means Clustering\n\nI chose k-means clustering with Euclidean distance for its efficiency with standardized numerical data. While k = 2 visually showed a clear distinction between red and white wines, higher k values (for example, 3 or 4) led to overlapping clusters, affecting the meaningful separation observed with k = 2.\n\n# Which Variables Are The Most Important To Wine Quality In Red Wines?\n\n## Perform MANOVAs To Determine If There Are A Significant Difference In Mean Vectors Between Wines With Different Quality/Quality Groups With A 95% Confidence Level.\n\n### MANOVA for Quality Levels 3, 4, 5, 6, 7, 8\n\n\\begin{align*}\n&H_0: \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 = \\mu_6 = \\mu_7 = \\mu_8 \\\\\n&H_A: \\text{At least one of } \\mu_2, \\mu_3, \\mu_4, \\mu_5, \\mu_6, \\mu_7, \\text{ or } \\mu_8 \\text{ is different}\n\\end{align*}\n\nIn the first analysis, we looked at the Original Quality Scores, and the results were highly significant. The p-value was super close to zero, less than $2.2 x 10^{-16}$. This means there are big differences in the average chemical properties for different quality scores. Therefore, the mean vectors for each quality level varied, providing strong support for rejecting the null hypothesis.\n\n\n### MANOVA for Quality Groups [Low, Medium, High]\n\n\\begin{align*}\n&H_0: \\mu_{\\text{Low}} = \\mu_{\\text{Medium}} = \\mu_{\\text{High}} \\\\\n&H_A: \\text{At least one of } \\mu_{\\text{Low}}, \\mu_{\\text{Medium}}, \\text{ or } \\mu_{\\text{High}} \\text{ is different}\n\\end{align*}\n\nIn the second analysis, we focused on Quality Groups (Low, Medium, High), and the results were also highly significant. The p-value was very close to zero, less than $2.2 x 10^{-16}$. This indicates significant differences in the average chemical properties across different quality groups. We have strong evidence that the mean vectors between each quality group differed. Therefore, we have evidence to reject the null and be in favor of the alternative.\n\n### Overall MANOVA Test Conclusion\n\nTo summarize, our MANOVA tests reveal significant differences in average values for both original quality scores and quality groups. For original scores, statistics like Pillai's trace and Wilks' lambda had extremely low p-values $p < 2.2 x 10^{-16}$. Quality groups exhibited similar results.\n\n\n## Perform Classification on Quality for Red Wines\n\n### Random Forest and SVM models\n\nI performed the same procedure from the previous classification of red and white wines. I've selected Random Forest & Support Vector Machine the top models for classification. Logistic Regression is not designed for multiple classes. Interestingly, the Random Forest performed the best again, followed by SVM. It's important to note that the models were not fine-tuned for hyperparameters at this stage.\n\n### Metrics & Variable Importance\n\nRandom Forest emerged as the top-performing model once again, with an accuracy of $69.2\\%$. For instance, we can observe that quality level 5 has the highest number of correct predictions $(163)$, while quality levels $4$ and $6$ have some misclassifications. Among the features, alcohol, total sulfur dioxide, and volatile acidity emerged as the top three influential variables, showcasing their significance in predicting wine quality in red wines.\n\n## Perform Principal Component Analysis\n\n### Explaining PC1 & PC2\n\nPC1 can be interpreted as representing \"Wine Body.\" Red wines with higher fixed acidity, citric acid, free sulfur dioxide, and total sulfur dioxide contribute positively to this component, indicating a fuller and more robust body. Hence, higher levels of volatile acidity, residual sugar, and alcohol contribute negatively to this component.\n\nPC2 can be labeled as \"Fermentation Characteristics.\" Additionally, red wines with elevated levels of free sulfur dioxide, total sulfur dioxide, and density contribute positively to this component, highlighting aspects related to the fermentation process. On the other end, higher alcohol content and volatile acidity contribute negatively to PC2.\n\n### Random Forest Model with 2 PCA\n\nIn the confusion matrix, it's evident that the model struggled to accurately predict certain classes, particularly in categories 3, 4, and 7, where the predicted values differ from the actual values. To add, random forest model achieved an accuracy of 58.07% which is quite below the previous models. Despite its limitations, the model demonstrated some success in capturing patterns related to \"Wine Body\" and \"Fermentation Characteristics.\" And it's with just 2 variables with linear combinations.\n\n\n### Random Forest Model with 11 PCAs\n\nThe random forest model attained an accuracy of 68.13%. Plus, it excelled in predicting class 5 but faced challenges in classes 3, 4, 6, and 7. Principal Component Analysis (PCA) highlights PC2 as the most influential (100%), followed by PC3 (80.49%), PC5 (32.47%), and others. This suggests a need for further analysis to enhance predictions in specific classes and leverage insights from key Principal Components for optimization.\n\n\n### Comparison between Random Forest Models (normal, 2PCs, 11PCs)\n\nIn comparing the Random Forest models, both the normal model and the 11 PCs model achieve an accuracy of approximately 68%, surpassing the 2 PCs model, which attains an accuracy of 58.07%. It's noteworthy that the 2 PCs model demonstrates the potency of PCA, albeit with a trade-off in interpretability. Despite the challenges encountered, each model variant provides valuable insights for optimizing the predictive power. The room for improvement is wide open. Factors such as hyperparameter tuning, other models that were not explored, feature engineering, and delving further into factor analysis are instances that could be used to maximize performance.\n\n\n\n\n{{< pagebreak >}}\n\n\n\n# Appendix\n\n\n# Goal 1: Distinguish White Wines From Red Wines\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set scipen to a high value to disable scientific notation\noptions(scipen = 999)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(cluster) \nlibrary(factoextra)\nlibrary(MASS)\nlibrary(rstatix)\n\n# Read in the wine datasets\nredWine <- read_csv(\"winequality-red.csv\")\nwhiteWine <- read_csv(\"winequality-white.csv\")\n\n# Look at the structure of the wine data\nglimpse(redWine)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,599\nColumns: 12\n$ `fixed acidity`        <dbl> 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     <dbl> 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          <dbl> 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       <dbl> 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              <dbl> 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  <dbl> 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` <dbl> 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                <dbl> 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     <dbl> 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              <dbl> 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                <dbl> 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n$ quality                <dbl> 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5,…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(whiteWine)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,898\nColumns: 12\n$ `fixed acidity`        <dbl> 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0, 6.3, 8.…\n$ `volatile acidity`     <dbl> 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.32, 0.27,…\n$ `citric acid`          <dbl> 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.16, 0.36,…\n$ `residual sugar`       <dbl> 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.00, 20.7…\n$ chlorides              <dbl> 0.045, 0.049, 0.050, 0.058, 0.058, 0.050, 0.045…\n$ `free sulfur dioxide`  <dbl> 45, 14, 30, 47, 47, 30, 30, 45, 14, 28, 11, 17,…\n$ `total sulfur dioxide` <dbl> 170, 132, 97, 186, 186, 97, 136, 170, 132, 129,…\n$ density                <dbl> 1.0010, 0.9940, 0.9951, 0.9956, 0.9956, 0.9951,…\n$ pH                     <dbl> 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.18, 3.00,…\n$ sulphates              <dbl> 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.47, 0.45,…\n$ alcohol                <dbl> 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.8, 9.5, …\n$ quality                <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7,…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run a quick summary for both datasets\n#summary(redWine)\n#summary(whiteWine)\n\n# Add a 'wine_type' column to identify the wine type\nredWine$wineType <- \"red\"\nwhiteWine$wineType <- \"white\"\n\n# Combine the datasets\nwine <- bind_rows(redWine, whiteWine)\n\n# Rename columns for better readability and consistency\nwine <- wine %>% \n  dplyr::mutate(fixedAcidity = `fixed acidity`,\n         volatileAcidity = `volatile acidity`,\n         citricAcid = `citric acid`,\n         residualSugar = `residual sugar`,\n         freeSulfurDioxide = `free sulfur dioxide`,\n         totalSulfurDioxide = `total sulfur dioxide`,\n         wineType = factor(wineType, levels = c(\"red\", \"white\"))) %>%\n  dplyr::select(-c(`fixed acidity`,\n            `volatile acidity`,\n            `citric acid`,\n            `residual sugar`,\n            `free sulfur dioxide`,\n            `total sulfur dioxide`))\n\n\n# Check for quality counts\nwine %>%\n  group_by(quality) %>%\n  summarise(totalCount = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 2\n  quality totalCount\n    <dbl>      <int>\n1       3         30\n2       4        216\n3       5       2138\n4       6       2836\n5       7       1079\n6       8        193\n7       9          5\n```\n\n\n:::\n\n```{.r .cell-code}\nwineDs <- wine %>%\n  select(-quality)\n\nnames(wineDs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"chlorides\"          \"density\"            \"pH\"                \n [4] \"sulphates\"          \"alcohol\"            \"wineType\"          \n [7] \"fixedAcidity\"       \"volatileAcidity\"    \"citricAcid\"        \n[10] \"residualSugar\"      \"freeSulfurDioxide\"  \"totalSulfurDioxide\"\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(wineDs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6,497\nColumns: 12\n$ chlorides          <dbl> 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069, 0.…\n$ density            <dbl> 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978, 0.9…\n$ pH                 <dbl> 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39, 3.3…\n$ sulphates          <dbl> 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47, 0.5…\n$ alcohol            <dbl> 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 10.5,…\n$ wineType           <fct> red, red, red, red, red, red, red, red, red, red, r…\n$ fixedAcidity       <dbl> 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7.5, …\n$ volatileAcidity    <dbl> 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600, 0.…\n$ citricAcid         <dbl> 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00, 0.0…\n$ residualSugar      <dbl> 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.1, 1…\n$ freeSulfurDioxide  <dbl> 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, 16, …\n$ totalSulfurDioxide <dbl> 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 102, 5…\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(is.na(wineDs))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n## Perform EDA on Red & White Wine Means:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the mean vectors for red and white wines \n# separately for each of the 11 chemical attributes\nmeanVectors <- wineDs %>% \n  group_by(wineType) %>%\n  summarize_all(mean)\n\n# Display the mean vectors\nhead(meanVectors, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 12\n  wineType chlorides density    pH sulphates alcohol fixedAcidity\n  <fct>        <dbl>   <dbl> <dbl>     <dbl>   <dbl>        <dbl>\n1 red         0.0875   0.997  3.31     0.658    10.4         8.32\n2 white       0.0458   0.994  3.19     0.490    10.5         6.85\n# ℹ 5 more variables: volatileAcidity <dbl>, citricAcid <dbl>,\n#   residualSugar <dbl>, freeSulfurDioxide <dbl>, totalSulfurDioxide <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert to long format for plotting\nmeanDs <- tidyr::gather(meanVectors,\n                        key = \"attribute\", \n                        value = \"means\", -wineType)\n\n#meanDs\n\n# Plot\np1 <- ggplot(meanDs, aes(x = means, y = attribute, fill = wineType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Mean Values for Red|White Wines\",\n       x = \"Mean Value\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\n\n# Plot\np2 <- ggplot(meanDs, aes(x = log(means), y = attribute, fill = wineType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Log-Transformed Mean Values for Red|White Wines\",\n       x = \"Mean Value\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\nmeanDifDs <- meanDs %>%\n  spread(wineType, means) %>%\n  mutate(meanDifference = red - white)\n\n# Plot the mean differences\np3 <- ggplot(meanDifDs, aes(x = meanDifference, y = attribute)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", \n           fill = \"red\", color = \"black\") +\n  labs(title = \"Mean Differences between Red & White Wines\",\n       x = \"Mean Difference\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\n\n# Show plot 1 for report\np1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\np2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\np3\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n:::\n\n\n## Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwineManova <- manova(cbind(chlorides, density, pH, sulphates, \n                           alcohol, fixedAcidity, volatileAcidity, \n                           citricAcid, residualSugar, freeSulfurDioxide,\n                           totalSulfurDioxide) ~ wineType, data = wineDs) \n\n# Print the summary of the MANOVA\nsummary(wineManova)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df  Pillai approx F num Df den Df                Pr(>F)    \nwineType     1 0.86158   3669.6     11   6485 < 0.00000000000000022 ***\nResiduals 6495                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### MANOVA Results\n\n## Perform Classification Modeling on Red & White Wines\n\n### Train-Test Split & Cross-Validation Set up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplitIndex <- createDataPartition(wineDs$wineType, p = 0.7, list = FALSE)\ntrainData <- wineDs[splitIndex, ]\ntestData <- wineDs[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n```\n:::\n\n\n### Random Forest, SVM and Logistic Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2013)\n\n# Train Random Forest\nrfModel <- train(wineType ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Train Logistic Regression\nlogisticModel <- train(wineType ~ ., \n                       data = trainData,\n                       method = \"glm\",\n                       family = \"binomial\")\n\n# Train Support Vector Machine\nsvmModel <- train(wineType ~ ., \n                  data = trainData,\n                  method = \"svmRadial\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred <- predict(rfModel, newdata = testData)\nlogisticPred <- predict(logisticModel, newdata = testData)\nsvmPred <- predict(svmModel, newdata = testData)\n```\n:::\n\n\n### Metrics & Variable Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Forest model metrics\nconfMatrixRF <- confusionMatrix(rfPred, testData$wineType, \n                                dnn = c(\"Prediction\", \"Reference\"))\naccuracyRF <- confMatrixRF$overall[\"Accuracy\"]\n\n# Logistic Regression model metrics\nconfMatrixLogistic <- confusionMatrix(logisticPred, testData$wineType, \n                                      dnn = c(\"Prediction\", \"Reference\"))\naccuracyLogistic <- confMatrixLogistic$overall[\"Accuracy\"]\n\n# SVM model metrics\nconfMatrixSVM <- confusionMatrix(svmPred, testData$wineType, \n                                 dnn = c(\"Prediction\", \"Reference\"))\naccuracySVM <- confMatrixSVM$overall[\"Accuracy\"]\n\n# Plot Confusion Matrices\nplotCM <- function(confMatrix, modelName) {\n  plt <- as.data.frame(confMatrix$table)\n  plt$Prediction <- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$wineType)) +\n    scale_y_discrete(labels = levels(testData$wineType))\n}\n\n#confMatrixRF\nconfMatrixLogistic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  red white\n     red    475     6\n     white    4  1463\n                                             \n               Accuracy : 0.9949             \n                 95% CI : (0.9906, 0.9975)   \n    No Information Rate : 0.7541             \n    P-Value [Acc > NIR] : <0.0000000000000002\n                                             \n                  Kappa : 0.9862             \n                                             \n Mcnemar's Test P-Value : 0.7518             \n                                             \n            Sensitivity : 0.9916             \n            Specificity : 0.9959             \n         Pos Pred Value : 0.9875             \n         Neg Pred Value : 0.9973             \n             Prevalence : 0.2459             \n         Detection Rate : 0.2438             \n   Detection Prevalence : 0.2469             \n      Balanced Accuracy : 0.9938             \n                                             \n       'Positive' Class : red                \n                                             \n```\n\n\n:::\n\n```{.r .cell-code}\n#confMatrixSVM\n\n# Plot Confusion Matrices for each model\nplotCM(confMatrixRF, \"Random Forest\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplotCM(confMatrixLogistic, \"Logistic Regression\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplotCM(confMatrixSVM, \"Support Vector Machine (SVM)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Print the metrics for each model\nprint(\"Random Forest Model Results\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Random Forest Model Results\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Accuracy:\", round(accuracyRF, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Accuracy: 0.9954\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Logistic Regression Model Results:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Logistic Regression Model Results:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Accuracy:\", round(accuracyLogistic, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Accuracy: 0.9949\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Support Vector Machine (SVM) Model Results:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Support Vector Machine (SVM) Model Results:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Accuracy:\", round(accuracySVM, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Accuracy: 0.9969\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get variable importance from the best model\nvarImp(rfModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nrf variable importance\n\n                   Overall\nchlorides          100.000\ntotalSulfurDioxide  96.093\nvolatileAcidity     50.052\ndensity             30.433\nfreeSulfurDioxide   27.940\nresidualSugar       19.545\nfixedAcidity        19.427\nsulphates           16.098\ncitricAcid           7.035\npH                   4.540\nalcohol              0.000\n```\n\n\n:::\n:::\n\n\n### Classifying a New Red Wine Drawn From The Same Population\n\n\n## Clustering:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nds <- wineDs %>%\n  select(-wineType)\n\nds <- scale(ds)\n\nfviz_nbclust(ds, kmeans, method='silhouette')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nkm.final <- kmeans(ds, 2, nstart = 30)\n\nfviz_cluster(km.final, data = ds, \n             geom = \"point\",\n             ellipse.type = \"convex\", \n             ggtheme = theme_bw())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\n# Goal 2: Which Variables Are The Most Important To Wine Quality In Red Wines.\n\n## Perform MANOVAs to determine if there are a significant difference in mean vectors between wines with different quality/quality groups with a 95% confidence level?\n\n### MANOVA for Quality Levels 3, 4, 5, 6, 7, 8\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nredWineDs <- wine %>%\n  filter(wineType == \"red\") %>%\n  select(-wineType)\n\n# Extracting Columns\ncolVars <-  cbind(\n  redWineDs$chlorides, redWineDs$density, redWineDs$pH, redWineDs$sulphates, \n  redWineDs$alcohol, redWineDs$fixedAcidity, redWineDs$volatileAcidity, \n  redWineDs$citricAcid, redWineDs$residualSugar, redWineDs$freeSulfurDioxide,\n  redWineDs$totalSulfurDioxide\n)\n\n# MANOVA Analysis - Quality, levels = 3,4,5,6,7,8\nmanaovaTest <- manova(colVars ~ quality, data = redWineDs)\nsummary(manaovaTest)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df  Pillai approx F num Df den Df                Pr(>F)    \nquality      1 0.36055   81.348     11   1587 < 0.00000000000000022 ***\nResiduals 1597                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### MANOVA for Quality Groups [Low, Medium, High]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adding qualityGroup\nredWineDs <- redWineDs %>% \n  mutate(\n    qualityGroup = case_when(\n      quality %in% 3:4 ~ \"Low\",\n      quality %in% 5:6 ~ \"Medium\",\n      quality %in% 7:8 ~ \"High\"\n    )\n  ) %>%\n  mutate(qualityGroup = factor(qualityGroup, levels = c(\"Low\", \n                                                        \"Medium\", \n                                                        \"High\")))\ncolVarsCategorized <-  cbind(\n  redWineDs$chlorides, redWineDs$density, redWineDs$pH, \n  redWineDs$sulphates, redWineDs$alcohol, redWineDs$fixedAcidity, \n  redWineDs$volatileAcidity, redWineDs$citricAcid, \n  redWineDs$residualSugar, redWineDs$freeSulfurDioxide,\n  redWineDs$totalSulfurDioxide\n)\n\n# MANOVA Analysis - QualityGroup, Levels = \"Low\", \"Medium\", \"High\"\nmanaovaTest <- manova(colVars ~ qualityGroup, data = redWineDs)\nsummary(manaovaTest)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Df  Pillai approx F num Df den Df                Pr(>F)    \nqualityGroup    2 0.30989   26.453     22   3174 < 0.00000000000000022 ***\nResiduals    1596                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### Overall MANOVA Test Conclusion\n\n## Perform Classification on Quality for Red Wines\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqualityDs <- redWineDs %>%\n  mutate(quality = factor(quality, levels = c(\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"))) %>%\n  select(-qualityGroup)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the dataset into training and testing sets\nsplitIndex <- createDataPartition(qualityDs$quality, p = 0.7, list = FALSE)\ntrainData <- qualityDs[splitIndex, ]\ntestData <- qualityDs[-splitIndex, ]\n\n# Create a train control object for repeated cross-validation\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n```\n:::\n\n\n### Random Forest and SVM models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2013)\n# Random Forest model\nrfModel <- train(quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# (SVM) model\nsvmModel <- train(quality ~ ., \n                  data = trainData,\n                  method = \"svmLinear\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred <- predict(rfModel, newdata = testData)\nsvmPred <- predict(svmModel, newdata = testData)\n```\n:::\n\n\n### Metrics & Variable Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Forest confusion matrix\nrfConfMatrix <- confusionMatrix(rfPred, testData$quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy <- rfConfMatrix$overall[\"Accuracy\"]\n\n# SVM confusion matrix\nsvmConfMatrix <- confusionMatrix(svmPred, testData$quality, \n                                 dnn = c(\"Prediction\", \"Reference\"))\nsvmAccuracy <- svmConfMatrix$overall[\"Accuracy\"]\n\n# Print the results\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Random Forest Accuracy: 0.691823899371069\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"SVM Accuracy:\", svmAccuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SVM Accuracy: 0.59538784067086\"\n```\n\n\n:::\n\n```{.r .cell-code}\nrfConfMatrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   0   1   0   0\n         5   3  10 163  46   5   0\n         6   0   4  40 137  24   4\n         7   0   1   1   7  30   1\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                               \n               Accuracy : 0.6918               \n                 95% CI : (0.6482, 0.733)      \n    No Information Rate : 0.4277               \n    P-Value [Acc > NIR] : < 0.00000000000000022\n                                               \n                  Kappa : 0.4953               \n                                               \n Mcnemar's Test P-Value : NA                   \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000 0.000000   0.7990   0.7173  0.50847  0.00000\nSpecificity          1.000000 0.997835   0.7656   0.7483  0.97608  1.00000\nPos Pred Value            NaN 0.000000   0.7181   0.6555  0.75000      NaN\nNeg Pred Value       0.993711 0.968487   0.8360   0.7985  0.93364  0.98952\nPrevalence           0.006289 0.031447   0.4277   0.4004  0.12369  0.01048\nDetection Rate       0.000000 0.000000   0.3417   0.2872  0.06289  0.00000\nDetection Prevalence 0.000000 0.002096   0.4759   0.4382  0.08386  0.00000\nBalanced Accuracy    0.500000 0.498918   0.7823   0.7328  0.74228  0.50000\n```\n\n\n:::\n\n```{.r .cell-code}\nsvmConfMatrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   1   0   0   0\n         4   0   0   0   0   0   0\n         5   2  11 159  66   6   0\n         6   1   4  44 125  53   5\n         7   0   0   0   0   0   0\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                            \n               Accuracy : 0.5954            \n                 95% CI : (0.5498, 0.6398)  \n    No Information Rate : 0.4277            \n    P-Value [Acc > NIR] : 0.0000000000001356\n                                            \n                  Kappa : 0.3101            \n                                            \n Mcnemar's Test P-Value : NA                \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000  0.00000   0.7794   0.6545   0.0000  0.00000\nSpecificity          0.997890  1.00000   0.6886   0.6259   1.0000  1.00000\nPos Pred Value       0.000000      NaN   0.6516   0.5388      NaN      NaN\nNeg Pred Value       0.993697  0.96855   0.8069   0.7306   0.8763  0.98952\nPrevalence           0.006289  0.03145   0.4277   0.4004   0.1237  0.01048\nDetection Rate       0.000000  0.00000   0.3333   0.2621   0.0000  0.00000\nDetection Prevalence 0.002096  0.00000   0.5115   0.4864   0.0000  0.00000\nBalanced Accuracy    0.498945  0.50000   0.7340   0.6402   0.5000  0.50000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot Confusion Matrices\nplotCM <- function(confMatrix, modelName) {\n  plt <- as.data.frame(confMatrix$table)\n  plt$Prediction <- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$quality)) +\n    scale_y_discrete(labels = levels(testData$quality))\n}\n\nplotCM(rfConfMatrix, \"Random Forest\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplotCM(svmConfMatrix, \"SVM\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(varImp(rfModel))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n:::\n\n\n## Perform Principal Component Analysis\n\n### Part 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds <- select(qualityDs, -quality)\n\n# Perform PCA\npcaResults <- prcomp(ds, scale = T, center = T)\n\n# Print PCA summary\nsummary(pcaResults)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     1.7604 1.3878 1.2452 1.1015 0.97943 0.81216 0.76406\nProportion of Variance 0.2817 0.1751 0.1410 0.1103 0.08721 0.05996 0.05307\nCumulative Proportion  0.2817 0.4568 0.5978 0.7081 0.79528 0.85525 0.90832\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.65035 0.58706 0.42583 0.24405\nProportion of Variance 0.03845 0.03133 0.01648 0.00541\nCumulative Proportion  0.94677 0.97810 0.99459 1.00000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize PCA results\nfviz_eig(pcaResults, addlabels = TRUE, kaiser = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfviz_pca_ind(pcaResults, geom = \"point\", col.ind = qualityDs$quality, palette = \"jco\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code}\nfviz_pca_biplot(pcaResults, geom = \"arrow\", col.var = \"contrib\", palette = \"jco\", alpha = 0.7)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n```{.r .cell-code}\npcaDs <- rownames_to_column(as.data.frame(pcaResults$rotation))\n\npcaDs %>% \n  select(rowname, PC1, PC2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              rowname         PC1          PC2\n1           chlorides  0.21224658  0.148051555\n2             density  0.39535301  0.233575490\n3                  pH -0.43851962  0.006710793\n4           sulphates  0.24292133 -0.037553916\n5             alcohol -0.11323207 -0.386180959\n6        fixedAcidity  0.48931422 -0.110502738\n7     volatileAcidity -0.23858436  0.274930480\n8          citricAcid  0.46363166 -0.151791356\n9       residualSugar  0.14610715  0.272080238\n10  freeSulfurDioxide -0.03615752  0.513566812\n11 totalSulfurDioxide  0.02357485  0.569486959\n```\n\n\n:::\n\n```{.r .cell-code}\n# Adding back the quality with principal components\nprc <- select(qualityDs, quality) %>%\n  bind_cols(as.data.frame(pcaResults$x)) %>%\n  select(quality, PC1, PC2) \n\n# Rename columns\nprc <- prc %>%\n  rename(\n    \"Quality\" = quality,\n    \"Wine Body\" = PC1,\n    \"Fermentation Characteristics\" = PC2\n  )\n\nprc2 <- select(qualityDs, quality) %>%\n  bind_cols(as.data.frame(pcaResults$x)) %>%\n  select(quality, PC1, PC2, PC3, PC4, PC5, PC6, PC7, PC8, PC9, PC10, PC11) \n\nprc2 <- prc2 %>%\n  rename(\"Quality\" = quality)\n\nprc2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,599 × 12\n   Quality    PC1    PC2    PC3      PC4     PC5    PC6    PC7    PC8      PC9\n   <fct>    <dbl>  <dbl>  <dbl>    <dbl>   <dbl>  <dbl>  <dbl>  <dbl>    <dbl>\n 1 5       -1.62   0.451 -1.77   0.0437  -0.0670 -0.914  0.161  0.282 -0.00510\n 2 5       -0.799  1.86  -0.911  0.548    0.0184  0.929  1.01  -0.762  0.521  \n 3 5       -0.748  0.882 -1.17   0.411    0.0435  0.401  0.539 -0.598  0.0868 \n 4 6        2.36  -0.270  0.243 -0.928    1.50   -0.131 -0.344  0.455 -0.0915 \n 5 5       -1.62   0.451 -1.77   0.0437  -0.0670 -0.914  0.161  0.282 -0.00510\n 6 5       -1.58   0.569 -1.54   0.0237   0.110  -0.993  0.110  0.314  0.0343 \n 7 5       -1.10   0.608 -1.08  -0.344    1.13    0.175 -0.261 -0.240  0.0273 \n 8 7       -2.25  -0.417 -0.987 -0.00120  0.780   0.286 -0.131 -0.119  0.614  \n 9 7       -1.09  -0.308 -1.52   0.00331  0.227  -0.512 -0.250 -0.439  0.399  \n10 5        0.655  1.66   1.21  -0.824   -1.72   -0.476 -0.230 -0.839 -1.27   \n# ℹ 1,589 more rows\n# ℹ 2 more variables: PC10 <dbl>, PC11 <dbl>\n```\n\n\n:::\n:::\n\n\n\n### Part 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2013)\n\nsplitIndex <- createDataPartition(prc$Quality, p = 0.7, list = FALSE)\ntrainData <- prc[splitIndex, ]\ntestData <- prc[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n# Train Random Forest\nrfModel <- train(Quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnote: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make predictions on the test set for each model\nrfPred <- predict(rfModel, newdata = testData)\n\n\n# Plot Confusion Matrices\nplotCM <- function(confMatrix, modelName) {\n  plt <- as.data.frame(confMatrix$table)\n  plt$Prediction <- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$Quality)) +\n    scale_y_discrete(labels = levels(testData$Quality))\n}\n\n# Random Forest metrics\nrfConfMatrix <- confusionMatrix(rfPred, testData$Quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy <- rfConfMatrix$overall[\"Accuracy\"]\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Random Forest Accuracy: 0.580712788259958\"\n```\n\n\n:::\n\n```{.r .cell-code}\nplotCM(rfConfMatrix, \"Random Forest\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrfConfMatrix$table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   3   2   0   0\n         5   1   7 135  53  11   1\n         6   2   6  56 117  23   4\n         7   0   2  10  19  25   0\n         8   0   0   0   0   0   0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variable Importance\nvarImp(rfModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nrf variable importance\n\n                               Overall\n`Fermentation Characteristics`     100\n`Wine Body`                          0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2013)\nsplitIndex <- createDataPartition(prc2$Quality, p = 0.7, list = FALSE)\ntrainData <- prc2[splitIndex, ]\ntestData <- prc2[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n# Train Random Forest\nrfModel <- train(Quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Make predictions on the test set for each model\nrfPred <- predict(rfModel, newdata = testData)\n\n\n# Plot Confusion Matrices\nplotCM <- function(confMatrix, modelName) {\n  plt <- as.data.frame(confMatrix$table)\n  plt$Prediction <- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$Quality)) +\n    scale_y_discrete(labels = levels(testData$Quality))\n}\n\n# Random Forest metrics\nrfConfMatrix <- confusionMatrix(rfPred, testData$Quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy <- rfConfMatrix$overall[\"Accuracy\"]\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Random Forest Accuracy: 0.681341719077568\"\n```\n\n\n:::\n\n```{.r .cell-code}\nplotCM(rfConfMatrix, \"Random Forest\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrfConfMatrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   1   0   0   0\n         5   3   9 161  47   4   0\n         6   0   6  39 134  25   3\n         7   0   0   3  10  30   2\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                               \n               Accuracy : 0.6813               \n                 95% CI : (0.6374, 0.723)      \n    No Information Rate : 0.4277               \n    P-Value [Acc > NIR] : < 0.00000000000000022\n                                               \n                  Kappa : 0.4807               \n                                               \n Mcnemar's Test P-Value : NA                   \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000 0.000000   0.7892   0.7016  0.50847  0.00000\nSpecificity          1.000000 0.997835   0.7692   0.7448  0.96411  1.00000\nPos Pred Value            NaN 0.000000   0.7188   0.6473  0.66667      NaN\nNeg Pred Value       0.993711 0.968487   0.8300   0.7889  0.93287  0.98952\nPrevalence           0.006289 0.031447   0.4277   0.4004  0.12369  0.01048\nDetection Rate       0.000000 0.000000   0.3375   0.2809  0.06289  0.00000\nDetection Prevalence 0.000000 0.002096   0.4696   0.4340  0.09434  0.00000\nBalanced Accuracy    0.500000 0.498918   0.7792   0.7232  0.73629  0.50000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variable Importance\nvarImp(rfModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nrf variable importance\n\n     Overall\nPC2  100.000\nPC3   80.491\nPC5   32.468\nPC9   30.188\nPC1   16.023\nPC4    7.563\nPC7    6.410\nPC8    5.631\nPC11   3.630\nPC10   3.489\nPC6    0.000\n```\n\n\n:::\n:::\n\n\n\n## Repeat for Grouped Quality Setting (Low, Medium, High)\n\n### Setting up data for classification\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroupedQualityDs <- redWineDs %>%\n  select(-quality)\n```\n:::\n\n\n### Train-Test-Split & Cross-Validation Set up\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the dataset into training and testing sets\nsplitIndex <- createDataPartition(groupedQualityDs$qualityGroup, p = 0.7, list = FALSE)\ntrainData <- groupedQualityDs[splitIndex, ]\ntestData <- groupedQualityDs[-splitIndex, ]\n\n# Create a train control object for repeated cross-validation\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n```\n:::\n\n\n### Random Forest and SVM models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2013)\n# Train Random Forest classifier using caret\nrfModel <- train(qualityGroup ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Train Support Vector Machine (SVM) model using caret\nsvmModel <- train(qualityGroup ~ ., \n                  data = trainData,\n                  method = \"svmRadial\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred <- predict(rfModel, newdata = testData)\nsvmPred <- predict(svmModel, newdata = testData)\n```\n:::\n\n\n### Metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot Confusion Matrices\nplotCM <- function(confMatrix, modelName) {\n  plt <- as.data.frame(confMatrix$table)\n  plt$Prediction <- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$qualityGroup)) +\n    scale_y_discrete(labels = levels(testData$qualityGroup))\n}\n\n# Random Forest metrics\nrfConfMatrix <- confusionMatrix(rfPred, testData$qualityGroup, \n                                dnn = c(\"Prediction\", \"Reference\"))\nsvmConfMatrix <- confusionMatrix(svmPred, testData$qualityGroup, \n                                 dnn = c(\"Prediction\", \"Reference\"))\n\nrfConfMatrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low      0      0    0\n    Medium  18    388   39\n    High     0      7   26\n\nOverall Statistics\n                                          \n               Accuracy : 0.8661          \n                 95% CI : (0.8323, 0.8953)\n    No Information Rate : 0.8264          \n    P-Value [Acc > NIR] : 0.01092         \n                                          \n                  Kappa : 0.395           \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity             0.00000        0.9823     0.40000\nSpecificity             1.00000        0.3133     0.98305\nPos Pred Value              NaN        0.8719     0.78788\nNeg Pred Value          0.96234        0.7879     0.91236\nPrevalence              0.03766        0.8264     0.13598\nDetection Rate          0.00000        0.8117     0.05439\nDetection Prevalence    0.00000        0.9310     0.06904\nBalanced Accuracy       0.50000        0.6478     0.69153\n```\n\n\n:::\n\n```{.r .cell-code}\nsvmConfMatrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low      0      0    0\n    Medium  18    393   50\n    High     0      2   15\n\nOverall Statistics\n                                         \n               Accuracy : 0.8536         \n                 95% CI : (0.8186, 0.884)\n    No Information Rate : 0.8264         \n    P-Value [Acc > NIR] : 0.06328        \n                                         \n                  Kappa : 0.2611         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity             0.00000        0.9949     0.23077\nSpecificity             1.00000        0.1807     0.99516\nPos Pred Value              NaN        0.8525     0.88235\nNeg Pred Value          0.96234        0.8824     0.89154\nPrevalence              0.03766        0.8264     0.13598\nDetection Rate          0.00000        0.8222     0.03138\nDetection Prevalence    0.00000        0.9644     0.03556\nBalanced Accuracy       0.50000        0.5878     0.61296\n```\n\n\n:::\n\n```{.r .cell-code}\nplotCM(rfConfMatrix, \"Random Forest\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplotCM(svmConfMatrix, \"SVM\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n:::\n\n\n### Look at variable importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(varImp(rfModel))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}