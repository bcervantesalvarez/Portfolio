{
  "hash": "42f42b16cd2b285038115224e77d4b25",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Symptom Severity Analysis\nauthor: Brian Cervantes Alvarez\ndate: 6-26-2023\ndescription: 'Using symptom analysis, this study employs a Random Forest approach to predict severity levels in patients, aiming to enhance healthcare decision-making.'\nimage: severity.jpeg\nformat:\n  html:\n    toc: true\n    toc-location: right\n    html-math-method: katex\n    page-layout: article\nexecute:\n  warning: false\n  message: false\ncategories:\n  - Python\n  - Machine Learning\n  - Random Forest\n  - Classification\n---\n\n![](severity.jpeg)\n\n## Abstract\n\nThis paper presents a machine learning study that aims to predict severity levels in patients by utilizing a random forest approach based on symptom analysis. The objective of the study was to develop accurate rules for classifying patients into three severity levels: Mild, Moderate, and Severe. The dataset consisted of patient symptoms, which were used in conjunction with a random forest model for the classification task. The results indicated that patients with 1 to 3 symptoms, including depression, were classified as Mild, while those with at least 3 symptoms, including depression and cramps, were classified as Moderate. Patients with 4 symptoms, including cramps and spasms, were classified as Severe. The study conducted comprehensive evaluations using metrics such as the confusion matrix, ROC AUC, precision vs recall, and kappa to assess the performance of the random forest model. The results highlighted the promising performance of the model in accurately predicting severity levels among patients. The findings from this study provide robust evidence that can contribute to personalized healthcare and effective treatment planning.\n\n## Introduction\n\nIn the field of rare diseases, physicians often observe variations in the severity levels of patients; however, there is a lack of established guidelines to map individual patients directly to specific severity levels. Our client, who is developing a product for this rare disease, aims to target patients with moderate to severe conditions. To address this challenge, the client possesses a comprehensive database containing clinically relevant information about the disease, including symptom flags and a symptom count variable. Additionally, each patient in the database has been rated by a physician as mild, moderate, or severe. The client has approached [REDACTED] with the objective of extracting the mental heuristics employed by physicians when assigning severity labels to patients. The task at hand involves deriving simple rules (1-3 per severity level) from the database that can effectively classify patients. For instance, a set of rules might involve identifying patients as severe if they exhibit fatigue or have exactly two symptoms. By extracting these rules, our aim is to provide the client with a clearer understanding of the factors influencing severity levels in order to enhance their product development and enable more personalized patient care.\n\n\n## Methodology\n\n::: {#0cbacd80 .cell execution_count=1}\n``` {.python .cell-code}\n# Import the required libraries\nimport pandas as pd                 # Library for data manipulation and analysis\nimport numpy as np                  # Library for numerical computations\nimport matplotlib.pyplot as plt     # Library for data visualization\nimport seaborn as sns\n\n# Import Machine Learning libraries\nfrom sklearn import tree            # Library for decision tree models\nfrom sklearn.ensemble import RandomForestClassifier  # Library for random forest models\nfrom sklearn.model_selection import (\n  cross_val_score,                   # Library for cross-validation\n  train_test_split,\n  GridSearchCV\n\n)  \nfrom sklearn.preprocessing import LabelEncoder      # Library for label encoding\nfrom sklearn.metrics import (                        # Library for model evaluation metrics\n    confusion_matrix,                                # Confusion matrix\n    roc_auc_score,                                   # ROC AUC score\n    roc_curve,                                       # ROC Curve plot\n    recall_score,                                    # Recall score\n    precision_score,                                 # Precision score\n    precision_recall_curve,                          # Recall vs. Precision Curve\n    cohen_kappa_score                                # Cohen's kappa score\n)\n\n# Load the dataset\nds = pd.read_csv(\"severityLevels.csv\")  \nds.head(10)  # Display the first 10 rows of the dataset\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Final Category</th>\n      <th>Fatigue</th>\n      <th>Weakness</th>\n      <th>Depression</th>\n      <th>Anxiety</th>\n      <th>Dry Skin</th>\n      <th>Spasms</th>\n      <th>Tingling</th>\n      <th>Headaches</th>\n      <th>Cramps</th>\n      <th>Number of Symptoms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Mild</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>Mild</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>Mild</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Exploratory Data Analysis\n\n::: {#7e1f6887 .cell execution_count=2}\n``` {.python .cell-code}\n# Display basic statistics of the dataset\nds.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Fatigue</th>\n      <th>Weakness</th>\n      <th>Depression</th>\n      <th>Anxiety</th>\n      <th>Dry Skin</th>\n      <th>Spasms</th>\n      <th>Tingling</th>\n      <th>Headaches</th>\n      <th>Cramps</th>\n      <th>Number of Symptoms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n      <td>300.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>150.500000</td>\n      <td>0.466667</td>\n      <td>0.396667</td>\n      <td>0.356667</td>\n      <td>0.363333</td>\n      <td>0.366667</td>\n      <td>0.200000</td>\n      <td>0.323333</td>\n      <td>0.333333</td>\n      <td>0.213333</td>\n      <td>3.020000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>86.746758</td>\n      <td>0.499721</td>\n      <td>0.490023</td>\n      <td>0.479816</td>\n      <td>0.481763</td>\n      <td>0.482700</td>\n      <td>0.400668</td>\n      <td>0.468530</td>\n      <td>0.472192</td>\n      <td>0.410346</td>\n      <td>1.703704</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>75.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>150.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>225.250000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>300.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>8.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#f8782841 .cell execution_count=3}\n``` {.python .cell-code}\n# Visualize the distribution of the target variable 'Final Category'\nplt.figure(figsize=(9, 6))\nds['Final Category'].value_counts().plot(kind='bar')\nplt.xlabel('Symptom Category')\nplt.ylabel('Number of Cases')\nplt.title('Distribution of Final Category')\nplt.xticks(rotation=0)  # Set rotation to 0 degrees for horizontal x-axis labels\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=742 height=523}\n:::\n:::\n\n\n::: {#7ac653fd .cell execution_count=4}\n``` {.python .cell-code}\n# Calculate the correlation matrix\ncorr_matrix = ds.corr()\n\n# Visualize the correlation matrix\nplt.figure(figsize=(10, 7))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.xticks(rotation=45)  # Set rotation to 0 degrees for horizontal x-axis labels\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=864 height=679}\n:::\n:::\n\n\n### Feature Engineering\n\nThe code performs some data preprocessing and feature selection to prepare the data for a Random Forest model. Let's go through each step:\n\n1. Drop the ID column: The code removes a column named \"Patient\" from the dataset. This column likely contains unique identifiers for each patient, and since it's not relevant for our analysis, we can safely remove it.\n\n2. Create dummy columns for \"Number of Symptoms\": The code converts a column called \"Number of Symptoms\" into multiple binary columns, known as dummy variables. Each dummy variable represents a unique value in the original column. This transformation helps us to use categorical data in our machine learning model.\n\n3. Concatenate the dummy columns with the original dataframe: The code combines the newly created dummy columns with the original dataset. This ensures that we retain all the existing information while incorporating the transformed categorical data.\n\n4. Drop the original \"Number of Symptoms\" column: Since we have created the dummy columns, we no longer need the original \"Number of Symptoms\" column. Therefore, the code removes this column from the dataset.\n\n5. Separate the features (X) and the target variable (y): The code splits the dataset into two parts. The features, represented by the variable X, contain all the columns except the \"Final Category\" column, which is the target variable we want to predict. The target variable, represented by the variable y, contains only the \"Final Category\" column. This is the severity cases of 'Mild', 'Moderate' and 'Severe'\n\n6. Perform feature selection using Random Forest: The code utilizes a machine learning algorithm called Random Forest to identify the most important features for each category in the target variable. It trains a separate Random Forest model for each category and determines the top three features that contribute the most to predicting that category.\n\n7. Store the top features for each category: The code stores the top three features for each category in a dictionary called \"top_features.\" Each category is represented by a label, and the corresponding top features are stored as a list.\n\n8. Print the top 3 features for each label: The code then prints the top three features for each category in the target variable. This information helps us understand which features are most influential in determining the predicted category.\n\nOverall, this code prepares the data by transforming categorical data into a suitable format and identifies the top features that contribute to predicting different categories. This sets the stage for further analysis and building the final machine learning model based on these selected features.\n\n::: {#96e7cab1 .cell execution_count=5}\n``` {.python .cell-code}\n# Drop the ID column\nds.drop('Patient', axis=1, inplace=True)\n\n# Create dummy columns from the \"Number of Symptoms\" column\ndummy_cols = pd.get_dummies(ds['Number of Symptoms'], prefix='Symptom')\n\n# Concatenate the dummy columns with the original dataframe\nds = pd.concat([ds, dummy_cols], axis=1)\n\n# Drop the original \"Number of Symptoms\" column\nds.drop('Number of Symptoms', axis=1, inplace=True)\n\nds.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Final Category</th>\n      <th>Fatigue</th>\n      <th>Weakness</th>\n      <th>Depression</th>\n      <th>Anxiety</th>\n      <th>Dry Skin</th>\n      <th>Spasms</th>\n      <th>Tingling</th>\n      <th>Headaches</th>\n      <th>Cramps</th>\n      <th>Symptom_0</th>\n      <th>Symptom_1</th>\n      <th>Symptom_2</th>\n      <th>Symptom_3</th>\n      <th>Symptom_4</th>\n      <th>Symptom_5</th>\n      <th>Symptom_6</th>\n      <th>Symptom_7</th>\n      <th>Symptom_8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Mild</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Mild</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Mild</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Mild</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#8245d2aa .cell execution_count=6}\n``` {.python .cell-code}\n# Separate the features (X) and the target variable (y)\nX = ds.drop('Final Category', axis=1)\ny = ds['Final Category']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store top features per label\ntop_features = {}\n\n# Perform feature selection for each label\nfor label in y_train.unique():\n    # Encode the target variable for the current label\n    y_label_train = (y_train == label).astype(int)\n    y_label_test = (y_test == label).astype(int)\n    \n    # Create a Random Forest model\n    rf_model = RandomForestClassifier(random_state=60)\n    \n    # Train the model\n    rf_model.fit(X_train, y_label_train)\n    \n    # Get feature importances\n    feature_importances = rf_model.feature_importances_\n    \n    # Sort features by importance in descending order\n    sorted_features = sorted(zip(X_train.columns, feature_importances), key=lambda x: x[1], reverse=True)\n    \n    # Get the top 3 features for the current label\n    selected_features = [feature for feature, _ in sorted_features[:3]]\n    \n    # Store the top features for the current label\n    top_features[label] = selected_features\n\n# Print the top 3 features for each label\nfor label, features in top_features.items():\n    print(f\"Top 3 features for {label}:\")\n    print(features)\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 3 features for Severe:\n['Cramps', 'Spasms', 'Symptom_4']\n\nTop 3 features for Mild:\n['Depression ', 'Symptom_1', 'Symptom_3']\n\nTop 3 features for Moderate:\n['Symptom_3', 'Depression ', 'Cramps']\n\n```\n:::\n:::\n\n\n#### Features Explained\n\nThe feature selection process aims to identify the most important factors (features) that contribute to determining the severity of a patient's condition. In this case, the severity levels are categorized as \"Mild,\" \"Moderate,\" and \"Severe.\" The top three features that were found to be most indicative for each severity level are as follows:\n\n**For patients rated as \"Mild\":**\n\n1. **Has 1 symptom:**\n\nThis feature indicates the presence of a specific symptom (let's say, symptom X) that is associated with a mild severity rating. If a patient has symptom X, it suggests a higher likelihood of being classified as \"Mild.\"\n\n2. **Depression:** \n\nThis feature refers to the presence or absence of depression symptoms in the patient. The presence of depression symptoms is considered important in determining a mild severity rating.\n\n3. **Has 3 symptoms:** \n\nThis feature represents the presence of 3 symptoms (let's call them symptoms A,B,C) that are associated with a mild severity rating. If a patient has symptoms A,B,C, it suggests a higher likelihood of being classified as \"Mild.\"\n\nGiven this information, it can be recommended that a threshold is established. It can be inferred that if a patient has 1-3 symptoms and/or has depression, they can be classified as \"Mild.\" This is addressed with the model later in the study.\n\n**For patients rated as \"Moderate\":**\n\n1. **Has 3 symptoms:** \n\nThis feature represents the presence of 3 symptoms (let's call them symptoms A,B,C again) that are associated with a moderate severity rating. If a patient has symptoms A,B,C, it suggests a higher likelihood of being classified as \"Moderate.\" \n\n2. **Depression:** \n\nThe presence or absence of depression symptoms also plays a role in determining a moderate severity rating.\n\n3. **Cramps:** \n\nThis feature represents the presence or absence of cramps in the patient. The presence of cramps is considered important in predicting a moderate severity rating.\n\nGiven this information, another threshold can be established. It can be inferred that if a patient has at least 3 symptoms and/or has depression and/or cramps, they can be classified as \"Moderate.\" This is addressed with the model later in the study.\n\n**For patients rated as \"Severe\":**\n\n1. **Cramps:** \n\nThis feature indicates the presence or absence of cramps, which is associated with a severe severity rating. If a patient has cramps, it suggests a higher likelihood of being classified as \"Severe.\"\n\n2. **Spasms:** \n\nThis feature refers to the presence or absence of muscle spasms in the patient. The presence of spasms is considered important in predicting a severe severity rating.\n\n3. **Has 4 symptoms:** \n\nThis feature represents the presence of symptoms (let's call it symptoms A,B,C,D) that are associated with a severe severity rating. If a patient has symptoms , it suggests a higher likelihood of being classified as \"Severe.\"\n\nGiven this information, a last threshold can be established. It can be inferred that if a patient has at least 4 symptoms and/or has cramps and/or spasms, they can be classified as \"Severe.\" This is addressed with the model later in the study.\n\n\nIn summary, the top features identified for each severity level provide insights into the specific symptoms and factors that contribute to determining the severity of a patient's condition. By considering the presence or absence of these features, the model can make predictions about the severity rating of a patient's condition, helping healthcare professionals assess the level of severity and provide appropriate care and treatment.\n\n### Random Forest Model\n\nThe code performs machine learning tasks using a Random Forest model with the selected features from the earlier model. Let's go through each step:\n\n1. Separate the features (X) and the target variable (y) using only the top features: The code selects specific features from the dataset based on their importance in predicting the target variable. These features are obtained from the \"top_features\" dictionary, which contains the top three features for each category (Mild, Moderate, and Severe) in the target variable.\n\n2. Encode the target variable using label encoding: The target variable \"Final Category\" is a categorical variable. To use it in the machine learning model, we need to convert it into numeric form. The code uses label encoding, which assigns a unique numeric value to each category in the target variable.\n\n3. Create a random forest model using the top features: The code initializes a Random Forest model with a specific random state. Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions.\n\n4. Perform 10-fold cross-validation: The code evaluates the performance of the Random Forest model using a technique called cross-validation. Cross-validation helps estimate how well the model will generalize to new, unseen data. In this case, 10-fold cross-validation is performed, which means the dataset is divided into 10 equal parts (folds). The model is trained and tested 10 times, with each fold serving as the test set once.\n\n5. Print the cross-validation scores: The code prints the cross-validation scores obtained from each fold. These scores indicate how well the model performed on each fold. Additionally, the mean cross-validation score is calculated, which provides an overall measure of the model's performance.\n\n6. Train the model: The code trains the Random Forest model using all the available data, as specified by the features (X) and target variable (y).\n\n7. Make predictions on the training set: The code uses the trained model to make predictions on the same dataset that was used for training. This helps evaluate how well the model can predict the target variable for the given features.\n\n8. Print the confusion matrix: The code prints a confusion matrix, which is a table that shows the number of correct and incorrect predictions made by the model. It provides insights into the model's performance for each category in the target variable.\n\n9. Calculate and print other evaluation metrics: The code calculates additional evaluation metrics such as ROC AUC score, recall, precision, and Kappa metric. These metrics help assess the model's performance in terms of classification accuracy, sensitivity, precision, and agreement beyond chance.\n\nOverall, this code builds a Random Forest model using selected features, evaluates its performance through cross-validation, and provides insights into the model's predictive capabilities using various evaluation metrics. The goal is to understand how well the model can predict the categories in the target variable based on the selected features.\n\n::: {#7fc82c0b .cell execution_count=7}\n``` {.python .cell-code}\n# Separate the features (X) and the target variable (y) using only the top features\nX = ds[top_features['Mild'] + top_features['Moderate'] + top_features['Severe']]\ny = ds['Final Category']\n\n# Encode the target variable using label encoding\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Create a random forest model\nrf_model = RandomForestClassifier(random_state=60)\n\n# Define the parameter grid for hyperparameter optimization\nparam_grid = {\n    'max_depth': [None],\n    'min_samples_leaf': [4],\n    'min_samples_split': [2],\n    'n_estimators': [100]\n}\n\n# Perform hyperparameter optimization using grid search and 10-fold cross-validation\ngrid_search = GridSearchCV(rf_model, param_grid, cv=10)\ngrid_search.fit(X, y)\n\n# Get the best random forest model with optimized hyperparameters\nrf_model = grid_search.best_estimator_\n\n# Perform 10-fold cross-validation with the optimized model\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Print the cross-validation scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())\nprint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCross-Validation Scores: [0.86666667 0.9        1.         0.9        0.96666667 0.9\n 0.9        0.83333333 0.86666667 0.8       ]\nMean CV Score: 0.8933333333333333\n\n```\n:::\n:::\n\n\n::: {#f6eb9e48 .cell execution_count=8}\n``` {.python .cell-code}\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=87)\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\n\n# Calculate and print ROC AUC score\nroc_auc = roc_auc_score(y_test, rf_model.predict_proba(X_test), multi_class='ovr')\nprint(\"ROC AUC:\", roc_auc)\nprint()\n\n# Calculate and print recall score\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint(\"Recall:\", recall)\nprint()\n\n# Calculate and print precision score\nprecision = precision_score(y_test, y_pred, average='weighted')\nprint(\"Precision:\", precision)\nprint()\n\n# Calculate and print Kappa metric\nkappa = cohen_kappa_score(y_test, y_pred)\nprint(\"Kappa:\", kappa)\nprint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix:\n[[18  4  0]\n [ 0 20  1]\n [ 0  1 16]]\n\nROC AUC: 0.9480560359313329\n\nRecall: 0.9\n\nPrecision: 0.9133333333333333\n\nKappa: 0.8493723849372385\n\n```\n:::\n:::\n\n\n### Metrics Explained\n\nLet's break down the provided metrics based on the given confusion matrix:\n\n#### Confusion Matrix\n\nThe confusion matrix is a table that helps us understand the performance of a classification model. It shows the predicted labels versus the actual labels for each class. In this case, the confusion matrix has three rows and three columns, representing the three severity rating categories. In this case, since I set the test set to be 20% of the sample of 300, there is 60 total patients that were randomly tested.\n\n\n$$\n\\begin{array}{c|ccc}\n& {\\text{Actual}} \\\\ \n\\text{Predicted} & \\text{Mild} & \\text{Moderate} & \\text{Severe} \\\\\n\\hline\n\\text{Mild} & 18 & 4 & 0 \\\\\n\\text{Moderate} & 0 & 20 & 1 \\\\\n\\text{Severe} & 0 & 1 & 16 \\\\\n\\end{array}\n$$\n\n\nFirst, the number 18 in the first row and column indicates that the model correctly predicted 18 patients as \"mild\" which coincides with the actual \"mild\" severity label. Next, the number 4 in the first row and second column indicates that the model incorrectly predicted 4 patients as \"moderate\" when their actual severity rating was \"mild.\" Lastly, the number 0 in the first row and third column indicates that the model correctly predicted 0 patients as being \"severe\", meaning that those patients were labeled correctly as \"mild\".\n\nSimilarly, the other numbers in the confusion matrix represent the model's predictions for the other severity rating categories. Given that this was a dataset with only 300 patients, it is very intriguing that it can label each patient with high accuarcy. \n\n\n#### ROC Curve\n\nNow, the ROC AUC (Receiver Operating Characteristic Area Under the Curve) is a measure of the model's ability to distinguish between different severity ratings. It represents the overall performance of the model across all severity levels. The value of 0.948 indicates a high level of performance, close to 1, suggesting that the model has good predictive capability for distinguishing between severity ratings.\n\n::: {#c2c561c1 .cell execution_count=9}\n``` {.python .cell-code}\n# Calculate predicted probabilities for each class\ny_pred_proba = rf_model.predict_proba(X_test)\n\n# Compute the ROC curve for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor class_index in range(len(label_encoder.classes_)):\n    fpr[class_index], tpr[class_index], _ = roc_curve(\n        (y_test == class_index).astype(int), y_pred_proba[:, class_index]\n    )\n    roc_auc[class_index] = roc_auc_score(\n        (y_test == class_index).astype(int), y_pred_proba[:, class_index]\n    )\n\n# Plot the ROC curve for each class\nplt.figure(figsize=(9, 8))\nfor class_index in range(len(label_encoder.classes_)):\n    plt.plot(\n        fpr[class_index],\n        tpr[class_index],\n        label=f\"Class {label_encoder.classes_[class_index]} (AUC = {roc_auc[class_index]:.2f})\",\n    )\n\n# Plot random guessing line\nplt.plot([0, 1], [0, 1], \"k--\")\n\n# Set plot properties\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\n\n# Show the plot\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=748 height=671}\n:::\n:::\n\n\n#### Recall vs. Precision Curve\n\nRecall, also known as sensitivity or true positive rate, measures the proportion of actual positive instances (patients with a particular severity rating) that the model correctly identified. A recall value of 0.9 means that the model identified nearly 90.0% of the patients with their correct severity rating.\n\nPrecision measures the proportion of instances that the model predicted correctly as positive (patients with a particular severity rating) out of all instances it predicted as positive. A precision value of 0.913 indicates that out of all the patients the model identified as having a specific severity rating, 91.3% of them were correct.\n\n::: {#eca81464 .cell execution_count=10}\n``` {.python .cell-code}\n# Calculate precision and recall values for each class\nprecision, recall, thresholds = precision_recall_curve(y_test, rf_model.predict_proba(X_test)[:, 1], pos_label=1)\n\n# Plot the recall vs. precision curve\nplt.figure(figsize=(9, 8))\nplt.plot(recall, precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Recall vs. Precision Curve')\nplt.grid(True)\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=738 height=671}\n:::\n:::\n\n\n#### Kappa\n\nThe Kappa statistic is a measure of agreement between the model's predictions and the actual severity ratings, taking into account the possibility of agreement occurring by chance. A Kappa value of 0.849 indicates a substantial level of agreement between the model's predictions and the actual severity ratings, suggesting a reliable performance of the model.\n\nOverall, these metrics indicate that the model has performed well in predicting the severity ratings of the patients, with high accuracy, good distinction between severity levels, and substantial agreement with the actual severity ratings provided by physicians.\n\n\n### Visualizing Random Forest's Best Decision Tree\n\n::: {#4e0e07a5 .cell execution_count=11}\n``` {.python .cell-code}\n# Perform 10-fold cross-validation\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Find the index of the best decision tree\nbest_tree_index = np.argmax(cv_scores)\n\n# Get the best decision tree from the random forest\nbest_tree = rf_model.estimators_[best_tree_index]\n\n# Visualize the best decision tree using matplotlib\nplt.figure(figsize=(12, 12))\ntree.plot_tree(best_tree, feature_names=X.columns, class_names=label_encoder.classes_, filled=True)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=912 height=906}\n:::\n:::\n\n\n## Results\n\n1. If a patient has between 1 to 3 symptoms, and one of those symptoms includes depression, they are classified as 'Mild'.\n\n2. If a patient has at least 3 symptoms, and two of those symptoms are depression and cramps, they are classified as 'Moderate'.\n\n3. Lastly, if a patient has 4 symptoms, and two of those symptoms are cramps and spasms, they are classified as 'Severe'.\n\n## Conclusion\n\nThis study focused on predicting severity levels in patients with a rare disease using a random forest approach based on symptom analysis. The client's goal was to map individual patients to appropriate severity levels, given the absence of established guidelines. By leveraging a comprehensive database containing clinically relevant information and severity ratings provided by physicians, we extracted simple rules to classify patients.\n\nThe study revealed that patients with 1 to 3 symptoms, including depression, were classified as 'Mild'. For patients with at least 3 symptoms, the presence of depression and cramps (at least 2 symptoms) indicated a classification of 'Moderate'. Lastly, patients presenting with 4 symptoms, including cramps and spasms (at least 2 symptoms), were categorized as 'Severe'.\n\nThe derived rules provide valuable insights into the mental heuristics employed by physicians when assessing severity levels in patients. By incorporating these rules into the client's product development, personalized healthcare targeting patients with moderate to severe disease can be enhanced. Furthermore, these findings contribute to filling the existing gap in severity level guidelines for this rare disease.\n\n## Further Research\n\nIt is recommended to validate and refine these rules using larger datasets. By expanding the dataset, researchers can ensure the generalizability of the derived rules and improve the accuracy of the classification. Additionally, exploring additional factors that may influence severity levels, such as demographic information, medical history, or genetic markers, can provide a more comprehensive understanding of the disease and enhance the prediction models.\n\nBy continuing to refine and validate the rules and incorporating more factors into the analysis, personalized treatment for patients with this rare disease can be further optimized, leading to improved patient outcomes. The combination of symptom analysis and machine learning approaches holds significant potential for facilitating accurate classification and personalized healthcare in various medical domains.\n\n## References\n\n**Pandas:**\nMcKinney, W. (2010). Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference, 445-451. [Link: https://conference.scipy.org/proceedings/scipy2010/mckinney.html]\n\n**NumPy:**\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., ... Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357-362. [Link: https://www.nature.com/articles/s41586-020-2649-2]\n\n**Matplotlib:**\nHunter, J. D. (2007). Matplotlib: A 2D Graphics Environment. Computing in Science & Engineering, 9(3), 90-95. [Link: https://ieeexplore.ieee.org/document/4160265]\n\n**Seaborn:**\nWaskom, M., Botvinnik, O., Hobson, P., … Halchenko, Y. (2021). mwaskom/seaborn: v0.11.1 (February 2021). Zenodo. [Link: https://doi.org/10.5281/zenodo.4473861]\n\n**Scikit-learn:**\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830. [Link: https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html]\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}