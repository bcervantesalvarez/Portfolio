{
  "hash": "f3f38850f1089d0f47848af1fe54dd77",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Pinot Province Prediction\"\nauthor: \"Brian Cervantes Alvarez\"\ndate: \"03-01-2023\"\ndate-modified: today\ndescription: \"Our project sets a new bar in wine origin identification, transforming how industry professionals use critic data.\"\nteaser: \"Classifying wine origin with advanced data analysis and ML.\"\ncategories: [\"Machine Learning\"]\nimage: /assets/images/pinot.jpeg\nformat:\n  html:\n    code-tools: true\n    code-fold: false\n    toc: true\n    toc-location: right\n    html-math-method: katex\n    page-layout: article\nexecute:\n  warning: false\n  message: false\nai-summary:\n  banner-title: \"Yapper Labs | AI Summary\"\n  model-title: \"Model: ChatGPT 4.5\"\n  model-img: \"/assets/images/OpenAI-white-monoblossom.svg\"\n  summary: \"I developed a predictive Random Forest model to classify the province of wine origin using critic-provided descriptions, utilizing advanced text preprocessing and feature engineering techniques such as tokenization, stemming, and stop-word removal. Through hyperparameter tuning and cross-validation, I achieved a strong Kappa score of 82%, indicating high accuracy and reliability in predicting wine origins based on sensory descriptors. The project demonstrates my proficiency in NLP, machine learning model building, and rigorous evaluation methodologies.\"\n---\n\n\n\n\n\n\n\n![](/assets/images/pinot.jpeg)\n\n## Purpose\n\nThe purpose of this project was to develop a predictive model for identifying the province of origin for wines based on descriptions provided by critics. To achieve this goal, a random forest model was built and evaluated for its performance, achieving a kappa score of 0.82. This project aimed to provide a useful tool for wine connoisseurs and industry professionals in identifying the origin of wines based on their sensory characteristics.\n\n## Setup\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)\n```\n:::\n\n\n\n\n## Feature Engineering\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine = read_rds(\"../../../assets/datasets/pinot.rds\") \n\nwine_words <- function(df, j, stem = T){ \n  data(stop_words)\n  words <- df %>%\n    unnest_tokens(word, description) %>%\n    anti_join(stop_words) %>%\n    filter(str_detect(string = word, pattern = \"[a-z+]\")) %>% # get rid weird non alphas \n    filter(str_length(word) >= 3) %>% # get rid of strings shorter than 3 characters \n    filter(!(word %in% c(\"wine\",\"pinot\", \"vineyard\"))) %>%\n    group_by(word) %>%\n    mutate(total=n()) %>%\n    ungroup()\n  \n  if(stem){\n    words <- words %>% \n      mutate(word = wordStem(word))\n  }\n  \n  words <- words %>% \n    count(id, word) %>% \n    group_by(id) %>% \n    mutate(exists = (n>0)) %>% \n    ungroup %>% \n    group_by(word) %>% \n    mutate(total = sum(n)) %>% \n    filter(total > j) %>% \n    pivot_wider(id_cols = id,\n                names_from = word,\n                values_from = exists,\n                values_fill = list(exists=0)) %>% \n    right_join(select(df,id,province)) %>% \n    select(-id) %>% \n    mutate(across(-province, ~replace_na(.x, F)))\n}\n\nwino <- wine_words(wine, j = 190, stem = T)\n```\n:::\n\n\n\n\n## Specification\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(504) \n\nctrl <- trainControl(method = \"cv\", number = 3)\n\n\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\ntest <- wino[-wine_index, ]\n\nfit <- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 100,\n             tuneLength = 15,\n             nodesize = 10,\n             verbose = TRUE,\n             trControl = ctrl,\n             metric = \"Kappa\")\n```\n:::\n\n\n\n\n## Model Performance\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(predict(fit, test),factor(test$province))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               216          5                 0           0        1\n  California              13        758                14          23       19\n  Casablanca_Valley        0          0                10           0        0\n  Marlborough              0          0                 0          12        0\n  New_York                 0          0                 0           0        0\n  Oregon                   9         28                 2          10        6\n                   Reference\nPrediction          Oregon\n  Burgundy              10\n  California            62\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               475\n\nOverall Statistics\n                                          \n               Accuracy : 0.8793          \n                 95% CI : (0.8627, 0.8945)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8069          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9076            0.9583                 0.384615\nSpecificity                   0.9889            0.8515                 1.000000\nPos Pred Value                0.9310            0.8526                 1.000000\nNeg Pred Value                0.9847            0.9579                 0.990379\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1291            0.4531                 0.005977\nDetection Prevalence          0.1387            0.5314                 0.005977\nBalanced Accuracy             0.9482            0.9049                 0.692308\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.266667         0.00000        0.8684\nSpecificity                    1.000000         1.00000        0.9512\nPos Pred Value                 1.000000             NaN        0.8962\nNeg Pred Value                 0.980132         0.98446        0.9370\nPrevalence                     0.026898         0.01554        0.3270\nDetection Rate                 0.007173         0.00000        0.2839\nDetection Prevalence           0.007173         0.00000        0.3168\nBalanced Accuracy              0.633333         0.50000        0.9098\n```\n\n\n:::\n:::\n\n\n\n\n## Re-fit and evaluation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1504)\n\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\ntest <- wino[-wine_index, ]\n\n# example spec for knn\nfit_final <- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             tuneGrid = fit$bestTune) \n# The last line means we will fit a model using the best tune parameters your CV found above.\n```\n:::\n\n\n\n\n## Final Model Performance\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(predict(fit_final, test),factor(test$province))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               219          7                 0           0        0\n  California               7        752                12          13       20\n  Casablanca_Valley        0          0                12           0        0\n  Marlborough              0          1                 0          22        0\n  New_York                 0          0                 0           0        1\n  Oregon                  12         31                 2          10        5\n                   Reference\nPrediction          Oregon\n  Burgundy               7\n  California            55\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               485\n\nOverall Statistics\n                                          \n               Accuracy : 0.8912          \n                 95% CI : (0.8753, 0.9057)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8274          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9202            0.9507                 0.461538\nSpecificity                   0.9902            0.8787                 1.000000\nPos Pred Value                0.9399            0.8754                 1.000000\nNeg Pred Value                0.9868            0.9521                 0.991571\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1309            0.4495                 0.007173\nDetection Prevalence          0.1393            0.5134                 0.007173\nBalanced Accuracy             0.9552            0.9147                 0.730769\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                     0.48889       0.0384615        0.8867\nSpecificity                     0.99939       1.0000000        0.9467\nPos Pred Value                  0.95652       1.0000000        0.8899\nNeg Pred Value                  0.98606       0.9850478        0.9450\nPrevalence                      0.02690       0.0155409        0.3270\nDetection Rate                  0.01315       0.0005977        0.2899\nDetection Prevalence            0.01375       0.0005977        0.3258\nBalanced Accuracy               0.74414       0.5192308        0.9167\n```\n\n\n:::\n:::\n\n\n\n\n## Conclusion\n\nThe kappa value of 0.82 for our random forest model signifies a high level of precision and accuracy, reflecting a very good agreement with the actual outcomes. This statistical measure, important for assessing classification model performance, confirms the model's efficacy in predicting the correct class labels. Thus, with a kappa value of 0.82, the model demonstrates reliable predictive performance.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}