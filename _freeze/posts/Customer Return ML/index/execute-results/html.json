{
  "hash": "b723b909c9fc0e9c1a7b0c86ac8db99d",
  "result": {
    "markdown": "---\ntitle: \"Predicting Customer Returns\"\nauthor: Brian Cervantes Alvarez\ndate: \"3-11-2023\"\nimage: return.jpeg\nformat:\n  html:\n    toc: true\n    toc-location: right\n    html-math-method: katex\n    page-layout: full\nexecute: \n  warning: false\n  message: false\ncategories: [R, Machine Learning, Random Forest]\n---\n\n\n![](return.jpeg)\n\n## Purpose\n\nIn this evaluation, I scrutinized a partially randomly generated customer returns dataset with the objective of forecasting whether a customer would opt to return their purchased item. By employing progressive techniques that advanced from Logistical to Random Forest, I achieved a robust prediction of customer behavior with respect to product returns.\n\n## Approach and Methodology\n\nTo ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as \"Season\", \"CustomerAge\", \"MSRP\", and \"PriceRange\", that may play a significant role in predicting a customer's probability of returning their product. I ran a Random Forest model and achieved an AUC of 0.625. This indicates moderate predictive power for the model, and suggests that further feature engineering or model tuning may be necessary for better performance.\n\nInitially, I utilized a logistic regression model, however, it required further refinement and tuning. Due to a three-hour time constraint, I focused on building a draft model. While not suitable for production, it is a step towards the right direction.\n\n## Load the required packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)\n```\n:::\n\n\n## Load the training and test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- read_csv(\"train.csv\") \ntest <- read_csv(\"test.csv\") \n\nglimpse(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…\n```\n:::\n:::\n\n\n## Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n:::\n\n\n## Feature Engineering\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 56, 56, 56, 56, 44, 44, 44, 56, 56, 56, 58, 58, 39, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n```\n:::\n\n```{.r .cell-code}\nsummary(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :26.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:36.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :47.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :48.54  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:60.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :77.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n```\n:::\n\n```{.r .cell-code}\ntable(train$Returned)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   No   Yes \n42035 22877 \n```\n:::\n:::\n\n\n## Fit a Random Forest Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(345)\n\n#Model using Random Forest \nctrl <- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)\nfit <- train(Returned ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 50,\n             tuneLength = 3,\n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest \n\n64912 samples\n   10 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 43274, 43275, 43275 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5560616  1.0000000  0.0000000\n  36    0.6252121  0.8329724  0.3620666\n  70    0.6253726  0.8273818  0.3716833\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 70.\n```\n:::\n:::\n\n\n## Make prediction on the test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntestPredictions <- predict(fit, newdata = test, type = \"prob\")[,2]\n```\n:::\n\n\n## Writing the Submission File\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\nwrite.csv(submission, \"submission.csv\", row.names = FALSE)\n```\n:::\n\n\n## Leftout Features that were considered\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}