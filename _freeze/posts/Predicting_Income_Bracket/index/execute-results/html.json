{
  "hash": "414621898dd02fe5ec0aaf97c48d5be3",
  "result": {
    "markdown": "---\ntitle: \"Predicting Individuals Who Earn More Than 50K\"\nauthor:  \n  - \"Brian Cervantes Alvarez\"\n  - \"Willa Van Liew\"\ndate: \"04-11-2023\"\nimage: income.jpeg\nformat:\n  html:\n    page-layout: full\n    toc: true\n    toc-location: right\n    html-math-method: katex\noutput: html_document\nexecute:\n  message: false\n  warning: false\ncategories: [R, machine learning, gradient boosting machine, logistic regression, stats]\n---\n\n\n![](income.jpeg)\n\n## Purpose\n\nIn this project, we conducted a comprehensive machine learning analysis, encompassing various stages of the data science workflow, including data preprocessing, feature engineering, and feature selection through PCA. Collaboratively, we sought to predict whether individuals had an income greater than 50K. By incorporating hyperparameter optimization and deploying a robust model pipeline, we achieved a final Kappa score of 0.9543. This project enabled us to hone our skills as aspiring Data Scientists and paves the way for future machine learning endeavors.\n\n## Highlight Of The Project\n\nWilla was instrumental in laying the foundation for the models and identifying the PCA features. Without her exceptional work, we would not have been able to create such a comprehensive and outstanding model for predicting income above \\$50,000. Her valuable contributions provided a critical starting point for our team, enabling us to add the feature importance at the start and optimize the models to their best parameters. Willa's dedication, skill, and hard work were truly impressive, and I feel grateful to have had her as a partner on this project.\n\nHere is the direct link to her [website](https://willavanliew.netlify.app/). Take a look at her data science projects!\n\n## Predicting Income >50K\n\n### Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(caret)\nlibrary(tidymodels)\nlibrary(fastDummies)\nlibrary(randomForest)\n```\n:::\n\n\n### Load The Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_income = read_csv(\"openml_1590.csv\", na = c(\"?\")) %>%\n  mutate(income_above_50k = ifelse(class == \">50K\",1,0))\n\nincome = read_csv(\"openml_1590.csv\", na = c(\"?\")) %>%\n  drop_na() %>%\n  mutate(income_above_50K = ifelse(class == \">50K\",1,0)) %>%\n  select(-class) %>%\n  dummy_cols(remove_selected_columns = T)\n```\n:::\n\n\n## Run Random Forest & Obtain Importance Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(504)\nraw_index <- createDataPartition(income$income_above_50K, p = 0.8, list = FALSE)\ntrain <- income[raw_index,]\ntest  <- income[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 3)\n\nfit <- train(income_above_50K ~ .,\n            data = train, \n            method = \"rf\",\n            ntree = 50,\n            tuneLength = 3,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest \n\n36178 samples\n  104 predictor\n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 24118, 24119, 24119 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n    2   0.3476024  0.4097953  0.2772094\n   53   0.3179101  0.4613186  0.1913666\n  104   0.3202068  0.4549034  0.1912386\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 53.\n```\n:::\n\n```{.r .cell-code}\nprint(varImp(fit), 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrf variable importance\n\n  only 10 most important variables shown (out of 104)\n\n                                    Overall\n`marital-status_Married-civ-spouse` 100.000\nfnlwgt                               88.391\n`capital-gain`                       72.132\nage                                  65.934\n`education-num`                      65.048\n`hours-per-week`                     38.148\nrelationship_Husband                 23.865\n`capital-loss`                       22.797\n`occupation_Exec-managerial`          8.631\n`occupation_Prof-specialty`           5.868\n```\n:::\n:::\n\n\n## PCA\n\n### Chose Top 8 Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninc <- income %>%\n  select(-c(fnlwgt,\n            `marital-status_Married-civ-spouse`,\n            age,\n            `capital-gain`,\n            `education-num`,\n            `hours-per-week`,\n            relationship_Husband,\n            `capital-loss`))\n\n#Remained unchanged\npr_income = prcomp(x = inc, scale=T, center = T)\nscreeplot(pr_income, type=\"lines\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrownames_to_column(as.data.frame(pr_income$rotation)) %>%\n  select(1:11) %>%\n  filter(abs(PC1) >= 0.35 | abs(PC2) >= 0.35 | abs(PC3) >= 0.35 | abs(PC4) >= 0.35 | abs(PC5) >= 0.35 | abs(PC6) >= 0.35 | abs(PC7) >= 0.35 | abs(PC8) >= 0.35 | abs(PC9) >= 0.35 | abs(PC10) >= 0.35)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       rowname         PC1         PC2          PC3         PC4\n1            workclass_Private -0.17526595 -0.13547253  0.304797799  0.02401763\n2   workclass_Self-emp-not-inc  0.14801244  0.01168218 -0.067006960  0.07757365\n3       education_Some-college -0.06530044  0.06007649  0.046053650 -0.12180499\n4   relationship_Not-in-family -0.11626278  0.09345537 -0.009681895  0.02508551\n5            relationship_Wife -0.07643452  0.10243694 -0.155839801  0.10495583\n6                   race_Black -0.20749199 -0.06347990 -0.071867670 -0.26136856\n7                   sex_Female -0.43938132  0.23499209 -0.149918556  0.12969011\n8                     sex_Male  0.43938132 -0.23499209  0.149918556 -0.12969011\n9 native-country_United-States  0.08714546  0.46368406  0.199639060 -0.22938060\n          PC5          PC6         PC7         PC8         PC9        PC10\n1 -0.20886710  0.441798469 -0.22645965 -0.03914163 -0.04681505  0.00291428\n2  0.14138682 -0.254839748  0.35058106  0.05445338 -0.03103841 -0.33625026\n3 -0.07546707 -0.063088752  0.11632314  0.19569918  0.43973405  0.15884684\n4 -0.12317719 -0.049422098 -0.03404059 -0.57428375  0.20605726 -0.14712496\n5 -0.06292606  0.110184232  0.03508095  0.39257178 -0.25791740  0.12888672\n6  0.37153500 -0.037152844 -0.32702635  0.10727161  0.06843460 -0.15897839\n7 -0.10696874  0.005407197  0.07370032  0.08521373 -0.09415713 -0.03364547\n8  0.10696874 -0.005407197 -0.07370032 -0.08521373  0.09415713  0.03364547\n9  0.15037231  0.067618017 -0.02636173 -0.01036183 -0.06064057 -0.09505529\n```\n:::\n:::\n\n\n### Chose First 10 PCA Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# IMPORTANT: Since I used 8 features, I updated the prc dataframe to include\n# the features + PCA 1-10\nprc <- \n  bind_cols(select(income, \n                   c(fnlwgt, \n                    `marital-status_Married-civ-spouse`, \n                    age, \n                    `capital-gain`, \n                    age, \n                    `hours-per-week`, \n                    relationship_Husband,\n                    `capital-loss`,\n                    income_above_50K)\n                   ), \n            as.data.frame(pr_income$x)\n            ) %>%\n  select(1:18) %>%\n  ungroup() %>%\n  rename(\"NonBlack_Men\" = PC1,\n         \"US_Women\" = PC2,\n         \"PrivateSec_Men\" = PC3,\n         \"NonUS_NonBlack\" = PC4,\n         \"NonPrivateSec_Black\" = PC5,\n         \"PrivateSec\" = PC6,\n         \"NonBlack_SelfEmploy\" = PC7,\n         \"Wives\" = PC8,\n         \"NonFamily_SomeCollege\" = PC9,\n         \"NotSelfEmployes_NonBlack\" = PC10)\n\nhead(prc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 18\n  fnlwgt marital-status_Married-civ-spou…¹   age `capital-gain` `hours-per-week`\n   <dbl>                             <int> <dbl>          <dbl>            <dbl>\n1 226802                                 0    25              0               40\n2  89814                                 1    38              0               50\n3 336951                                 1    28              0               40\n4 160323                                 1    44           7688               40\n5 198693                                 0    34              0               30\n6 104626                                 1    63           3103               32\n# ℹ abbreviated name: ¹​`marital-status_Married-civ-spouse`\n# ℹ 13 more variables: relationship_Husband <int>, `capital-loss` <dbl>,\n#   income_above_50K <dbl>, NonBlack_Men <dbl>, US_Women <dbl>,\n#   PrivateSec_Men <dbl>, NonUS_NonBlack <dbl>, NonPrivateSec_Black <dbl>,\n#   PrivateSec <dbl>, NonBlack_SelfEmploy <dbl>, Wives <dbl>,\n#   NonFamily_SomeCollege <dbl>, NotSelfEmployes_NonBlack <dbl>\n```\n:::\n:::\n\n\n## Gradient Boosting Machine\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#IMPORTANT: I took a while and messed around with the hyperparameters\n# Went From 0.2 Kappa to 0.6 Kappa BEFORE updating the features.\n# After updating to the top 8 features + PCA 1-5, it jumped to \n# 0.88 Kappa. Then I added PCA 1-10 and it jumped to 0.95 for the Kappa!\nset.seed(504)\nraw_index <- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain <- prc[raw_index,]\ntest  <- prc[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 5)\nweights <- ifelse(income$income_above_50K == 1, 75, 25)\n\nhyperparameters <- expand.grid(interaction.depth = 9, \n                    n.trees = 300, \n                    shrinkage = 0.1, \n                    n.minobsinnode = 4)\nfit <- train(factor(income_above_50K) ~ .,\n            data = train, \n            method = \"gbm\",\n            verbose = FALSE,\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStochastic Gradient Boosting \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9827796  0.9533806\n\nTuning parameter 'n.trees' was held constant at a value of 300\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 4\n```\n:::\n:::\n\n\n### Confusion Matrix For GBM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6798  111\n         1   42 2093\n                                          \n               Accuracy : 0.9831          \n                 95% CI : (0.9802, 0.9856)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9536          \n                                          \n Mcnemar's Test P-Value : 3.853e-08       \n                                          \n            Sensitivity : 0.9939          \n            Specificity : 0.9496          \n         Pos Pred Value : 0.9839          \n         Neg Pred Value : 0.9803          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7517          \n   Detection Prevalence : 0.7639          \n      Balanced Accuracy : 0.9717          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n## Logistical Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#I messed around with using a logistical model\n#It turns out that it's pretty good too! Not as great as the GBM\n#But a great and easy model to explain!\n\nset.seed(504)\nraw_index <- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain <- prc[raw_index,]\ntest  <- prc[-raw_index, ]\nctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 3, verboseIter = FALSE)\nhyperparameters <- expand.grid(alpha = 1, \n                               lambda = 0.001)\n\nfit <- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"glmnet\",\n            family = \"binomial\",\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\",\n            importance = TRUE)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nglmnet \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 32560, 32561, 32561, 32561, 32559, 32559, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.9610166  0.895033\n\nTuning parameter 'alpha' was held constant at a value of 1\nTuning\n parameter 'lambda' was held constant at a value of 0.001\n```\n:::\n:::\n\n\n### Confusion Matrix For Logistical Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6679  197\n         1  161 2007\n                                          \n               Accuracy : 0.9604          \n                 95% CI : (0.9562, 0.9643)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.892           \n                                          \n Mcnemar's Test P-Value : 0.06434         \n                                          \n            Sensitivity : 0.9765          \n            Specificity : 0.9106          \n         Pos Pred Value : 0.9713          \n         Neg Pred Value : 0.9257          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7385          \n   Detection Prevalence : 0.7603          \n      Balanced Accuracy : 0.9435          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n## KMeans Clustering\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkclust <- kmeans(na.omit(prc), centers = 4)\nkclust$centers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    fnlwgt marital-status_Married-civ-spouse      age capital-gain\n1  86931.3                         0.4798800 39.36223     1178.652\n2 307822.2                         0.4480979 37.29197      993.978\n3 486698.8                         0.4317060 35.22655     1093.921\n4 188848.4                         0.4654970 38.77250     1090.494\n  hours-per-week relationship_Husband capital-loss income_above_50K\n1       41.21239            0.4228618     85.99175        0.2453963\n2       40.99760            0.4015361     87.95284        0.2414497\n3       40.18760            0.3812397     69.68404        0.2144816\n4       40.78357            0.4129092     92.41845        0.2551951\n  NonBlack_Men    US_Women PrivateSec_Men NonUS_NonBlack NonPrivateSec_Black\n1   0.06956303  0.10628238  -0.0890912042    -0.03258891        -0.008490584\n2  -0.04531689 -0.15055893   0.0006150531    -0.02235042         0.014458490\n3  -0.20329023 -0.44646489   0.1964744251     0.04160186         0.115358306\n4  -0.01331648  0.02500494   0.0462143267     0.02882888        -0.010110363\n   PrivateSec NonBlack_SelfEmploy        Wives NonFamily_SomeCollege\n1  0.03384552          0.17173996 -0.029902238           -0.02188199\n2 -0.07527701         -0.12068976 -0.008023897            0.09248884\n3 -0.26626813         -0.36982627  0.123605508            0.13942943\n4  0.03021208         -0.04107775  0.013720687           -0.03450577\n  NotSelfEmployes_NonBlack\n1              -0.03597294\n2               0.01302787\n3              -0.02812971\n4               0.02304227\n```\n:::\n\n```{.r .cell-code}\nkclusts <- tibble(k = 1:9) %>%\n  mutate(\n    kclust = map(k, ~kmeans(prc, .x)),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, prc)\n  )\n\nclusterings <- kclusts %>%\n  unnest(glanced, .drop = TRUE)\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Augumenting The GBM Model with KMeans Clustering\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprc2 <- augment(kclust, prc)\n\nset.seed(504)\nraw_index <- createDataPartition(prc2$income_above_50K, p = 0.8, list = FALSE)\n\ntrain <- prc2[raw_index,]\ntest  <- prc2[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 5)\n\nhyperparameters <- expand.grid(\n  n.trees = 500,\n  interaction.depth = 5,\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n\n\nfit <- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"gbm\",\n            trControl = ctrl,\n            tuneGrid = hyperparameters,\n            verbose = FALSE)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStochastic Gradient Boosting \n\n36178 samples\n   18 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9836365  0.9557243\n\nTuning parameter 'n.trees' was held constant at a value of 500\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\n```\n:::\n:::\n\n\n### Confusion Matrix For KMeans + GBM\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#We should be getting a Kappa of 0.9543!\n#Sensitivity = 0.9930, Specificity = 0.9533\n#Excellent Numbers!\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6792  103\n         1   48 2101\n                                          \n               Accuracy : 0.9833          \n                 95% CI : (0.9804, 0.9858)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9543          \n                                          \n Mcnemar's Test P-Value : 1.11e-05        \n                                          \n            Sensitivity : 0.9930          \n            Specificity : 0.9533          \n         Pos Pred Value : 0.9851          \n         Neg Pred Value : 0.9777          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7510          \n   Detection Prevalence : 0.7624          \n      Balanced Accuracy : 0.9731          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n## Results\n\nWe used a random forest model to identify the most important variables and selected the top 8 features, then utilized principal component analysis (PCA) to further analyze the data. Our final model incorporated a gradient boosting machine (GBM) algorithm with optimized hyperparameters and achieved an accuracy rate of 0.9829731 and a Kappa score of 0.9538928. To ensure the model was not overfitting, we benchmarked it with a logistic regression model that achieved a Kappa score of 0.895033 and an accuracy rate of 0.9610166. We further improved the model's accuracy by incorporating unsupervised machine learning with Kmeans clustering, achieving a Kappa score of 0.9543 and an accuracy rate of 0.9833. Overall, our approach of feature selection and PCA was effective and could be applied to future data analysis projects with some additional fine-tuning.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}