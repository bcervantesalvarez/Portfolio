{
  "hash": "5d8561f40a206d9afbd001d10c83770b",
  "result": {
    "markdown": "---\ntitle: 'Boosting Fraud Detection: Enhancing Accuracy with GBM'\nauthor: Brian Cervantes Alvarez\ndate: 5-30-2023\ndescription: 'I developed a model that enhances fraud detection in this dataset, contributing to ongoing advancements in this challenging task'\nimage: fraud.png\nformat:\n  html:\n    toc: true\n    toc-location: right\n    html-math-method: katex\n    page-layout: full\nexecute:\n  warning: false\n  message: false\ncategories:\n  - Python\n  - R\n  - Machine Learning\n  - Gradient Boosting Machine\n---\n\n![](fraud.png)\n\n\n<div class=\"download-buttons\" style=\"text-align: center;\">\n  <style>\n    .download-buttons a.button {\n      display: inline-block;\n      padding-right: 10px;\n      padding-left: 35px;\n      padding-top: 10px;\n      padding-bottom: 10px;\n      margin-right: 25px;\n      background-color: #343a40;\n      color: #fff;\n      text-decoration: none;\n      border-radius: 15px;\n      position: relative;\n    }\n\n    .download-buttons a.button:last-child {\n      margin-right: 0;\n    }\n\n    .download-buttons a.button:hover {\n      background-color: #55595c;\n    }\n\n    .download-buttons a.button:after {\n      content: \"\\2193\";\n      font-size: 20px;\n      position: absolute;\n      top: 50%;\n      left: 12px; /* Added margin from the text */\n      transform: translateY(-50%);\n      border-bottom: 2px solid #fff;\n      padding: 1px; /* Padding for the mini border block */\n    }\n  </style>\n\n  <script>\n    function createDynamicButton(href, text) {\n      var button = document.createElement(\"a\");\n      button.href = href;\n      button.className = \"button\";\n      button.appendChild(document.createTextNode(text));\n      return button;\n    }\n\n    var downloadButtonsDiv = document.querySelector(\".download-buttons\");\n\n    var documentButton = createDynamicButton(\n      \"https://github.com/bcervantesalvarez/Portfolio/blob/main/posts/FraudML/index.qmd\",\n      \"Download .qmd\"\n    );\n    downloadButtonsDiv.appendChild(documentButton);\n\n    var csvButton = createDynamicButton(\n      \"https://www.kaggle.com/datasets/harithapliyal/card-transaction-log/download?datasetVersionNumber=1\",\n      \"Download .zip\"\n    );\n    downloadButtonsDiv.appendChild(csvButton);\n  </script>\n</div>\n\n\n## Abstract\n\nFraud detection is a challenging task, but advancements are being made each year. This project aims to improve fraud detection using a Gradient Boosting Machine (GBM) model. The dataset is prepared, and the model is trained and evaluated on validation and test sets. Performance metrics such as accuracy, precision, recall, AUC, and Kappa coefficient are computed. A comparison is made with the existing 'isFlaggedFraud' column model. The GBM model demonstrates higher performance in various aspects, highlighting its superiority in classifying fraudulent transactions. However, further improvements are needed to enhance fraud detection while maintaining precision.\n\n## Introduction\n\nFraudulent activities pose a significant threat in various industries, including financial transactions. Detecting fraudulent transactions is a complex task, but advancements in machine learning techniques have shown promise in improving accuracy and precision. In this project, we focus on enhancing fraud detection using a Gradient Boosting Machine (GBM) model.\n\n## Methodology \n\nFirst, we set up the environment by importing necessary libraries and loading the dataset. The dataset, named 'Card-Transaction_log.csv', contains relevant information for training and evaluation. We explore the dataset by printing the first few rows and examining its shape.\n\nNext, we perform data wrangling by applying one-hot encoding to the features, excluding 'isFraud', 'nameOrig', 'nameDest', and 'isFlaggedFraud' columns. This process prepares the data for training the GBM model.\n\nTo evaluate the model's performance, we split the data into training, validation, and test sets. The random seed is set for reproducibility. The GBM model is trained on the training set using the specified parameters, such as loss function, learning rate, number of estimators, and maximum depth. The model's training time is recorded for analysis.\n\nWe then evaluate the GBM model on the validation set to measure its accuracy, precision, recall, confusion matrix, AUC, and Kappa coefficient. These metrics provide insights into the model's performance in classifying fraudulent and non-fraudulent transactions.\n\nFurther, we assess the GBM model's performance on the test set and compute the corresponding metrics. The test accuracy, precision, recall, confusion matrix, AUC, and Kappa coefficient provide a comprehensive evaluation of the model's ability to classify transactions accurately.\n\nTo compare the GBM model with the existing 'isFlaggedFraud' column model, we calculate the accuracy, precision, and recall scores for the 'isFlaggedFraud' column. This analysis allows us to assess the GBM model's superiority in detecting fraudulent transactions.\n\nBased on the results, we discuss the GBM model's performance, highlighting its accuracy, precision, recall, AUC, and Kappa coefficient. We analyze the confusion matrix to identify areas where the model misclassifies transactions. While the GBM model outperforms the 'isFlaggedFraud' column model in several aspects, it still has room for improvement in detecting fraudulent instances while maintaining high precision.\n\n\n::: {.callout-caution}\n## Caution\n\nEnsure you are using the correct Python version: Python 3.10.9 ('base') ~/anaconda3/bin/python\n:::\n\n\n### Set up\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.ensemble import GradientBoostingClassifier  # For classification tasks\nfrom sklearn.ensemble import GradientBoostingRegressor  # For regression tasks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n    roc_curve,\n    cohen_kappa_score,\n)\n\nds = pd.read_csv('Card-Transaction_log.csv')\n\n# Print the first 5 rows\nprint(ds.head(5))\n\n# Print the shape of the frame\nprint(ds.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n\n      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n0  M1979787155             0.0             0.0        0               0  \n1  M2044282225             0.0             0.0        0               0  \n2   C553264065             0.0             0.0        1               0  \n3    C38997010         21182.0             0.0        1               0  \n4  M1230701703             0.0             0.0        0               0  \n(6362620, 11)\n```\n:::\n:::\n\n\n### Data Wrangling \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nX = pd.get_dummies(ds.drop(['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1))   # Perform one-hot encoding on the features\ny = ds['isFraud']\n```\n:::\n\n\n::: {.callout-note}\n## Note\n\nThis dataset was already cleaned and tidy, so no additional wrangling was needed.\n:::\n\n### Split the data into training, validation, and test sets\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Set the random seed\nnp.random.seed(57)\n\n# Training split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n\n\n# Train the model\nstart_time = time.time()  # Start the timer\n\ngbm = GradientBoostingClassifier(loss='log_loss', learning_rate=0.1, n_estimators=100, max_depth=3)\ngbm.fit(X_train, y_train)\n\nend_time = time.time()  # Stop the timer\n\nelapsed_time = end_time - start_time\nprint(f\"Elapsed Time: {elapsed_time} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nElapsed Time: 1044.689668893814 seconds\n```\n:::\n:::\n\n\n### Evaluate on the validation set\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ny_pred_val = gbm.predict(X_val)\naccuracy_val = accuracy_score(y_val, y_pred_val)\nprecision_val = precision_score(y_val, y_pred_val)\nrecall_val = recall_score(y_val, y_pred_val)\nconfusion_matrix_val = confusion_matrix(y_val, y_pred_val)\nauc_val = roc_auc_score(y_val, y_pred_val)\nfpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_pred_val)\nkappa_val = cohen_kappa_score(y_val, y_pred_val)\n\nprint(f\"Validation Accuracy: {accuracy_val}\")\nprint(f\"Validation Precision: {precision_val}\")\nprint(f\"Validation Recall: {recall_val}\")\nprint(\"Validation Confusion Matrix:\")\nprint(confusion_matrix_val)\nprint(f\"Validation AUC: {auc_val}\")\nprint(f\"Validation Kappa: {kappa_val}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValidation Accuracy: 0.9993241777758219\nValidation Precision: 0.9693396226415094\nValidation Recall: 0.4963768115942029\nValidation Confusion Matrix:\n[[635421     13]\n [   417    411]]\nValidation AUC: 0.7481781765679447\nValidation Kappa: 0.6562465275067701\n```\n:::\n:::\n\n\n### Evaluate on the test set\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ny_pred_test = gbm.predict(X_test)\naccuracy_test = accuracy_score(y_test, y_pred_test)\nprecision_test = precision_score(y_test, y_pred_test)\nrecall_test = recall_score(y_test, y_pred_test)\nconfusion_matrix_test = confusion_matrix(y_test, y_pred_test)\nauc_test = roc_auc_score(y_test, y_pred_test)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_test)\nkappa_test = cohen_kappa_score(y_test, y_pred_test)\n\nprint(f\"Test Accuracy: {accuracy_test}\")\nprint(f\"Test Precision: {precision_test}\")\nprint(f\"Test Recall: {recall_test}\")\nprint(\"Test Confusion Matrix:\")\nprint(confusion_matrix_test)\nprint(f\"Test AUC: {auc_test}\")\nprint(f\"Test Kappa: {kappa_test}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 0.9992958875431819\nTest Precision: 0.9455958549222798\nTest Recall: 0.46085858585858586\nTest Confusion Matrix:\n[[635449     21]\n [   427    365]]\nTest AUC: 0.7304127697259946\nTest Kappa: 0.6193839067580158\n```\n:::\n:::\n\n\n### How does this compare to the current model?\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Comparison with 'isFlaggedFraud' column\nis_flagged_fraud_accuracy = accuracy_score(ds['isFraud'], ds['isFlaggedFraud'])\nis_flagged_fraud_precision = precision_score(ds['isFraud'], ds['isFlaggedFraud'])\nis_flagged_fraud_recall = recall_score(ds['isFraud'], ds['isFlaggedFraud'])\n\nprint(f\"Accuracy (isFlaggedFraud): {is_flagged_fraud_accuracy}\")\nprint(f\"Precision (isFlaggedFraud): {is_flagged_fraud_precision}\")\nprint(f\"Recall (isFlaggedFraud): {is_flagged_fraud_recall}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy (isFlaggedFraud): 0.9987116942391656\nPrecision (isFlaggedFraud): 1.0\nRecall (isFlaggedFraud): 0.0019481310118105442\n```\n:::\n:::\n\n\n## Results\n\nBased on the evaluation of the GBM (Gradient Boosting Machine) model on the test set, the following performance metrics were obtained:\n\n- Test Accuracy: 0.9992958875431819\n- Test Precision: 0.9455958549222798\n- Test Recall: 0.46085858585858586\n- Test AUC: 0.7304127697259946\n- Test Kappa: 0.6193839067580158\n\nThe GBM model achieved a high accuracy score of 0.999, indicating a strong ability to correctly classify transactions in the test set. With a precision of 0.946, the GBM model accurately identified approximately 94.6% of the predicted fraudulent transactions. However, the recall score of 0.461 suggests that the GBM model only captured around 46.1% of the actual fraudulent transactions.\n\nAnalyzing the confusion matrix, the GBM model correctly classified a large number of non-fraudulent transactions (true negatives) and a significant portion of fraudulent transactions (true positives). However, there were some instances where the GBM model misclassified non-fraudulent transactions as fraudulent (false positives) and failed to identify certain fraudulent transactions (false negatives).\n\nThe AUC score of 0.730 indicates a moderate level of discrimination between fraudulent and non-fraudulent transactions for the GBM model. Although the GBM model demonstrates substantial agreement beyond chance with a Kappa coefficient of 0.619, there is room for improvement in its ability to detect fraudulent instances while maintaining high precision.\n\n### Comparison with Original Model\n\nComparing the GBM model with the 'isFlaggedFraud' column model, the GBM model outperforms in several key aspects. It achieves higher accuracy, precision, recall, and AUC scores, indicating superior overall performance in classifying fraudulent transactions. The 'isFlaggedFraud' column model, although having perfect precision, only detects a very small number of actual fraud cases, resulting in low recall.\n\n## Limitations and Further Study\n\nIt is important to consider the specific goals and requirements of the application. While the GBM model provides better overall performance, it may have a higher number of false positives compared to the 'isFlaggedFraud' column model. Further enhancements can be made to improve the GBM model's ability to detect fraudulent transactions while maintaining high precision.\n\nThese results provide valuable insights into the GBM model's performance on unseen data, indicating its generalization capability. It is crucial to consider the test set performance as the final evaluation of the GBM model's effectiveness in real-world scenarios.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}