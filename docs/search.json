[
  {
    "objectID": "Content/About/index.html",
    "href": "Content/About/index.html",
    "title": "Brian Cervantes Alvarez",
    "section": "",
    "text": "Educational Background\n\nBachelor’s Degree: Mathematics at Linfield University\nMaster’s Degree: Data Science at Willamette University\nCurrent Goal: Master’s in Statistics at Oregon State University\nFuture Goal: Ph.D. in Statistics at Oregon State University\n\n\n\nExpertise and Goals\n\nHold advanced analytical skills in predictive modeling, time-to-event analysis, and data visualization\nAdept at communicating data-driven insights for actionable solutions\nPassionate about interactive data visualization and statistical research, with a focus on classification methods\nCommitted to continuous learning and adaptation in the rapidly evolving fields of data science and statistics\n\n\n\nProfessional Aspirations\n\nReady to leverage expertise in statistical analysis, data science, and interactive visualization\nEager to contribute value to the Department of Statistics at Oregon State University\nCommitted to addressing real-world complexities through innovative data-driven projects"
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#project-overview",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#project-overview",
    "title": "Predicting $50K+ Incomes",
    "section": "Project Overview",
    "text": "Project Overview\nIn this project, our goal was to predict whether individuals earn more than $50K using machine learning techniques. We engaged in a thorough data science process, from data preprocessing and feature engineering to selecting principal components through PCA and optimizing hyperparameters for our model. Our collaborative efforts led to a high-performing model with a Kappa score of 0.9543, enhancing our data science expertise and setting the stage for future projects."
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#team-contribution",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#team-contribution",
    "title": "Predicting $50K+ Incomes",
    "section": "Team Contribution",
    "text": "Team Contribution\nWilla played a pivotal role in developing the foundational models and pinpointing the key PCA features. Her outstanding efforts laid the groundwork for our high-quality model that accurately predicts incomes over $50,000. Willa’s expertise not only provided a solid base for our project but also enabled us to fine-tune our models for peak performance. I’m thankful for her invaluable input and dedication, which significantly contributed to the success of our work.\nExplore more of Willa’s data science work on her website."
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#predicting-income-50k",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#predicting-income-50k",
    "title": "Predicting $50K+ Incomes",
    "section": "Predicting Income >50K",
    "text": "Predicting Income &gt;50K\n\nLoad Libraries\n\n#knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(caret)\nlibrary(tidymodels)\nlibrary(fastDummies)\nlibrary(randomForest)\n\n\n\nLoad The Data\n\nincome = read_csv(\"../../../Assets/Datasets/openml_1590.csv\", na = c(\"?\")) %&gt;%\n  drop_na() %&gt;%\n  mutate(income_above_50K = ifelse(class == \"&gt;50K\",1,0)) %&gt;%\n  select(-class) %&gt;%\n  dummy_cols(remove_selected_columns = T)"
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#run-random-forest-obtain-importance-features",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#run-random-forest-obtain-importance-features",
    "title": "Predicting $50K+ Incomes",
    "section": "Run Random Forest & Obtain Importance Features",
    "text": "Run Random Forest & Obtain Importance Features\n\nset.seed(504)\nraw_index &lt;- createDataPartition(income$income_above_50K, p = 0.8, list = FALSE)\ntrain &lt;- income[raw_index,]\ntest  &lt;- income[-raw_index, ]\nctrl &lt;- trainControl(method = \"cv\", number = 3)\n\nfit &lt;- train(income_above_50K ~ .,\n            data = train, \n            method = \"rf\",\n            ntree = 50,\n            tuneLength = 3,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n\nRandom Forest \n\n36178 samples\n  104 predictor\n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 24118, 24119, 24119 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n    2   0.3476024  0.4097953  0.2772094\n   53   0.3179101  0.4613186  0.1913666\n  104   0.3202068  0.4549034  0.1912386\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 53.\n\nprint(varImp(fit), 10)\n\nrf variable importance\n\n  only 10 most important variables shown (out of 104)\n\n                                    Overall\n`marital-status_Married-civ-spouse` 100.000\nfnlwgt                               88.391\n`capital-gain`                       72.132\nage                                  65.934\n`education-num`                      65.048\n`hours-per-week`                     38.148\nrelationship_Husband                 23.865\n`capital-loss`                       22.797\n`occupation_Exec-managerial`          8.631\n`occupation_Prof-specialty`           5.868"
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#pca",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#pca",
    "title": "Predicting $50K+ Incomes",
    "section": "PCA",
    "text": "PCA\n\nChose Top 8 Features\n\ninc &lt;- income %&gt;%\n  select(-c(fnlwgt,\n            `marital-status_Married-civ-spouse`,\n            age,\n            `capital-gain`,\n            `education-num`,\n            `hours-per-week`,\n            relationship_Husband,\n            `capital-loss`))\n\n#Remained unchanged\npr_income = prcomp(x = inc, scale=T, center = T)\nscreeplot(pr_income, type=\"lines\")\n\n\n\n\n\n\n\nrownames_to_column(as.data.frame(pr_income$rotation)) %&gt;%\n  select(1:11) %&gt;%\n  filter(abs(PC1) &gt;= 0.35 | abs(PC2) &gt;= 0.35 | abs(PC3) &gt;= 0.35 | abs(PC4) &gt;= 0.35 | abs(PC5) &gt;= 0.35 | abs(PC6) &gt;= 0.35 | abs(PC7) &gt;= 0.35 | abs(PC8) &gt;= 0.35 | abs(PC9) &gt;= 0.35 | abs(PC10) &gt;= 0.35)\n\n                       rowname         PC1         PC2          PC3         PC4\n1            workclass_Private -0.17526595 -0.13547253  0.304797799  0.02401763\n2   workclass_Self-emp-not-inc  0.14801244  0.01168218 -0.067006960  0.07757365\n3       education_Some-college -0.06530044  0.06007649  0.046053650 -0.12180499\n4   relationship_Not-in-family -0.11626278  0.09345537 -0.009681895  0.02508551\n5            relationship_Wife -0.07643452  0.10243694 -0.155839801  0.10495583\n6                   race_Black -0.20749199 -0.06347990 -0.071867670 -0.26136856\n7                   sex_Female -0.43938132  0.23499209 -0.149918556  0.12969011\n8                     sex_Male  0.43938132 -0.23499209  0.149918556 -0.12969011\n9 native-country_United-States  0.08714546  0.46368406  0.199639060 -0.22938060\n          PC5          PC6         PC7         PC8         PC9        PC10\n1 -0.20886710  0.441798469 -0.22645965 -0.03914163 -0.04681505  0.00291428\n2  0.14138682 -0.254839748  0.35058106  0.05445338 -0.03103841 -0.33625026\n3 -0.07546707 -0.063088752  0.11632314  0.19569918  0.43973405  0.15884684\n4 -0.12317719 -0.049422098 -0.03404059 -0.57428375  0.20605726 -0.14712496\n5 -0.06292606  0.110184232  0.03508095  0.39257178 -0.25791740  0.12888672\n6  0.37153500 -0.037152844 -0.32702635  0.10727161  0.06843460 -0.15897839\n7 -0.10696874  0.005407197  0.07370032  0.08521373 -0.09415713 -0.03364547\n8  0.10696874 -0.005407197 -0.07370032 -0.08521373  0.09415713  0.03364547\n9  0.15037231  0.067618017 -0.02636173 -0.01036183 -0.06064057 -0.09505529\n\n\n\n\nChose First 10 PCA Features\n\n# IMPORTANT: Since I used 8 features, I updated the prc dataframe to include\n# the features + PCA 1-10\nprc &lt;- \n  bind_cols(select(income, \n                   c(fnlwgt, \n                    `marital-status_Married-civ-spouse`, \n                    age, \n                    `capital-gain`, \n                    age, \n                    `hours-per-week`, \n                    relationship_Husband,\n                    `capital-loss`,\n                    income_above_50K)\n                   ), \n            as.data.frame(pr_income$x)\n            ) %&gt;%\n  select(1:18) %&gt;%\n  ungroup() %&gt;%\n  rename(\"NonBlack_Men\" = PC1,\n         \"US_Women\" = PC2,\n         \"PrivateSec_Men\" = PC3,\n         \"NonUS_NonBlack\" = PC4,\n         \"NonPrivateSec_Black\" = PC5,\n         \"PrivateSec\" = PC6,\n         \"NonBlack_SelfEmploy\" = PC7,\n         \"Wives\" = PC8,\n         \"NonFamily_SomeCollege\" = PC9,\n         \"NotSelfEmployes_NonBlack\" = PC10)\n\nhead(prc)\n\n# A tibble: 6 × 18\n  fnlwgt marital-status_Married-civ-spou…¹   age `capital-gain` `hours-per-week`\n   &lt;dbl&gt;                             &lt;int&gt; &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1 226802                                 0    25              0               40\n2  89814                                 1    38              0               50\n3 336951                                 1    28              0               40\n4 160323                                 1    44           7688               40\n5 198693                                 0    34              0               30\n6 104626                                 1    63           3103               32\n# ℹ abbreviated name: ¹​`marital-status_Married-civ-spouse`\n# ℹ 13 more variables: relationship_Husband &lt;int&gt;, `capital-loss` &lt;dbl&gt;,\n#   income_above_50K &lt;dbl&gt;, NonBlack_Men &lt;dbl&gt;, US_Women &lt;dbl&gt;,\n#   PrivateSec_Men &lt;dbl&gt;, NonUS_NonBlack &lt;dbl&gt;, NonPrivateSec_Black &lt;dbl&gt;,\n#   PrivateSec &lt;dbl&gt;, NonBlack_SelfEmploy &lt;dbl&gt;, Wives &lt;dbl&gt;,\n#   NonFamily_SomeCollege &lt;dbl&gt;, NotSelfEmployes_NonBlack &lt;dbl&gt;"
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#gradient-boosting-machine",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#gradient-boosting-machine",
    "title": "Predicting $50K+ Incomes",
    "section": "Gradient Boosting Machine",
    "text": "Gradient Boosting Machine\n\n#IMPORTANT: I took a while and messed around with the hyperparameters\n# Went From 0.2 Kappa to 0.6 Kappa BEFORE updating the features.\n# After updating to the top 8 features + PCA 1-5, it jumped to \n# 0.88 Kappa. Then I added PCA 1-10 and it jumped to 0.95 for the Kappa!\nset.seed(504)\nraw_index &lt;- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain &lt;- prc[raw_index,]\ntest  &lt;- prc[-raw_index, ]\nctrl &lt;- trainControl(method = \"cv\", number = 5)\nweights &lt;- ifelse(income$income_above_50K == 1, 75, 25)\n\nhyperparameters &lt;- expand.grid(interaction.depth = 9, \n                    n.trees = 300, \n                    shrinkage = 0.1, \n                    n.minobsinnode = 4)\nfit &lt;- train(factor(income_above_50K) ~ .,\n            data = train, \n            method = \"gbm\",\n            verbose = FALSE,\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n\nStochastic Gradient Boosting \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9827796  0.9533806\n\nTuning parameter 'n.trees' was held constant at a value of 300\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 4\n\n\n\nConfusion Matrix For GBM\n\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6798  111\n         1   42 2093\n                                          \n               Accuracy : 0.9831          \n                 95% CI : (0.9802, 0.9856)\n    No Information Rate : 0.7563          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9536          \n                                          \n Mcnemar's Test P-Value : 3.853e-08       \n                                          \n            Sensitivity : 0.9939          \n            Specificity : 0.9496          \n         Pos Pred Value : 0.9839          \n         Neg Pred Value : 0.9803          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7517          \n   Detection Prevalence : 0.7639          \n      Balanced Accuracy : 0.9717          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#logistical-model",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#logistical-model",
    "title": "Predicting $50K+ Incomes",
    "section": "Logistical Model",
    "text": "Logistical Model\n\n#I messed around with using a logistical model\n#It turns out that it's pretty good too! Not as great as the GBM\n#But a great and easy model to explain!\n\nset.seed(504)\nraw_index &lt;- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain &lt;- prc[raw_index,]\ntest  &lt;- prc[-raw_index, ]\nctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3, verboseIter = FALSE)\nhyperparameters &lt;- expand.grid(alpha = 1, \n                               lambda = 0.001)\n\nfit &lt;- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"glmnet\",\n            family = \"binomial\",\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\",\n            importance = TRUE)\nfit\n\nglmnet \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 32560, 32561, 32561, 32561, 32559, 32559, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.9610166  0.895033\n\nTuning parameter 'alpha' was held constant at a value of 1\nTuning\n parameter 'lambda' was held constant at a value of 0.001\n\n\n\nConfusion Matrix For Logistical Regression\n\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6679  197\n         1  161 2007\n                                          \n               Accuracy : 0.9604          \n                 95% CI : (0.9562, 0.9643)\n    No Information Rate : 0.7563          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.892           \n                                          \n Mcnemar's Test P-Value : 0.06434         \n                                          \n            Sensitivity : 0.9765          \n            Specificity : 0.9106          \n         Pos Pred Value : 0.9713          \n         Neg Pred Value : 0.9257          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7385          \n   Detection Prevalence : 0.7603          \n      Balanced Accuracy : 0.9435          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#kmeans-clustering",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#kmeans-clustering",
    "title": "Predicting $50K+ Incomes",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\n\nkclust &lt;- kmeans(na.omit(prc), centers = 4)\nkclust$centers\n\n    fnlwgt marital-status_Married-civ-spouse      age capital-gain\n1  86931.3                         0.4798800 39.36223     1178.652\n2 307822.2                         0.4480979 37.29197      993.978\n3 486698.8                         0.4317060 35.22655     1093.921\n4 188848.4                         0.4654970 38.77250     1090.494\n  hours-per-week relationship_Husband capital-loss income_above_50K\n1       41.21239            0.4228618     85.99175        0.2453963\n2       40.99760            0.4015361     87.95284        0.2414497\n3       40.18760            0.3812397     69.68404        0.2144816\n4       40.78357            0.4129092     92.41845        0.2551951\n  NonBlack_Men    US_Women PrivateSec_Men NonUS_NonBlack NonPrivateSec_Black\n1   0.06956303  0.10628238  -0.0890912042    -0.03258891        -0.008490584\n2  -0.04531689 -0.15055893   0.0006150531    -0.02235042         0.014458490\n3  -0.20329023 -0.44646489   0.1964744251     0.04160186         0.115358306\n4  -0.01331648  0.02500494   0.0462143267     0.02882888        -0.010110363\n   PrivateSec NonBlack_SelfEmploy        Wives NonFamily_SomeCollege\n1  0.03384552          0.17173996 -0.029902238           -0.02188199\n2 -0.07527701         -0.12068976 -0.008023897            0.09248884\n3 -0.26626813         -0.36982627  0.123605508            0.13942943\n4  0.03021208         -0.04107775  0.013720687           -0.03450577\n  NotSelfEmployes_NonBlack\n1              -0.03597294\n2               0.01302787\n3              -0.02812971\n4               0.02304227\n\nkclusts &lt;- tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(prc, .x)),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, prc)\n  )\n\nclusterings &lt;- kclusts %&gt;%\n  unnest(glanced, .drop = TRUE)\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line()"
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#augumenting-the-gbm-model-with-kmeans-clustering",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#augumenting-the-gbm-model-with-kmeans-clustering",
    "title": "Predicting $50K+ Incomes",
    "section": "Augumenting The GBM Model with KMeans Clustering",
    "text": "Augumenting The GBM Model with KMeans Clustering\n\nprc2 &lt;- augment(kclust, prc)\n\nset.seed(504)\nraw_index &lt;- createDataPartition(prc2$income_above_50K, p = 0.8, list = FALSE)\n\ntrain &lt;- prc2[raw_index,]\ntest  &lt;- prc2[-raw_index, ]\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nhyperparameters &lt;- expand.grid(\n  n.trees = 500,\n  interaction.depth = 5,\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n\n\nfit &lt;- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"gbm\",\n            trControl = ctrl,\n            tuneGrid = hyperparameters,\n            verbose = FALSE)\nfit\n\nStochastic Gradient Boosting \n\n36178 samples\n   18 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9836365  0.9557243\n\nTuning parameter 'n.trees' was held constant at a value of 500\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\n\n\n\nConfusion Matrix For KMeans + GBM\n\n#We should be getting a Kappa of 0.9543!\n#Sensitivity = 0.9930, Specificity = 0.9533\n#Excellent Numbers!\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6792  103\n         1   48 2101\n                                          \n               Accuracy : 0.9833          \n                 95% CI : (0.9804, 0.9858)\n    No Information Rate : 0.7563          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9543          \n                                          \n Mcnemar's Test P-Value : 1.11e-05        \n                                          \n            Sensitivity : 0.9930          \n            Specificity : 0.9533          \n         Pos Pred Value : 0.9851          \n         Neg Pred Value : 0.9777          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7510          \n   Detection Prevalence : 0.7624          \n      Balanced Accuracy : 0.9731          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Content/Projects/Predicting_Income_Bracket/index.html#results",
    "href": "Content/Projects/Predicting_Income_Bracket/index.html#results",
    "title": "Predicting $50K+ Incomes",
    "section": "Results",
    "text": "Results\nOur analysis began with a random forest model to pinpoint critical factors, focusing on the top 8 features for in-depth examination. We then applied principal component analysis for further insight. The culmination of our work was a gradient boosting machine model, finely tuned for peak performance, which boasted a remarkable accuracy rate of 98.3% and a Kappa score of 95.4%. To validate our model’s reliability and guard against overfitting, we compared it with a basic logistic regression model, which showed a Kappa score of 89.5% and an accuracy of 96.1%. Enhancements were made by integrating Kmeans clustering, an unsupervised learning technique, pushing the Kappa score slightly higher to 95.4% and accuracy to 98.3%. Our methodology, combining feature selection and PCA, proved highly effective, offering a solid foundation for future projects with potential for even finer adjustments."
  },
  {
    "objectID": "Assets/Scripts/Portfolio/readme.html",
    "href": "Assets/Scripts/Portfolio/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "Assets/Scripts/About/readme.html",
    "href": "Assets/Scripts/About/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "Assets/HTML/Portfolio/readme.html",
    "href": "Assets/HTML/Portfolio/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "Assets/HTML/About/readme.html",
    "href": "Assets/HTML/About/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "Assets/Images/readme.html",
    "href": "Assets/Images/readme.html",
    "title": "Folder contains all images for the website",
    "section": "",
    "text": "Use /Assets/Images/image.ext, where ext = png, jpeg, jpg, svg, webp, heif, bmp, and gif"
  },
  {
    "objectID": "Assets/Images/readme.html#to-reference-images",
    "href": "Assets/Images/readme.html#to-reference-images",
    "title": "Folder contains all images for the website",
    "section": "",
    "text": "Use /Assets/Images/image.ext, where ext = png, jpeg, jpg, svg, webp, heif, bmp, and gif"
  },
  {
    "objectID": "Content/Blog/GymRat_Plotly/index.html#abstract",
    "href": "Content/Blog/GymRat_Plotly/index.html#abstract",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Abstract",
    "text": "Abstract\nThis study examines the role of stretching in muscle building and its implications for fitness enthusiasts. While stretching offers benefits in terms of flexibility and injury prevention, it falls short as a primary method for muscle growth. The focus of stretching on range of motion rather than muscle mass and strength limits its effectiveness. In isolation, stretching lacks the necessary resistance to stimulate muscle growth. Moreover, my analysis using R reveals that stretching ranks the lowest in terms of workout ratings. The findings highlight the importance of incorporating weight and resistance training as the primary approaches for muscle development."
  },
  {
    "objectID": "Content/Blog/GymRat_Plotly/index.html#introduction",
    "href": "Content/Blog/GymRat_Plotly/index.html#introduction",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Introduction",
    "text": "Introduction\nStretching enhances flexibility and prevents injuries, but its effectiveness in building muscle mass and strength is debated. This study explores the role of stretching in muscle building compared to other exercises. Stretching prioritizes flexibility over muscle development due to inadequate resistance and tension. It may not yield significant gains in muscle mass or strength. Excessive stretching can even reduce muscle activation and power output. Analysis of workout ratings using R shows stretching ranks lowest for muscle building. Weight and resistance training are emphasized as primary methods, with stretching as a supplementary activity for flexibility."
  },
  {
    "objectID": "Content/Blog/GymRat_Plotly/index.html#general-setup",
    "href": "Content/Blog/GymRat_Plotly/index.html#general-setup",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "General Setup",
    "text": "General Setup\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(extrafont)\nlibrary(plotly)\nlibrary(htmlwidgets)\n# font_import()\nloadfonts()"
  },
  {
    "objectID": "Content/Blog/GymRat_Plotly/index.html#data-wrangling",
    "href": "Content/Blog/GymRat_Plotly/index.html#data-wrangling",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ngymDs &lt;- read_csv(\"../../../Assets/Datasets/megaGymDataset.csv\")\n\n# head(gymDs, 5)\n# names(gymDs)\n\nds &lt;- gymDs %&gt;%\n    mutate(\n        ID = ...1,\n        Level = factor(Level, levels = c(\"Beginner\", \"Intermediate\", \"Expert\")),\n        Type = as.factor(Type)\n    ) %&gt;%\n    select(-...1) %&gt;%\n    drop_na()"
  },
  {
    "objectID": "Content/Blog/GymRat_Plotly/index.html#unleashing-the-power-of-plotly",
    "href": "Content/Blog/GymRat_Plotly/index.html#unleashing-the-power-of-plotly",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Unleashing the Power of Plotly",
    "text": "Unleashing the Power of Plotly\n\nplotDs &lt;- ds %&gt;%\n    group_by(Type, Level) %&gt;%\n    summarize(meanRating = mean(Rating)) %&gt;%\n    arrange(Type, meanRating) %&gt;%\n    ungroup()\n\np &lt;- plotDs %&gt;%\n    highlight_key(., ~ reorder_within(Type, meanRating, Level)) %&gt;%\n    ggplot(aes(\n        x = meanRating,\n        y = reorder_within(Type, meanRating, Level),\n        fill = fct_reorder(Type, meanRating),\n        text = paste0(\n            \"Rating: \", round(meanRating, 2),\n            \"&lt;br&gt;Type: \", Type\n        )\n    )) +\n    geom_col(color = \"black\") +\n    facet_grid(\n        rows = vars(Level),\n        scales = \"free_y\",\n        switch = \"y\",\n        space = \"free_y\"\n    ) +\n    scale_y_reordered() +\n    scale_fill_brewer(palette = \"PuBuGn\") +\n    labs(\n        title = \"Ranking Exercise Type According to Experience Level of Individuals\",\n        x = \"Average Rating of Each Exercise Type\",\n        fill = \"Workout Types\"\n    ) +\n    theme_minimal() +\n    myTheme\n\n\nggplotly(p, tooltip = \"text\") %&gt;%\n    config(displayModeBar = FALSE) %&gt;%\n    highlight(on = \"plotly_hover\", off = \"plotly_doubleclick\") %&gt;%\n    layout(\n        uniformtext = list(minsize = 8, mode = \"hide\"),\n        margin = list(b = 70, l = 140, r = 140)\n    )"
  },
  {
    "objectID": "Content/Blog/GymRat_Plotly/index.html#conclusion",
    "href": "Content/Blog/GymRat_Plotly/index.html#conclusion",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Conclusion",
    "text": "Conclusion\nWhile stretching remains valuable for improving flexibility and reducing the risk of injuries, it should be viewed as a supplementary activity rather than the main strategy for building muscle mass and strength. The incorporation of weight and resistance training, supported by the utilization of interactive plots using R, offers a more effective and comprehensive approach to achieving desired muscle development goals."
  },
  {
    "objectID": "Content/Blog/Pokemon_Database/index.html#purpose",
    "href": "Content/Blog/Pokemon_Database/index.html#purpose",
    "title": "Pokédex Database",
    "section": "Purpose",
    "text": "Purpose\nThe main objective of this project was to construct a fully operational Postgresql database in a time frame of fewer than two weeks by employing the Extract, Transform, Load (ETL) methodology. The purpose of this approach was to extract data from various sources, transform it into a format that could be easily integrated into the database, and finally load the transformed data into the database.\nThe process involved several intricate steps, including identifying the relevant data sources, cleansing the extracted data to remove inconsistencies, standardizing the data to a uniform format, and applying data validation and verification techniques to ensure accuracy and completeness. Furthermore, it required careful consideration of the database schema, including the design of tables, relationships between tables, and the use of appropriate data types.\nThe successful implementation of this project was dependent on the utilization of cutting-edge technologies and tools, such as data integration software, data profiling tools, and scripting languages. The result was a functional database that can efficiently store and manage data, making it readily available for analysis, decision-making, and reporting purposes."
  },
  {
    "objectID": "Content/Blog/Pokemon_Database/index.html#summary",
    "href": "Content/Blog/Pokemon_Database/index.html#summary",
    "title": "Pokédex Database",
    "section": "Summary",
    "text": "Summary\nThe inquiry of identifying the optimal base stat Pokemon type sparked my interest, prompting me to delve into the realm of data engineering. In order to craft a well-informed response to this question, I began by utilizing the expansive and multifaceted “Pokémon of Kanto, Johto, and Hoenn Region” dataset to establish a structured and organized database."
  },
  {
    "objectID": "Content/Blog/Pokemon_Database/index.html#unleashing-the-power-of-postgresql-building-a-database",
    "href": "Content/Blog/Pokemon_Database/index.html#unleashing-the-power-of-postgresql-building-a-database",
    "title": "Pokédex Database",
    "section": "Unleashing the Power of PostgreSQL: Building a Database",
    "text": "Unleashing the Power of PostgreSQL: Building a Database\nThis SQL code creates several tables for storing Pokémon data. The tables include information about Pokémon, their types, abilities, generations, and moves. The code establishes primary keys, foreign key constraints, and defines the data types for each column. These tables form the foundation for a comprehensive Pokémon database, enabling efficient storage and retrieval of Pokémon-related information."
  },
  {
    "objectID": "Content/Blog/Pokemon_Database/index.html#data-transformation-and-csv-preparation-in-sql-a-step-by-step-guide",
    "href": "Content/Blog/Pokemon_Database/index.html#data-transformation-and-csv-preparation-in-sql-a-step-by-step-guide",
    "title": "Pokédex Database",
    "section": "Data Transformation and CSV Preparation in SQL: A Step-by-Step Guide",
    "text": "Data Transformation and CSV Preparation in SQL: A Step-by-Step Guide\nThis section of the SQL file focuses on transforming and preparing a CSV file for analysis. It involves multiple SELECT statements that extract relevant data from different tables and join them together. The extracted data is then inserted into temporary tables, including ‘temp1’, ‘temp2’, and ‘temp3’, with each step refining the data further. Finally, the transformed data in ‘temp3’ is selected and filtered based on specific conditions, ordered, and then exported to a CSV file named ‘scuffed_pokedex.csv’. This process prepares the data for further analysis and exploration in external tools or applications.\n\n\nSELECT \n    identifier AS pokemon_name, \n    pokemon_types.type_id,\n    pokemon_abilities.ability_id\nINTO temp1\nFROM pokemon\nLEFT JOIN pokemon_types\nON pokemon.id = pokemon_types.pokemon_id\nLEFT JOIN pokemon_abilities\nON pokemon.id  = pokemon_abilities.pokemon_id;\n\n\nSELECT \n    pokemon_name,\n    types.identifier AS pokemon_type,\n    abilities.identifier AS pokemon_ability,\n    types.generation_id AS gen_id,\n    types.id AS type_id\nINTO temp2\nFROM temp1\nLEFT JOIN types\nON temp1.type_id = types.id\nLEFT JOIN abilities\nON temp1.ability_id = abilities.id;\n\n\nDROP TABLE temp3;\nSELECT \n    pokemon_name,\n    pokemon_type,\n    pokemon_ability,\n    generations.identifier AS pokemon_generation,\n    moves.identifier AS pokemon_move,\n    moves.power AS pokemon_power,\n    moves.accuracy AS pokemon_accuracy,\n    moves.pp AS pokemon_pp\nINTO temp3\nFROM temp2\nLEFT JOIN generations\nON temp2.gen_id = generations.main_region_id\nLEFT JOIN moves\nON temp2.type_id = moves.type_id;\n\nSELECT *\nFROM temp3\nWHERE pokemon_power IS NOT NULL \n    AND pokemon_accuracy IS NOT NULL\nORDER BY pokemon_accuracy, pokemon_power;\n\n\nCOPY temp3\nTO '/Users/Shared/Data_503/Datasets/scuffed_pokedex.csv'\nWITH (FORMAT CSV, HEADER);"
  },
  {
    "objectID": "Content/Blog/Pokemon_Database/index.html#unveiling-pokémon-insights-analyzing-damage-output-and-accuracy",
    "href": "Content/Blog/Pokemon_Database/index.html#unveiling-pokémon-insights-analyzing-damage-output-and-accuracy",
    "title": "Pokédex Database",
    "section": "Unveiling Pokémon Insights: Analyzing Damage Output and Accuracy",
    "text": "Unveiling Pokémon Insights: Analyzing Damage Output and Accuracy\nThrough the power of data analysis and visualization, we have delved into the world of Pokémon to uncover insights about types, damage output, accuracy, and their relationships. Our exploration has shed light on the best Pokémon type for damage output, the most accurate contenders, and the interplay between power and accuracy. By combining the captivating nature of Pokémon with the analytical capabilities of R, we have gained valuable knowledge and set the stage for further investigations in the vast Pokémon universe.\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\n\npokemon &lt;- read_csv(\"../../../Assets/Datasets/scuffed_pokedex.csv\")\n\nnames(pokemon)\n\nnb.cols &lt;- 18\nmycolors &lt;- colorRampPalette(brewer.pal(8, \"YlOrRd\"))(nb.cols)\n\npokemon %&gt;% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %&gt;%\n  group_by(pokemon_type) %&gt;%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = avg_power, y = reorder(pokemon_type, avg_power), fill = reorder(pokemon_type, avg_power)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average power\",\n       y = \"Pokemon type\",\n       title = \"FIRE! The Best Pokemon Type For Damage Output Is...?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\nnb.cols &lt;- 18\nmycolors &lt;- colorRampPalette(brewer.pal(8, \"Blues\"))(nb.cols)\n\npokemon %&gt;% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %&gt;%\n  group_by(pokemon_type) %&gt;%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = avg_accuracy, y = reorder(pokemon_type, avg_accuracy), fill = reorder(pokemon_type, avg_accuracy)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average accuracy\",\n       y = \"Pokemon type\",\n       title = \"Ouch! Who wins the bullseye competition?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\npokemon %&gt;% \n  group_by(pokemon_type) %&gt;%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = avg_power, y = avg_accuarcy, color = pokemon_type)) +\n  geom_point()\n\n\nstats &lt;- pokemon %&gt;% \n  group_by(pokemon_type) %&gt;%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE))\n            \nmodel &lt;- lm(data = stats, avg_accuracy ~ avg_power)\nplot(model)"
  },
  {
    "objectID": "Content/Blog/Pokemon_Database/index.html#presentation",
    "href": "Content/Blog/Pokemon_Database/index.html#presentation",
    "title": "Pokédex Database",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "Content/Blog/CollegeStudent_Debt_Tool/index.html#overview",
    "href": "Content/Blog/CollegeStudent_Debt_Tool/index.html#overview",
    "title": "College Debt Shiny App",
    "section": "Overview",
    "text": "Overview\nIn collaboration with Corey Cassell, our team developed an interactive tool tailored to support students in making informed decisions about their educational journey. This tool offers crucial financial insights, including salary and tuition estimators, as well as a debt calculator, empowering users to navigate their educational choices confidently. By exploring potential salaries, estimating tuition costs, and visualizing projected student debts, individuals can gain valuable perspectives on the financial aspects of their chosen career paths. Recognizing the significance of financial considerations in higher education, our tool comprises four essential components: salary estimator, tuition estimator, debt estimator, and debt calculator. Together, these components provide a comprehensive platform for prospective students to assess potential earnings, anticipate tuition expenses, calculate degree-related debt, and visualize future financial commitments aligned with their chosen majors."
  },
  {
    "objectID": "Content/Blog/CollegeStudent_Debt_Tool/index.html#college-salary-tuition-debt-tool",
    "href": "Content/Blog/CollegeStudent_Debt_Tool/index.html#college-salary-tuition-debt-tool",
    "title": "College Debt Shiny App",
    "section": "College Salary, Tuition, & Debt Tool",
    "text": "College Salary, Tuition, & Debt Tool"
  },
  {
    "objectID": "Content/Blog/CollegeStudent_Debt_Tool/index.html#introduction-and-setup",
    "href": "Content/Blog/CollegeStudent_Debt_Tool/index.html#introduction-and-setup",
    "title": "College Debt Shiny App",
    "section": "Introduction and Setup",
    "text": "Introduction and Setup\nAs students make crucial decisions about their higher education, it’s imperative to equip them with insights into the financial aspects of their chosen career paths. To address this need, we’ve developed an interactive tool comprising four components: the salary estimator, tuition estimator, debt estimator, and debt calculator. This setup section initializes the necessary libraries and performs data wrangling to prepare the datasets for visualization.\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(rsconnect)\nlibrary(colorspace)\nlibrary(plotly)\nlibrary(shinyWidgets)\nlibrary(scales)\nlibrary(ggplot2)\n\n# Reading necessary files from GitHub\nallAgesDf &lt;- read_csv(\"../../../Assets/Datasets/all-ages.csv\")\ntuition_cost &lt;- read_csv(\"../../../Assets/Datasets/tuition_income.csv\")\ntuition &lt;- read_csv(\"../../../Assets/Datasets/tuition_cost.csv\")\nds4 &lt;- read_csv(\"../../../Assets/Datasets/salary_and_stats.csv\")\n\n# Wrangling Salary Potential\n# This section prepares data related to salary estimates for various majors.\nsalary &lt;- allAgesDf %&gt;%\n    dplyr::select(Major, P25th, Median, P75th) %&gt;%\n    pivot_longer(c(P25th, Median, P75th),\n        names_to = \"Percentile_Range\", values_to = \"Salary\"\n    ) %&gt;%\n    arrange(Major) %&gt;%\n    mutate(\n        Percentile_Range = as.factor(Percentile_Range),\n        Major = as.factor(Major)\n    )\n\n# Wrangling Potential Tuition Burden\n# This part of the code prepares data related to tuition costs.\ntuition_cost &lt;- tuition_cost %&gt;%\n    filter(year == 2018 & net_cost &gt; 0) %&gt;%\n    arrange(name) %&gt;%\n    mutate(\n        income_lvl = as.factor(income_lvl),\n        name = as.factor(name)\n    )\n\n# Adjusting income levels for readability\ntuition_cost$income_lvl &lt;- recode(tuition_cost$income_lvl,\n    \"0 to 30,000\" = \"$0 to $30,000\",\n    \"30,001 to 48,000\" = \"$30,001 to $48,000\",\n    \"48_001 to 75,000\" = \"$48,001 to $75,000\",\n    \"75,001 to 110,000\" = \"$75,001 to $110,000\",\n    \"Over 110,000\" = \"Over $110,000\"\n)\n\n# Adjusting data for visualization\nsalary$Percentile_Range &lt;- factor(salary$Percentile_Range, levels = c(\"P25th\", \"Median\", \"P75th\"))\nsalary$Percentile_Range &lt;- recode(salary$Percentile_Range,\n    \"P25th\" = \"Early Career\",\n    \"Median\" = \"Middle Career\",\n    \"P75th\" = \"Late Career\"\n)\nsalary$Major &lt;- str_to_title(salary$Major)\nsalary$Major &lt;- gsub(\"And\", \"and\", salary$Major)\n\n# Further data preparation for visualization\ndf &lt;- tuition %&gt;%\n    group_by(state, degree_length, type) %&gt;%\n    filter(!is.na(state) & degree_length != \"Other\") %&gt;%\n    summarise(\n        room_expenses = mean(room_and_board, na.rm = TRUE),\n        inStateTotal = mean(in_state_total, na.rm = TRUE),\n        outOfStateTotal = mean(out_of_state_total, na.rm = TRUE)\n    )\n\ndf$degree_length &lt;- as.factor(df$degree_length)\ndf$type &lt;- as.factor(df$type)\n\ndf &lt;- df %&gt;% rename(\n    \"Room and Board\" = room_expenses,\n    \"In State Tuition\" = inStateTotal,\n    \"Out of State Tuition\" = outOfStateTotal\n)"
  },
  {
    "objectID": "Content/Blog/CollegeStudent_Debt_Tool/index.html#color-theme",
    "href": "Content/Blog/CollegeStudent_Debt_Tool/index.html#color-theme",
    "title": "College Debt Shiny App",
    "section": "Color Theme",
    "text": "Color Theme\nThis section defines the visual theme to maintain consistency across all plots and enhance readability.\n\n# Definitions for visual theme\ntitle &lt;- 25\nsubtitle &lt;- 20\nfacet_title &lt;- 25\naxis_title &lt;- 18\ntick_numbers &lt;- 13\ntitle_color &lt;- \"black\"\nbackground &lt;- \"gainsboro\"\nplot_background &lt;- \"gainsboro\"\nfacet_header_background &lt;- \"gainsboro\"\nline_type &lt;- \"solid\"\n\n# Custom theme for plots\nCoreyPlotTheme &lt;- theme(\n    text = element_text(family = \"Futura\"),\n    plot.background = element_rect(fill = background),\n    panel.background = element_blank(),\n    panel.grid.major = element_line(size = .1, linetype = line_type, colour = \"gainsboro\"),\n    panel.grid.minor = element_line(size = .1, linetype = line_type, colour = \"black\"),\n    plot.title = element_text(color = title_color, size = title, family = \"Futura\", hjust = 0.5),\n    plot.subtitle = element_text(color = title_color, size = subtitle, family = \"Futura\", hjust = 0.5),\n    plot.caption = element_text(color = title_color, face = \"bold\", size = tick_numbers, family = \"Futura\", hjust = 0),\n    strip.text = element_text(color = title_color, size = facet_title, family = \"Futura\"),\n    strip.background = element_rect(fill = facet_header_background),\n    axis.text = element_text(color = title_color, size = tick_numbers, family = \"Futura\"),\n    axis.title = element_text(color = title_color, size = axis_title, family = \"Futura\"),\n    axis.ticks.x = element_blank(),\n    legend.title = element_text(color = title_color, size = subtitle, family = \"Futura\"),\n    legend.background = element_rect(fill = plot_background),\n    legend.text = element_text(size = tick_numbers, family = \"Futura\")\n)"
  },
  {
    "objectID": "Content/Blog/CollegeStudent_Debt_Tool/index.html#interactive-inputs",
    "href": "Content/Blog/CollegeStudent_Debt_Tool/index.html#interactive-inputs",
    "title": "College Debt Shiny App",
    "section": "Interactive Inputs",
    "text": "Interactive Inputs\nThis section presents the user interface elements allowing users to interact with the data and customize visualizations according to their preferences.\n\nSalary Estimator Selectors\n\n# Inputs for Salary Estimator plot\ninput1 &lt;- inputPanel(\n    selectInput(\"selectInput1\",\n        label = \"Choose your major:\",\n        choices = unique(salary$Major),\n        selected = \"ART HISTORY AND CRITICISM\"\n    ),\n    checkboxGroupInput(\"percentile_choice\",\n        label = \"Pick your career level:\",\n        choices = list(\n            \"Early Career \" = \"Early Career\",\n            \"Middle Career \" = \"Middle Career\",\n            \"Late Career \" = \"Late Career\"\n        ),\n        selected = c(\"Early Career\", \"Middle Career\", \"Late Career\")\n    ),\n)\n\n\n\nTuition Estimator Options\n\n# Inputs for Tuition Estimator plot\ninput2 &lt;- inputPanel(\n    selectInput(\"money\",\n        label = \"Select the type of expense:\",\n        choices = c(\n            \"Room and Board\" = \"Room and Board\",\n           \n\n \"In State Tuition\" = \"In State Tuition\",\n            \"Out of State Tuition\" = \"Out of State Tuition\"\n        ),\n        selected = \"In State Tuition\"\n    ),\n    selectInput(\"state\",\n        label = \"Pick your State:\",\n        choices = unique(df$state),\n        selected = \"Oregon\"\n    ),\n)\n\n\n\nDebt Estimator Levels\n\n# Inputs for Debt Estimator plot\ninput3 &lt;- inputPanel(\n    selectInput(\"selectInput2\",\n        label = \"Select your university:\",\n        choices = unique(tuition_cost$name),\n        selected = \"Willamette University\"\n    ),\n    checkboxGroupInput(\"checkGroup\",\n        label = \"Select your household income bracket:\",\n        choices = list(\n            \"$0 to $30,000\" = \"$0 to $30,000\",\n            \"$30,001 to $48,000\" = \"$30,001 to $48,000\",\n            \"$48,001 to $75,000\" = \"$48,001 to $75,000\",\n            \"$75,001 to $110,000\" = \"$75,001 to $110,000\",\n            \"Over $110,000\" = \"Over $110,000\"\n        ),\n        selected = c(\n            \"$0 to $30,000\",\n            \"$30,001 to $48,000\",\n            \"$48,001 to $75,000\",\n            \"$75,001 to $110,000\",\n            \"Over $110,000\"\n        )\n    ),\n)\n\n\n\nDebt Calculator Choices\n\n# Inputs for Debt Calculator plot\ninput4 &lt;- inputPanel(\n    selectInput(\"major_category\",\n        label = \"Pick a major category:\",\n        choices = unique(ds4$major_category),\n        selected = \"Computers & Mathematics\"\n    ),\n)"
  },
  {
    "objectID": "Content/Blog/CollegeStudent_Debt_Tool/index.html#plots",
    "href": "Content/Blog/CollegeStudent_Debt_Tool/index.html#plots",
    "title": "College Debt Shiny App",
    "section": "Plots",
    "text": "Plots\nThese plots dynamically visualize various aspects of higher education finances based on user-selected inputs.\n\nSalary Estimator\n\n# Plot for Salary Estimator\nplot1 &lt;- renderPlot({\n    salary %&gt;%\n        filter((Major %in% input$selectInput1) & (Percentile_Range %in% input$percentile_choice)) %&gt;%\n        ggplot(aes(x = Percentile_Range, y = Salary, fill = Percentile_Range)) +\n        geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n        geom_label(\n            aes(\n                y = Salary,\n                label = print(paste0(\"$\", round(Salary / 1000, 2), \"K\"))\n            ),\n            show.legend = FALSE,\n            size = 7,\n            family = \"Futura\",\n            fill = \"white\"\n        ) +\n        scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Estimated Salary for \", input$selectInput1),\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\nTuition Estimator\n\n# Plot for Tuition Estimator\nplot2 &lt;- renderPlot({\n    df %&gt;%\n        filter(state == input$state) %&gt;%\n        ggplot(aes(x = degree_length, y = .data[[input$money]], fill = degree_length)) +\n        geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n        facet_wrap(~type) +\n        geom_label(\n            aes(\n                y = .data[[input$money]],\n                label = print(paste0(\"$\", round(.data[[input$money]] / 1000, 2), \"K\"))\n            ),\n            family = \"Oswald\",\n            size = 7,\n            show.legend = FALSE,\n            fill = \"white\"\n        ) +\n        scale_y_continuous(\n            labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3),\n            limits = c(0, 55000)\n        ) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Average \", input$money, \" for \", input$state, \" Universities\"),\n            subtitle = \"For Undergraduate Degrees\",\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\nDebt Estimator\n\n# Plot for Debt Estimator\nplot3 &lt;- renderPlot({\n    tuition_cost %&gt;%\n        filter((income_lvl %in% input$checkGroup) & (name %in% input$selectInput2)) %&gt;%\n        ggplot(aes(x = income_lvl, y = net_cost, fill = income_lvl)) +\n        geom_col(color = \"black\", width = 0.4, position = \"dodge\", show.legend = FALSE) +\n        geom_label(\n            aes(\n                y = net_cost,\n                label = print(paste0(\"$\", round(net_cost / 1000, 2), \"K\"))\n            ),\n            family = \"Oswald\",\n            size = 7,\n            show.legend = FALSE,\n            fill = \"white\"\n        ) +\n        scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Median Student Loan Debt for \", input$selectInput2),\n            subtitle = \"After Completing Their Undergraduate Degree\",\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\nDebt Calculator\n\n# Plot for Debt Calculator\nplot4 &lt;- renderPlot({\n    ds4 %&gt;%\n        filter(major_category == input$major_category) %&gt;%\n        ggplot(aes(perfect_payback_period, reorder(major, perfect_payback_period), fill = perfect_payback_period)) +\n        geom_col(show.legend = FALSE) +\n        geom_label(aes(label = paste(round(perfect_payback_period, 2), \" yrs.\")),\n            show.legend = FALSE,\n            fill = \"white\",\n            hjust = 1.1\n        ) +\n        theme(\n            axis.title.y = element_blank(),\n            axis.text.x = element_blank()\n        ) +\n        labs(\n            title = \"How Long Will You Be In Debt?\",\n            subtitle = \"Based on Your\n\n Major\",\n            x = \"Time to pay off loans\"\n        ) +\n        CoreyPlotTheme +\n        theme(plot.title = element_text(hjust = 0.5)) +\n        scale_fill_continuous_sequential(\"PuBuGn\")\n})"
  },
  {
    "objectID": "Content/Blog/CollegeStudent_Debt_Tool/index.html#conclusion",
    "href": "Content/Blog/CollegeStudent_Debt_Tool/index.html#conclusion",
    "title": "College Debt Shiny App",
    "section": "Conclusion",
    "text": "Conclusion\nThe interactive tool provides valuable resources for high school students considering higher education. By offering comprehensive tools for estimating salaries, tuition costs, and student debt accumulation, we empower students to make informed decisions about their future. This project showcases the power of interactive visualizations in providing crucial information to prospective college students, guiding them towards successful career paths and financial planning."
  },
  {
    "objectID": "Content/Blog/FancyTables/index.html#abstract",
    "href": "Content/Blog/FancyTables/index.html#abstract",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Abstract",
    "text": "Abstract\nThis project aims to explore alternative techniques for designing visually appealing and comprehensible data tables. Traditional Excel spreadsheets often lack readability and visual impact. By incorporating design principles such as color theory, typography, and layout, we aim to create visually striking data tables that effectively convey information. Additionally, we will evaluate innovative software tools and platforms that offer user-friendly options for creating functional and aesthetically pleasing data tables. Enhancing data presentation is crucial for improving interpretation and understanding."
  },
  {
    "objectID": "Content/Blog/FancyTables/index.html#introduction",
    "href": "Content/Blog/FancyTables/index.html#introduction",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Introduction",
    "text": "Introduction\nExcel spreadsheets are widely used for organizing and presenting data. However, their conventional format can be tedious and challenging to read, hindering data comprehension. This project seeks to address this limitation by exploring various techniques to design visually appealing and comprehensible data tables.\nThe primary focus is to create data tables that are not only aesthetically pleasing but also convey information effectively. By employing design principles such as color theory, typography, and layout, we aim to enhance the visual impact and readability of data tables. This will involve experimenting with different combinations of colors, fonts, and arrangement patterns to find the most optimal design choices.\nIt is crucial to recognize that the presentation of data plays a significant role in its interpretation and understanding. The traditional Excel format often lacks visual cues to highlight key data points or insights. Therefore, this project seeks to explore new and innovative methods of presenting data tables that not only serve their functional purpose but also captivate the audience with their visual appeal."
  },
  {
    "objectID": "Content/Blog/FancyTables/index.html#advanced-data-tables",
    "href": "Content/Blog/FancyTables/index.html#advanced-data-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Advanced Data Tables",
    "text": "Advanced Data Tables\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(DT)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(countrycode)\nlibrary(ggflags)\nlibrary(downloadthis)\n\nds &lt;- read_rds(\"../../../Assets/Datasets/pinot.rds\")\n\n# head(ds)\n\n\nds_starter &lt;- ds %&gt;%\n    mutate(\n        province = as.factor(province),\n        price = price,\n        thetaPointMean = mean(points),\n        thetaPriceMean = mean(price)\n    )\n\nds_starter %&gt;%\n    arrange(province, year) %&gt;%\n    select(\n        Province = province,\n        Year = year,\n        Price = price,\n        Points = points,\n        Description = description\n    ) %&gt;%\n    datatable(.,\n        filter = \"bottom\",\n        extensions = \"Buttons\",\n        options = list(\n            dom = \"Bfrtip\",\n            buttons = c(\"copy\", \"csv\", \"excel\"),\n            initComplete = JS(\n                \"function(settings, json) {\",\n                \"$(this.api().table().header()).css({'background-color': '#131F4F', 'color': '#fff'});\",\n                \"}\"\n            )\n        )\n    )\n\n\n\n\n\n\nds_summary &lt;- ds_starter %&gt;%\n    group_by(province) %&gt;%\n    arrange(year) %&gt;%\n    summarise(\n        pointsMean = mean(points, na.rm = TRUE),\n        pointsSD = sd(points),\n        priceMean = mean(price, na.rm = TRUE),\n        priceSD = sd(price),\n        points = list(points),\n        price = list(price),\n        .groups = \"drop\"\n    )\n\n\nexcel_file_attachment &lt;- ds_summary %&gt;%\n    download_this(\n        output_name = \"Pinot_Noir_Summary\",\n        output_extension = \".xlsx\", # Excel file type\n        button_label = \"Download Excel\",\n        button_type = \"primary\", # change button type\n    )"
  },
  {
    "objectID": "Content/Blog/FancyTables/index.html#adding-trend-lines-to-summary-tables",
    "href": "Content/Blog/FancyTables/index.html#adding-trend-lines-to-summary-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Adding Trend Lines To Summary Tables",
    "text": "Adding Trend Lines To Summary Tables\n\nfancyTbl &lt;- ds_summary %&gt;%\n    gt() %&gt;%\n    # format the numeric output to 3 digit rounding\n    fmt_number(\n        columns = c(pointsMean, pointsSD, priceMean, priceSD),\n        decimals = 3\n    ) %&gt;%\n    # create nice labels for a few ugly variable names\n    cols_label(\n        province = \"Province\",\n        pointsMean = \"Avg. Points\",\n        pointsSD = \"Std. Dev. Points\",\n        priceMean = \"Avg. Price\",\n        priceSD = \"Std. Dev. Price\",\n        points = \"Points Trend\",\n        price = \"Price Trend\",\n    ) %&gt;%\n    # Plot the sparklines from the list column\n    gt_plt_sparkline(points,\n        type = \"ref_median\",\n        same_limit = TRUE\n    ) %&gt;%\n    gt_plt_sparkline(price,\n        type = \"ref_median\",\n        same_limit = TRUE\n    ) %&gt;%\n    # use the guardian's table theme\n    gt_theme_guardian() %&gt;%\n    # give hulk coloring to the Mean Human Rights Score\n    gt_hulk_col_numeric(pointsMean) %&gt;%\n    gt_hulk_col_numeric(priceMean) %&gt;%\n    # create a header and subheader\n    tab_header(title = \"Province Pinot Wine Summary\", subtitle = \"Source: Dr. Hendrick\") %&gt;%\n    # attach excel file\n    tab_source_note(excel_file_attachment)\n# save the original as an image\n# gtsave(fancyTbl, \"table.png\")\n# show the table themed in accordance with the page\nfancyTbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProvince Pinot Wine Summary\n\n\nSource: Dr. Hendrick\n\n\nProvince\nAvg. Points\nStd. Dev. Points\nAvg. Price\nStd. Dev. Price\nPoints Trend\nPrice Trend\n\n\n\n\nBurgundy\n90.438\n2.989\n98.035\n132.856\n\n\n\n   89.0\n\n\n\n\n   83.0\n\n\n\nCalifornia\n90.517\n2.831\n47.465\n18.553\n\n\n\n   91.0\n\n\n\n\n   34.0\n\n\n\nCasablanca_Valley\n86.282\n2.428\n21.107\n11.953\n\n\n\n   87.0\n\n\n\n\n   30.0\n\n\n\nMarlborough\n87.550\n2.245\n27.668\n13.833\n\n\n\n   85.0\n\n\n\n\n   25.0\n\n\n\nNew_York\n87.748\n2.268\n25.679\n9.565\n\n\n\n   88.0\n\n\n\n\n   35.0\n\n\n\nOregon\n89.489\n2.663\n44.856\n20.209\n\n\n\n   90.0\n\n\n\n\n   22.0\n\n\n\n\n\n Download Excel"
  },
  {
    "objectID": "Content/Blog/FancyTables/index.html#conclusion",
    "href": "Content/Blog/FancyTables/index.html#conclusion",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Conclusion",
    "text": "Conclusion\nThis project highlights the importance of visually appealing and comprehensible data tables as an alternative to Excel. By incorporating design principles and exploring innovative tools, we enhance data presentation and interpretation. It calls for adopting alternative techniques to design data tables. By embracing visually appealing formats, we improve data comprehension, communication, and unlock new possibilities for visualization and analysis."
  },
  {
    "objectID": "Content/Talks/ParasiteSurvAnalysis/index.html#unveiling-natures-heatwave-the-hidden-impact-of-temperature-on-pesticide-toxicity",
    "href": "Content/Talks/ParasiteSurvAnalysis/index.html#unveiling-natures-heatwave-the-hidden-impact-of-temperature-on-pesticide-toxicity",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Unveiling Nature’s Heatwave: The Hidden Impact of Temperature on Pesticide Toxicity",
    "text": "Unveiling Nature’s Heatwave: The Hidden Impact of Temperature on Pesticide Toxicity\nIn this captivating study, we embark on a quest to unravel the influence of temperature on the toxicity of pesticides, driven by growing concerns about the effects of climate change. Our mission extends beyond conventional pesticide testing, as we strive to shed light on the potential impacts of varying temperatures. We seek to answer a pivotal question: Does temperature significantly shape pesticide toxicity? To uncover the truth, we employ survival analysis, delving into the realm of time-to-death for the exposed subjects. By deciphering the intricate interplay between temperature and pesticide toxicity, we aim to reveal the magnitude of this effect and its implications for our changing world."
  },
  {
    "objectID": "Content/Talks/ParasiteSurvAnalysis/index.html#parasitic-marvels-unveiled-delving-into-the-intricate-world-of-echinostoma-trivolvis",
    "href": "Content/Talks/ParasiteSurvAnalysis/index.html#parasitic-marvels-unveiled-delving-into-the-intricate-world-of-echinostoma-trivolvis",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Parasitic Marvels Unveiled: Delving into the Intricate World of Echinostoma trivolvis",
    "text": "Parasitic Marvels Unveiled: Delving into the Intricate World of Echinostoma trivolvis\nPrepare to be amazed as we take you on a captivating journey into the hidden intricacies of Echinostoma trivolvis, a remarkable species of trematode parasite. From its widespread infection of birds, mammals, and reptiles to its complex life cycle involving snails, birds, and mammals, this parasite holds many secrets waiting to be unlocked. Join us as we explore its adult worms’ intriguing residence in the small intestine, where they feed on blood and nutrients, causing a range of symptoms from abdominal pain to malnutrition. Found abundantly in North American wetlands and aquatic environments, Echinostoma trivolvis is a true marvel of nature that continues to fascinate and astonish researchers and nature enthusiasts alike."
  },
  {
    "objectID": "Content/Talks/ParasiteSurvAnalysis/index.html#presentation",
    "href": "Content/Talks/ParasiteSurvAnalysis/index.html#presentation",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "Content/Talks/index.html",
    "href": "Content/Talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 6, 2024\n\n\nInteractive Teaching with webR\n\n\nBrian Cervantes Alvarez\n\n\n\n\nNov 14, 2023\n\n\nQuarto 101\n\n\nBrian Cervantes Alvarez\n\n\n\n\nFeb 27, 2023\n\n\nInvestigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach\n\n\nBrian Cervantes Alvarez, Willa Van Liew, Hans Lehndorff\n\n\n\n\nJan 7, 2023\n\n\nAbout Myself - MSDS 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Content/Projects/PredictSeverityLevels/index.html#abstract",
    "href": "Content/Projects/PredictSeverityLevels/index.html#abstract",
    "title": "Symptom Severity Analysis",
    "section": "Abstract",
    "text": "Abstract\nThis paper presents a machine learning study that aims to predict severity levels in patients by utilizing a random forest approach based on symptom analysis. The objective of the study was to develop accurate rules for classifying patients into three severity levels: Mild, Moderate, and Severe. The dataset consisted of patient symptoms, which were used in conjunction with a random forest model for the classification task. The results indicated that patients with 1 to 3 symptoms, including depression, were classified as Mild, while those with at least 3 symptoms, including depression and cramps, were classified as Moderate. Patients with 4 symptoms, including cramps and spasms, were classified as Severe. The study conducted comprehensive evaluations using metrics such as the confusion matrix, ROC AUC, precision vs recall, and kappa to assess the performance of the random forest model. The results highlighted the promising performance of the model in accurately predicting severity levels among patients. The findings from this study provide robust evidence that can contribute to personalized healthcare and effective treatment planning."
  },
  {
    "objectID": "Content/Projects/PredictSeverityLevels/index.html#introduction",
    "href": "Content/Projects/PredictSeverityLevels/index.html#introduction",
    "title": "Symptom Severity Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn the field of rare diseases, physicians often observe variations in the severity levels of patients; however, there is a lack of established guidelines to map individual patients directly to specific severity levels. Our client, who is developing a product for this rare disease, aims to target patients with moderate to severe conditions. To address this challenge, the client possesses a comprehensive database containing clinically relevant information about the disease, including symptom flags and a symptom count variable. Additionally, each patient in the database has been rated by a physician as mild, moderate, or severe. The client has approached [REDACTED] with the objective of extracting the mental heuristics employed by physicians when assigning severity labels to patients. The task at hand involves deriving simple rules (1-3 per severity level) from the database that can effectively classify patients. For instance, a set of rules might involve identifying patients as severe if they exhibit fatigue or have exactly two symptoms. By extracting these rules, our aim is to provide the client with a clearer understanding of the factors influencing severity levels in order to enhance their product development and enable more personalized patient care."
  },
  {
    "objectID": "Content/Projects/PredictSeverityLevels/index.html#methodology",
    "href": "Content/Projects/PredictSeverityLevels/index.html#methodology",
    "title": "Symptom Severity Analysis",
    "section": "Methodology",
    "text": "Methodology\n\n# Import the required libraries\nimport pandas as pd                 # Library for data manipulation and analysis\nimport numpy as np                  # Library for numerical computations\nimport matplotlib.pyplot as plt     # Library for data visualization\nimport seaborn as sns\n\n# Import Machine Learning libraries\nfrom sklearn import tree            # Library for decision tree models\nfrom sklearn.ensemble import RandomForestClassifier  # Library for random forest models\nfrom sklearn.model_selection import (\n  cross_val_score,                   # Library for cross-validation\n  train_test_split,\n  GridSearchCV\n\n)  \nfrom sklearn.preprocessing import LabelEncoder      # Library for label encoding\nfrom sklearn.metrics import (                        # Library for model evaluation metrics\n    confusion_matrix,                                # Confusion matrix\n    roc_auc_score,                                   # ROC AUC score\n    roc_curve,                                       # ROC Curve plot\n    recall_score,                                    # Recall score\n    precision_score,                                 # Precision score\n    precision_recall_curve,                          # Recall vs. Precision Curve\n    cohen_kappa_score                                # Cohen's kappa score\n)\n\n# Load the dataset\nds = pd.read_csv(\"../../../Assets/Datasets/severityLevels.csv\")  \nds.head(10)  # Display the first 10 rows of the dataset\n\n\n\n\n\n\n\n\nPatient\nFinal Category\nFatigue\nWeakness\nDepression\nAnxiety\nDry Skin\nSpasms\nTingling\nHeadaches\nCramps\nNumber of Symptoms\n\n\n\n\n0\n1\nMild\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\nMild\n0\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\n2\n3\nMild\n1\n0\n0\n0\n1\n0\n1\n1\n0\n4\n\n\n3\n4\nMild\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n5\nMild\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n5\n6\nMild\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n6\n7\nMild\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n8\nMild\n0\n0\n1\n0\n0\n0\n0\n1\n0\n2\n\n\n8\n9\nMild\n1\n0\n0\n0\n0\n0\n1\n0\n0\n2\n\n\n9\n10\nMild\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n# Display basic statistics of the dataset\nds.describe()\n\n\n\n\n\n\n\n\nPatient\nFatigue\nWeakness\nDepression\nAnxiety\nDry Skin\nSpasms\nTingling\nHeadaches\nCramps\nNumber of Symptoms\n\n\n\n\ncount\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n\n\nmean\n150.500000\n0.466667\n0.396667\n0.356667\n0.363333\n0.366667\n0.200000\n0.323333\n0.333333\n0.213333\n3.020000\n\n\nstd\n86.746758\n0.499721\n0.490023\n0.479816\n0.481763\n0.482700\n0.400668\n0.468530\n0.472192\n0.410346\n1.703704\n\n\nmin\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n75.750000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2.000000\n\n\n50%\n150.500000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3.000000\n\n\n75%\n225.250000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n1.000000\n1.000000\n0.000000\n4.000000\n\n\nmax\n300.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n8.000000\n\n\n\n\n\n\n\n\n# Visualize the distribution of the target variable 'Final Category'\nplt.figure(figsize=(9, 6))\nds['Final Category'].value_counts().plot(kind='bar')\nplt.xlabel('Symptom Category')\nplt.ylabel('Number of Cases')\nplt.title('Distribution of Final Category')\nplt.xticks(rotation=0)  # Set rotation to 0 degrees for horizontal x-axis labels\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calculate the correlation matrix\ncorr_matrix = ds.corr()\n\n# Visualize the correlation matrix\nplt.figure(figsize=(10, 7))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.xticks(rotation=45)  # Set rotation to 0 degrees for horizontal x-axis labels\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFeature Engineering\nThe code performs some data preprocessing and feature selection to prepare the data for a Random Forest model. Let’s go through each step:\n\nDrop the ID column: The code removes a column named “Patient” from the dataset. This column likely contains unique identifiers for each patient, and since it’s not relevant for our analysis, we can safely remove it.\nCreate dummy columns for “Number of Symptoms”: The code converts a column called “Number of Symptoms” into multiple binary columns, known as dummy variables. Each dummy variable represents a unique value in the original column. This transformation helps us to use categorical data in our machine learning model.\nConcatenate the dummy columns with the original dataframe: The code combines the newly created dummy columns with the original dataset. This ensures that we retain all the existing information while incorporating the transformed categorical data.\nDrop the original “Number of Symptoms” column: Since we have created the dummy columns, we no longer need the original “Number of Symptoms” column. Therefore, the code removes this column from the dataset.\nSeparate the features (X) and the target variable (y): The code splits the dataset into two parts. The features, represented by the variable X, contain all the columns except the “Final Category” column, which is the target variable we want to predict. The target variable, represented by the variable y, contains only the “Final Category” column. This is the severity cases of ‘Mild’, ‘Moderate’ and ‘Severe’\nPerform feature selection using Random Forest: The code utilizes a machine learning algorithm called Random Forest to identify the most important features for each category in the target variable. It trains a separate Random Forest model for each category and determines the top three features that contribute the most to predicting that category.\nStore the top features for each category: The code stores the top three features for each category in a dictionary called “top_features.” Each category is represented by a label, and the corresponding top features are stored as a list.\nPrint the top 3 features for each label: The code then prints the top three features for each category in the target variable. This information helps us understand which features are most influential in determining the predicted category.\n\nOverall, this code prepares the data by transforming categorical data into a suitable format and identifies the top features that contribute to predicting different categories. This sets the stage for further analysis and building the final machine learning model based on these selected features.\n\n# Drop the ID column\nds.drop('Patient', axis=1, inplace=True)\n\n# Create dummy columns from the \"Number of Symptoms\" column\ndummy_cols = pd.get_dummies(ds['Number of Symptoms'], prefix='Symptom')\n\n# Concatenate the dummy columns with the original dataframe\nds = pd.concat([ds, dummy_cols], axis=1)\n\n# Drop the original \"Number of Symptoms\" column\nds.drop('Number of Symptoms', axis=1, inplace=True)\n\nds.head(10)\n\n\n\n\n\n\n\n\nFinal Category\nFatigue\nWeakness\nDepression\nAnxiety\nDry Skin\nSpasms\nTingling\nHeadaches\nCramps\nSymptom_0\nSymptom_1\nSymptom_2\nSymptom_3\nSymptom_4\nSymptom_5\nSymptom_6\nSymptom_7\nSymptom_8\n\n\n\n\n0\nMild\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\nMild\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n2\nMild\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\nMild\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nMild\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nMild\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nMild\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\nMild\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n8\nMild\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n9\nMild\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n# Separate the features (X) and the target variable (y)\nX = ds.drop('Final Category', axis=1)\ny = ds['Final Category']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store top features per label\ntop_features = {}\n\n# Perform feature selection for each label\nfor label in y_train.unique():\n    # Encode the target variable for the current label\n    y_label_train = (y_train == label).astype(int)\n    y_label_test = (y_test == label).astype(int)\n    \n    # Create a Random Forest model\n    rf_model = RandomForestClassifier(random_state=60)\n    \n    # Train the model\n    rf_model.fit(X_train, y_label_train)\n    \n    # Get feature importances\n    feature_importances = rf_model.feature_importances_\n    \n    # Sort features by importance in descending order\n    sorted_features = sorted(zip(X_train.columns, feature_importances), key=lambda x: x[1], reverse=True)\n    \n    # Get the top 3 features for the current label\n    selected_features = [feature for feature, _ in sorted_features[:3]]\n    \n    # Store the top features for the current label\n    top_features[label] = selected_features\n\n# Print the top 3 features for each label\nfor label, features in top_features.items():\n    print(f\"Top 3 features for {label}:\")\n    print(features)\n    print()\n\nTop 3 features for Severe:\n['Cramps', 'Spasms', 'Symptom_4']\n\nTop 3 features for Mild:\n['Depression ', 'Symptom_1', 'Symptom_3']\n\nTop 3 features for Moderate:\n['Symptom_3', 'Depression ', 'Cramps']\n\n\n\n\nFeatures Explained\nThe feature selection process aims to identify the most important factors (features) that contribute to determining the severity of a patient’s condition. In this case, the severity levels are categorized as “Mild,” “Moderate,” and “Severe.” The top three features that were found to be most indicative for each severity level are as follows:\nFor patients rated as “Mild”:\n\nHas 1 symptom:\n\nThis feature indicates the presence of a specific symptom (let’s say, symptom X) that is associated with a mild severity rating. If a patient has symptom X, it suggests a higher likelihood of being classified as “Mild.”\n\nDepression:\n\nThis feature refers to the presence or absence of depression symptoms in the patient. The presence of depression symptoms is considered important in determining a mild severity rating.\n\nHas 3 symptoms:\n\nThis feature represents the presence of 3 symptoms (let’s call them symptoms A,B,C) that are associated with a mild severity rating. If a patient has symptoms A,B,C, it suggests a higher likelihood of being classified as “Mild.”\nGiven this information, it can be recommended that a threshold is established. It can be inferred that if a patient has 1-3 symptoms and/or has depression, they can be classified as “Mild.” This is addressed with the model later in the study.\nFor patients rated as “Moderate”:\n\nHas 3 symptoms:\n\nThis feature represents the presence of 3 symptoms (let’s call them symptoms A,B,C again) that are associated with a moderate severity rating. If a patient has symptoms A,B,C, it suggests a higher likelihood of being classified as “Moderate.”\n\nDepression:\n\nThe presence or absence of depression symptoms also plays a role in determining a moderate severity rating.\n\nCramps:\n\nThis feature represents the presence or absence of cramps in the patient. The presence of cramps is considered important in predicting a moderate severity rating.\nGiven this information, another threshold can be established. It can be inferred that if a patient has at least 3 symptoms and/or has depression and/or cramps, they can be classified as “Moderate.” This is addressed with the model later in the study.\nFor patients rated as “Severe”:\n\nCramps:\n\nThis feature indicates the presence or absence of cramps, which is associated with a severe severity rating. If a patient has cramps, it suggests a higher likelihood of being classified as “Severe.”\n\nSpasms:\n\nThis feature refers to the presence or absence of muscle spasms in the patient. The presence of spasms is considered important in predicting a severe severity rating.\n\nHas 4 symptoms:\n\nThis feature represents the presence of symptoms (let’s call it symptoms A,B,C,D) that are associated with a severe severity rating. If a patient has symptoms , it suggests a higher likelihood of being classified as “Severe.”\nGiven this information, a last threshold can be established. It can be inferred that if a patient has at least 4 symptoms and/or has cramps and/or spasms, they can be classified as “Severe.” This is addressed with the model later in the study.\nIn summary, the top features identified for each severity level provide insights into the specific symptoms and factors that contribute to determining the severity of a patient’s condition. By considering the presence or absence of these features, the model can make predictions about the severity rating of a patient’s condition, helping healthcare professionals assess the level of severity and provide appropriate care and treatment.\n\n\n\nRandom Forest Model\nThe code performs machine learning tasks using a Random Forest model with the selected features from the earlier model. Let’s go through each step:\n\nSeparate the features (X) and the target variable (y) using only the top features: The code selects specific features from the dataset based on their importance in predicting the target variable. These features are obtained from the “top_features” dictionary, which contains the top three features for each category (Mild, Moderate, and Severe) in the target variable.\nEncode the target variable using label encoding: The target variable “Final Category” is a categorical variable. To use it in the machine learning model, we need to convert it into numeric form. The code uses label encoding, which assigns a unique numeric value to each category in the target variable.\nCreate a random forest model using the top features: The code initializes a Random Forest model with a specific random state. Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions.\nPerform 10-fold cross-validation: The code evaluates the performance of the Random Forest model using a technique called cross-validation. Cross-validation helps estimate how well the model will generalize to new, unseen data. In this case, 10-fold cross-validation is performed, which means the dataset is divided into 10 equal parts (folds). The model is trained and tested 10 times, with each fold serving as the test set once.\nPrint the cross-validation scores: The code prints the cross-validation scores obtained from each fold. These scores indicate how well the model performed on each fold. Additionally, the mean cross-validation score is calculated, which provides an overall measure of the model’s performance.\nTrain the model: The code trains the Random Forest model using all the available data, as specified by the features (X) and target variable (y).\nMake predictions on the training set: The code uses the trained model to make predictions on the same dataset that was used for training. This helps evaluate how well the model can predict the target variable for the given features.\nPrint the confusion matrix: The code prints a confusion matrix, which is a table that shows the number of correct and incorrect predictions made by the model. It provides insights into the model’s performance for each category in the target variable.\nCalculate and print other evaluation metrics: The code calculates additional evaluation metrics such as ROC AUC score, recall, precision, and Kappa metric. These metrics help assess the model’s performance in terms of classification accuracy, sensitivity, precision, and agreement beyond chance.\n\nOverall, this code builds a Random Forest model using selected features, evaluates its performance through cross-validation, and provides insights into the model’s predictive capabilities using various evaluation metrics. The goal is to understand how well the model can predict the categories in the target variable based on the selected features.\n\n# Separate the features (X) and the target variable (y) using only the top features\nX = ds[top_features['Mild'] + top_features['Moderate'] + top_features['Severe']]\ny = ds['Final Category']\n\n# Encode the target variable using label encoding\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Create a random forest model\nrf_model = RandomForestClassifier(random_state=60)\n\n# Define the parameter grid for hyperparameter optimization\nparam_grid = {\n    'max_depth': [None],\n    'min_samples_leaf': [4],\n    'min_samples_split': [2],\n    'n_estimators': [100]\n}\n\n# Perform hyperparameter optimization using grid search and 10-fold cross-validation\ngrid_search = GridSearchCV(rf_model, param_grid, cv=10)\ngrid_search.fit(X, y)\n\n# Get the best random forest model with optimized hyperparameters\nrf_model = grid_search.best_estimator_\n\n# Perform 10-fold cross-validation with the optimized model\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Print the cross-validation scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())\nprint()\n\nCross-Validation Scores: [0.86666667 0.9        1.         0.9        0.96666667 0.9\n 0.9        0.83333333 0.86666667 0.8       ]\nMean CV Score: 0.8933333333333333\n\n\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=87)\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\n\n# Calculate and print ROC AUC score\nroc_auc = roc_auc_score(y_test, rf_model.predict_proba(X_test), multi_class='ovr')\nprint(\"ROC AUC:\", roc_auc)\nprint()\n\n# Calculate and print recall score\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint(\"Recall:\", recall)\nprint()\n\n# Calculate and print precision score\nprecision = precision_score(y_test, y_pred, average='weighted')\nprint(\"Precision:\", precision)\nprint()\n\n# Calculate and print Kappa metric\nkappa = cohen_kappa_score(y_test, y_pred)\nprint(\"Kappa:\", kappa)\nprint()\n\nConfusion Matrix:\n[[18  4  0]\n [ 0 20  1]\n [ 0  1 16]]\n\nROC AUC: 0.9480560359313329\n\nRecall: 0.9\n\nPrecision: 0.9133333333333333\n\nKappa: 0.8493723849372385\n\n\n\n\n\nMetrics Explained\nLet’s break down the provided metrics based on the given confusion matrix:\n\nConfusion Matrix\nThe confusion matrix is a table that helps us understand the performance of a classification model. It shows the predicted labels versus the actual labels for each class. In this case, the confusion matrix has three rows and three columns, representing the three severity rating categories. In this case, since I set the test set to be 20% of the sample of 300, there is 60 total patients that were randomly tested.\n\n\\begin{array}{c|ccc}\n& {\\text{Actual}} \\\\\n\\text{Predicted} & \\text{Mild} & \\text{Moderate} & \\text{Severe} \\\\\n\\hline\n\\text{Mild} & 18 & 4 & 0 \\\\\n\\text{Moderate} & 0 & 20 & 1 \\\\\n\\text{Severe} & 0 & 1 & 16 \\\\\n\\end{array}\n\nFirst, the number 18 in the first row and column indicates that the model correctly predicted 18 patients as “mild” which coincides with the actual “mild” severity label. Next, the number 4 in the first row and second column indicates that the model incorrectly predicted 4 patients as “moderate” when their actual severity rating was “mild.” Lastly, the number 0 in the first row and third column indicates that the model correctly predicted 0 patients as being “severe”, meaning that those patients were labeled correctly as “mild”.\nSimilarly, the other numbers in the confusion matrix represent the model’s predictions for the other severity rating categories. Given that this was a dataset with only 300 patients, it is very intriguing that it can label each patient with high accuarcy.\n\n\nROC Curve\nNow, the ROC AUC (Receiver Operating Characteristic Area Under the Curve) is a measure of the model’s ability to distinguish between different severity ratings. It represents the overall performance of the model across all severity levels. The value of 0.948 indicates a high level of performance, close to 1, suggesting that the model has good predictive capability for distinguishing between severity ratings.\n\n# Calculate predicted probabilities for each class\ny_pred_proba = rf_model.predict_proba(X_test)\n\n# Compute the ROC curve for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor class_index in range(len(label_encoder.classes_)):\n    fpr[class_index], tpr[class_index], _ = roc_curve(\n        (y_test == class_index).astype(int), y_pred_proba[:, class_index]\n    )\n    roc_auc[class_index] = roc_auc_score(\n        (y_test == class_index).astype(int), y_pred_proba[:, class_index]\n    )\n\n# Plot the ROC curve for each class\nplt.figure(figsize=(9, 8))\nfor class_index in range(len(label_encoder.classes_)):\n    plt.plot(\n        fpr[class_index],\n        tpr[class_index],\n        label=f\"Class {label_encoder.classes_[class_index]} (AUC = {roc_auc[class_index]:.2f})\",\n    )\n\n# Plot random guessing line\nplt.plot([0, 1], [0, 1], \"k--\")\n\n# Set plot properties\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\n\n# Show the plot\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRecall vs. Precision Curve\nRecall, also known as sensitivity or true positive rate, measures the proportion of actual positive instances (patients with a particular severity rating) that the model correctly identified. A recall value of 0.9 means that the model identified nearly 90.0% of the patients with their correct severity rating.\nPrecision measures the proportion of instances that the model predicted correctly as positive (patients with a particular severity rating) out of all instances it predicted as positive. A precision value of 0.913 indicates that out of all the patients the model identified as having a specific severity rating, 91.3% of them were correct.\n\n# Calculate precision and recall values for each class\nprecision, recall, thresholds = precision_recall_curve(y_test, rf_model.predict_proba(X_test)[:, 1], pos_label=1)\n\n# Plot the recall vs. precision curve\nplt.figure(figsize=(9, 8))\nplt.plot(recall, precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Recall vs. Precision Curve')\nplt.grid(True)\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nKappa\nThe Kappa statistic is a measure of agreement between the model’s predictions and the actual severity ratings, taking into account the possibility of agreement occurring by chance. A Kappa value of 0.849 indicates a substantial level of agreement between the model’s predictions and the actual severity ratings, suggesting a reliable performance of the model.\nOverall, these metrics indicate that the model has performed well in predicting the severity ratings of the patients, with high accuracy, good distinction between severity levels, and substantial agreement with the actual severity ratings provided by physicians.\n\n\n\nVisualizing Random Forest’s Best Decision Tree\n\n# Perform 10-fold cross-validation\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Find the index of the best decision tree\nbest_tree_index = np.argmax(cv_scores)\n\n# Get the best decision tree from the random forest\nbest_tree = rf_model.estimators_[best_tree_index]\n\n# Visualize the best decision tree using matplotlib\nplt.figure(figsize=(12, 12))\ntree.plot_tree(best_tree, feature_names=X.columns, class_names=label_encoder.classes_, filled=True)\n\nplt.show()"
  },
  {
    "objectID": "Content/Projects/PredictSeverityLevels/index.html#results",
    "href": "Content/Projects/PredictSeverityLevels/index.html#results",
    "title": "Symptom Severity Analysis",
    "section": "Results",
    "text": "Results\n\nIf a patient has between 1 to 3 symptoms, and one of those symptoms includes depression, they are classified as ‘Mild’.\nIf a patient has at least 3 symptoms, and two of those symptoms are depression and cramps, they are classified as ‘Moderate’.\nLastly, if a patient has 4 symptoms, and two of those symptoms are cramps and spasms, they are classified as ‘Severe’."
  },
  {
    "objectID": "Content/Projects/PredictSeverityLevels/index.html#conclusion",
    "href": "Content/Projects/PredictSeverityLevels/index.html#conclusion",
    "title": "Symptom Severity Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis study focused on predicting severity levels in patients with a rare disease using a random forest approach based on symptom analysis. The client’s goal was to map individual patients to appropriate severity levels, given the absence of established guidelines. By leveraging a comprehensive database containing clinically relevant information and severity ratings provided by physicians, we extracted simple rules to classify patients.\nThe study revealed that patients with 1 to 3 symptoms, including depression, were classified as ‘Mild’. For patients with at least 3 symptoms, the presence of depression and cramps (at least 2 symptoms) indicated a classification of ‘Moderate’. Lastly, patients presenting with 4 symptoms, including cramps and spasms (at least 2 symptoms), were categorized as ‘Severe’.\nThe derived rules provide valuable insights into the mental heuristics employed by physicians when assessing severity levels in patients. By incorporating these rules into the client’s product development, personalized healthcare targeting patients with moderate to severe disease can be enhanced. Furthermore, these findings contribute to filling the existing gap in severity level guidelines for this rare disease."
  },
  {
    "objectID": "Content/Projects/PredictSeverityLevels/index.html#further-research",
    "href": "Content/Projects/PredictSeverityLevels/index.html#further-research",
    "title": "Symptom Severity Analysis",
    "section": "Further Research",
    "text": "Further Research\nIt is recommended to validate and refine these rules using larger datasets. By expanding the dataset, researchers can ensure the generalizability of the derived rules and improve the accuracy of the classification. Additionally, exploring additional factors that may influence severity levels, such as demographic information, medical history, or genetic markers, can provide a more comprehensive understanding of the disease and enhance the prediction models.\nBy continuing to refine and validate the rules and incorporating more factors into the analysis, personalized treatment for patients with this rare disease can be further optimized, leading to improved patient outcomes. The combination of symptom analysis and machine learning approaches holds significant potential for facilitating accurate classification and personalized healthcare in various medical domains."
  },
  {
    "objectID": "Content/Projects/PredictSeverityLevels/index.html#references",
    "href": "Content/Projects/PredictSeverityLevels/index.html#references",
    "title": "Symptom Severity Analysis",
    "section": "References",
    "text": "References\nPandas: McKinney, W. (2010). Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference, 445-451. [Link: https://conference.scipy.org/proceedings/scipy2010/mckinney.html]\nNumPy: Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357-362. [Link: https://www.nature.com/articles/s41586-020-2649-2]\nMatplotlib: Hunter, J. D. (2007). Matplotlib: A 2D Graphics Environment. Computing in Science & Engineering, 9(3), 90-95. [Link: https://ieeexplore.ieee.org/document/4160265]\nSeaborn: Waskom, M., Botvinnik, O., Hobson, P., … Halchenko, Y. (2021). mwaskom/seaborn: v0.11.1 (February 2021). Zenodo. [Link: https://doi.org/10.5281/zenodo.4473861]\nScikit-learn: Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., … Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830. [Link: https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html]"
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#objective",
    "href": "Content/Projects/Customer Return ML/index.html#objective",
    "title": "Predicting Customer Returns",
    "section": "Objective",
    "text": "Objective\nThis project aimed to predict customer return behavior using a dataset with various product and customer attributes. By employing logistic regression and Random Forest models, we sought to determine the likelihood of a customer returning a purchased item."
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#methodology",
    "href": "Content/Projects/Customer Return ML/index.html#methodology",
    "title": "Predicting Customer Returns",
    "section": "Methodology",
    "text": "Methodology\nOur analysis began with loading essential libraries and datasets, followed by an exploratory data analysis (EDA) to understand the distribution of returns across different attributes like product department, size, and customer state. Key features such as season, customer age, manufacturer’s suggested retail price (MSRP), and price range were engineered to enrich the dataset.\nWe initially applied logistic regression for its suitability for binary outcomes. However, to improve our model’s predictive accuracy, we transitioned to a Random Forest approach. This model yielded an AUC score of 0.625, indicating moderate predictive capability and highlighting areas for potential improvement through further feature engineering or model tuning.\n\nKey Steps and Code Highlights\n\nPreparation and EDA:\n\nLoaded necessary R packages: tidyverse, lubridate, caret, and glmnet.\nImported and glimpsed at the training and testing data.\nConducted EDA to analyze returns by product department, size, and customer state.\n\nFeature Engineering:\n\nDeveloped a feature engineering function to create relevant variables such as Season, CustomerAge, MSRP, and PriceRange.\nCleaned the dataset by transforming data types and removing irrelevant columns.\n\nModeling:\n\nFitted a Random Forest model with cross-validation, optimizing for the ROC metric.\nPredictions were made on the test dataset.\n\nFinal Output:\n\nGenerated a submission file containing predictions.\n\n\n\n\nImprovements and Considerations\n\nThe logistic regression model served as an initial step, indicating the need for refinement.\nThe moderate AUC score from the Random Forest model suggests exploring additional feature engineering or alternative modeling techniques could enhance predictive performance.\n\n\n\nTechnical Documentation\n\nData Loading and Exploration:\n\nEssential libraries for data manipulation and modeling were loaded.\nInitial exploration involved visualizing returns across different attributes to identify trends.\n\nFeature Engineering Function:\n\nTransformed categorical variables and engineered new features to improve model input.\nSimplified the dataset by excluding non-essential columns and adjusting data types.\n\nRandom Forest Modeling:\n\nImplemented a Random Forest model with a focus on the ROC metric for evaluation.\nApplied cross-validation for model training to ensure robustness.\n\nPrediction and Submission:\n\nPredicted probabilities of returns on the test set.\nPrepared the submission file with ID and predicted probabilities.\n\n\n\n\nComments and Clarifications\n\nData Cleaning and Preparation: Made clear the purpose of data transformations and feature engineering to prepare for modeling.\nModel Selection and Evaluation: Explained the choice of models and their evaluation, highlighting the transition from logistic regression to Random Forest for improved accuracy.\nSubmission Process: Detailed the steps for preparing the final submission, ensuring clarity on the expected output format."
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#load-the-required-packages",
    "href": "Content/Projects/Customer Return ML/index.html#load-the-required-packages",
    "title": "Predicting Customer Returns",
    "section": "Load the required packages",
    "text": "Load the required packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)"
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#load-the-training-and-test-data",
    "href": "Content/Projects/Customer Return ML/index.html#load-the-training-and-test-data",
    "title": "Predicting Customer Returns",
    "section": "Load the training and test data",
    "text": "Load the training and test data\n\ntrain &lt;- read_csv(\"../../../Assets/Datasets/customerReturnTrain.csv\") \ntest &lt;- read_csv(\"../../../Assets/Datasets/customerReturnTest.csv\") \n\nglimpse(train)\n\nRows: 64,912\nColumns: 12\n$ ID                &lt;chr&gt; \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           &lt;chr&gt; \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        &lt;chr&gt; \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     &lt;chr&gt; \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate &lt;date&gt; 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         &lt;date&gt; 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment &lt;chr&gt; \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       &lt;chr&gt; \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       &lt;dbl&gt; 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       &lt;dbl&gt; 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     &lt;dbl&gt; 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…"
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#data-exploration",
    "href": "Content/Projects/Customer Return ML/index.html#data-exploration",
    "title": "Predicting Customer Returns",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Look at Product Department\ntrain %&gt;% \n  filter(Returned == 1) %&gt;%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n\n\n\n\n\n\n\n# Look at Product Size\ntrain %&gt;% \n  filter(Returned == 1) %&gt;%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n\n\n\n\n\n\n\n#This won't be that valuable\ntrain %&gt;% \n  mutate(CustomerState = factor(CustomerState)) %&gt;%\n  filter(Returned == 1) %&gt;%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()"
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#feature-engineering",
    "href": "Content/Projects/Customer Return ML/index.html#feature-engineering",
    "title": "Predicting Customer Returns",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Creates the features\nbuildFeatures &lt;- function(ds){\n  CurrentDate &lt;- Sys.Date()\n  ds %&gt;%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP &gt;= 13 & MSRP &lt;= 30 ~ \"$13-$30\",\n                             MSRP &gt; 30 & MSRP &lt;= 60 ~ \"$31-$60\",\n                             MSRP &gt; 60 & MSRP &lt;= 100 ~ \"$61-$100\",\n                             MSRP &gt; 100 ~ \"&gt;$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %&gt;%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols &lt;- test$ID\n\n#Removes and adds columns for train and test sets\ntrain &lt;- buildFeatures(train)\ntest &lt;- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n\nRows: 64,912\nColumns: 11\n$ CustomerState     &lt;fct&gt; Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment &lt;fct&gt; Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       &lt;fct&gt; M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       &lt;dbl&gt; 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       &lt;dbl&gt; 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     &lt;dbl&gt; 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          &lt;fct&gt; No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            &lt;fct&gt; Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       &lt;dbl&gt; 57, 57, 57, 57, 45, 45, 45, 57, 57, 57, 59, 59, 40, …\n$ MSRP              &lt;dbl&gt; 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        &lt;fct&gt; $13-$30, $31-$60, $31-$60, $61-$100, &gt;$100, &gt;$100, $…\n\nsummary(train)\n\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :28.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:38.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :48.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :49.81  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:61.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :78.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   &gt;$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n\ntable(train$Returned)\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#fit-a-random-forest-model",
    "href": "Content/Projects/Customer Return ML/index.html#fit-a-random-forest-model",
    "title": "Predicting Customer Returns",
    "section": "Fit a Random Forest Model",
    "text": "Fit a Random Forest Model\n\nset.seed(345)\n\n#Model using Random Forest \nctrl &lt;- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)\nfit &lt;- train(Returned ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 50,\n             tuneLength = 3,\n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n\nRandom Forest \n\n64912 samples\n   10 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 43274, 43275, 43275 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5551023  1.0000000  0.0000000\n  36    0.6241038  0.8325442  0.3603618\n  70    0.6245050  0.8245271  0.3713335\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 70."
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#make-prediction-on-the-test-data",
    "href": "Content/Projects/Customer Return ML/index.html#make-prediction-on-the-test-data",
    "title": "Predicting Customer Returns",
    "section": "Make prediction on the test data",
    "text": "Make prediction on the test data\n\ntestPredictions &lt;- predict(fit, newdata = test, type = \"prob\")[,2]"
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#writing-the-submission-file",
    "href": "Content/Projects/Customer Return ML/index.html#writing-the-submission-file",
    "title": "Predicting Customer Returns",
    "section": "Writing the Submission File",
    "text": "Writing the Submission File\n\nsubmission &lt;- data.frame(ID = IDCols, Prediction = testPredictions)\n# write.csv(submission, \"submission.csv\", row.names = FALSE)"
  },
  {
    "objectID": "Content/Projects/Customer Return ML/index.html#leftout-features-that-were-considered",
    "href": "Content/Projects/Customer Return ML/index.html#leftout-features-that-were-considered",
    "title": "Predicting Customer Returns",
    "section": "Leftout Features that were considered",
    "text": "Leftout Features that were considered\n\n#odds_ratio &lt;- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %&gt;%  \n#  arrange(desc(odds_ratio)) %&gt;% \n#  head()\n\n#Sampling\n#train_index &lt;- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train &lt;- returns_train[train_index, ]\n#test &lt;- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge &gt;= 18 & CustomerAge &lt;= 30 ~ \"18-30\",\n#                              CustomerAge &gt; 30 & CustomerAge &lt;= 45 ~ \"31-45\",\n#                              CustomerAge &gt; 45 & CustomerAge &lt;= 60 ~ \"46-60\",\n#                              CustomerAge &gt; 60 ~ \"&gt;61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \"&gt;61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)"
  },
  {
    "objectID": "Content/Projects/US_HealthCare_Spending/index.html#abstract",
    "href": "Content/Projects/US_HealthCare_Spending/index.html#abstract",
    "title": "Healthcare Spending Analysis",
    "section": "Abstract",
    "text": "Abstract\nSince 1980, healthcare costs in the United States have been consistently increasing across all categories. Various factors contribute to this rise, such as population growth and higher wages for doctors. This report examines expenditure trends from 1980 to 2005, extending up to 2014. The findings reveal an unprecedented surge in healthcare costs across every sector in the United States. Consequently, this report sheds light on the reasons behind the country’s reputation as one of the world’s most expensive nations in terms of healthcare."
  },
  {
    "objectID": "Content/Projects/US_HealthCare_Spending/index.html#introduction",
    "href": "Content/Projects/US_HealthCare_Spending/index.html#introduction",
    "title": "Healthcare Spending Analysis",
    "section": "Introduction",
    "text": "Introduction\nHealthcare plays a crucial role in our lives, providing essential support for our well-being and longevity. However, healthcare spending continues to soar annually. This report uncovers the alarming reality of escalating healthcare expenditure, presenting a visual representation of each component. It explores overall national spending and delves into individual categories, demonstrating the persistent upward trend in healthcare costs."
  },
  {
    "objectID": "Content/Projects/US_HealthCare_Spending/index.html#background",
    "href": "Content/Projects/US_HealthCare_Spending/index.html#background",
    "title": "Healthcare Spending Analysis",
    "section": "Background",
    "text": "Background\nThe dataset utilized for this report is titled “US Healthcare Spending Per Capita” and was obtained from Kaggle. The dataset’s format posed a challenge, as it followed a wide format with numerous columns and few rows. Notably, the years were presented in the format “Y####,” initially impeding analysis. However, by employing pivoting techniques and manipulating the strings, the dataset was transformed, enabling comprehensive analysis. The subsequent section outlines the complete step-by-step process."
  },
  {
    "objectID": "Content/Projects/US_HealthCare_Spending/index.html#methodology",
    "href": "Content/Projects/US_HealthCare_Spending/index.html#methodology",
    "title": "Healthcare Spending Analysis",
    "section": "Methodology",
    "text": "Methodology\nTo begin, it is essential to assess whether the data is in a “wide” or “long” format. This involves examining the number of rows and columns to facilitate necessary data wrangling.\n\n\n\n\n\n\nVersion Control\n\n\n\nMake sure to use RStudio’s version 2023.12.1 or higher\n\n\n\n\n# A tibble: 5 × 42\n   Code Item     Group Region_Number Region_Name State_Name  Y1980  Y1981  Y1982\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 Persona… Unit…             0 United Sta… &lt;NA&gt;       216977 251789 283073\n2     1 Persona… Regi…             1 New England &lt;NA&gt;        12960  14845  16759\n3     1 Persona… Regi…             2 Mideast     &lt;NA&gt;        43479  49604  55406\n4     1 Persona… Regi…             3 Great Lakes &lt;NA&gt;        40658  46668  51440\n5     1 Persona… Regi…             4 Plains      &lt;NA&gt;        16980  19682  21919\n# ℹ 33 more variables: Y1983 &lt;dbl&gt;, Y1984 &lt;dbl&gt;, Y1985 &lt;dbl&gt;, Y1986 &lt;dbl&gt;,\n#   Y1987 &lt;dbl&gt;, Y1988 &lt;dbl&gt;, Y1989 &lt;dbl&gt;, Y1990 &lt;dbl&gt;, Y1991 &lt;dbl&gt;,\n#   Y1992 &lt;dbl&gt;, Y1993 &lt;dbl&gt;, Y1994 &lt;dbl&gt;, Y1995 &lt;dbl&gt;, Y1996 &lt;dbl&gt;,\n#   Y1997 &lt;dbl&gt;, Y1998 &lt;dbl&gt;, Y1999 &lt;dbl&gt;, Y2000 &lt;dbl&gt;, Y2001 &lt;dbl&gt;,\n#   Y2002 &lt;dbl&gt;, Y2003 &lt;dbl&gt;, Y2004 &lt;dbl&gt;, Y2005 &lt;dbl&gt;, Y2006 &lt;dbl&gt;,\n#   Y2007 &lt;dbl&gt;, Y2008 &lt;dbl&gt;, Y2009 &lt;dbl&gt;, Y2010 &lt;dbl&gt;, Y2011 &lt;dbl&gt;,\n#   Y2012 &lt;dbl&gt;, Y2013 &lt;dbl&gt;, Y2014 &lt;dbl&gt;, …\n\n\n [1] \"Code\"                          \"Item\"                         \n [3] \"Group\"                         \"Region_Number\"                \n [5] \"Region_Name\"                   \"State_Name\"                   \n [7] \"Y1980\"                         \"Y1981\"                        \n [9] \"Y1982\"                         \"Y1983\"                        \n[11] \"Y1984\"                         \"Y1985\"                        \n[13] \"Y1986\"                         \"Y1987\"                        \n[15] \"Y1988\"                         \"Y1989\"                        \n[17] \"Y1990\"                         \"Y1991\"                        \n[19] \"Y1992\"                         \"Y1993\"                        \n[21] \"Y1994\"                         \"Y1995\"                        \n[23] \"Y1996\"                         \"Y1997\"                        \n[25] \"Y1998\"                         \"Y1999\"                        \n[27] \"Y2000\"                         \"Y2001\"                        \n[29] \"Y2002\"                         \"Y2003\"                        \n[31] \"Y2004\"                         \"Y2005\"                        \n[33] \"Y2006\"                         \"Y2007\"                        \n[35] \"Y2008\"                         \"Y2009\"                        \n[37] \"Y2010\"                         \"Y2011\"                        \n[39] \"Y2012\"                         \"Y2013\"                        \n[41] \"Y2014\"                         \"Average_Annual_Percent_Growth\"\n\n\nEarlier, we noticed that the dataset had a wide format, which means the years were in separate columns. To make it easier to analyze, we rearranged the data using a special technique. We combined the year columns into a single “Year” column and placed their corresponding values in a new column called “Cost.”\nWe also made some adjustments to the “Year” column by removing a specific symbol and converting it to numbers. This way, we can work with the years as numeric values instead of text.\nAdditionally, we transformed certain columns into categories, which help us group and analyze the data more effectively. These categories include “Item,” “Region_Name,” “Group,” and “State_Name.”\nFinally, we selected specific columns, including “Item,” “Region_Name,” “State_Name,” “Year,” and “Cost,” to focus on for further analysis. This will provide us with a clearer understanding of the data.\n\n\n# A tibble: 6 × 5\n  Item                 Region_Name   State_Name  Year   Cost\n  &lt;fct&gt;                &lt;fct&gt;         &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Personal Health Care United States &lt;NA&gt;        1980 216977\n2 Personal Health Care United States &lt;NA&gt;        1981 251789\n3 Personal Health Care United States &lt;NA&gt;        1982 283073\n4 Personal Health Care United States &lt;NA&gt;        1983 311677\n5 Personal Health Care United States &lt;NA&gt;        1984 341645\n6 Personal Health Care United States &lt;NA&gt;        1985 376376"
  },
  {
    "objectID": "Content/Projects/US_HealthCare_Spending/index.html#rising-health-care-costs",
    "href": "Content/Projects/US_HealthCare_Spending/index.html#rising-health-care-costs",
    "title": "Healthcare Spending Analysis",
    "section": "Rising Health Care Costs",
    "text": "Rising Health Care Costs\nLet’s jump right into the first visualization. It’s evident that healthcare spending has been consistently increasing and shows no signs of slowing down. This graph focuses on the years 1980 to 2005, highlighting the era of escalating healthcare costs.\n\n\n\n\n\n\n\n\n\n\nDominant Spending Categories: Personal, Hospital, and Physician & Clinical Care\nWow! Personal health care expenses skyrocketed from around $10K to nearly $80K within a relatively short period. Both Hospital and Clinical Care play significant roles in healthcare spending. Surprisingly, all three categories follow a similar upward trend, which reveals some unsettling information.\n\n\n\n\n\n\n\n\n\n\n\nConsistent Trends Across Regions\nIt’s disheartening to report that every region has been experiencing relentless growth in healthcare spending. The trend lines for each region are strikingly similar and proportionate to one another. Notably, the Mideast stands out as the most expensive region, while the Rocky Mountains region appears to have comparatively lower healthcare costs. It’s important to note that this analysis considers spending up to 2005, and we can hope for potential changes by 2014.\n\n\n\n\n\n\n\n\n\n\n\nPersistent Trends: Rising Healthcare Spending Across U.S. Regions\nThe bar chart vividly demonstrates the ongoing trends in healthcare spending across different regions. Notably, the Plains, New England, and the Rocky Mountains regions emerge as some of the lowest in terms of medical funding. Surprisingly, their costs can be as low as one-third compared to the most expensive regions. This stark contrast highlights the significant disparities in healthcare expenditure throughout the United States.\n\n\n\n\n\n\n\n\n\n\n\nFar West: A Surprising 3rd Place in Healthcare Spending\nIn 2014, the Far West region experienced a significant surge in healthcare spending, landing them in the 3rd position. This unexpected leap challenges the assumption that states within this region are heavy spenders. However, the subsequent graphic reveals an intriguing revelation that contradicts this perception.\n\n\n\n\n\n\n\n\n\n[1] \"$21.81M\"\n\n\n\n\nOregon: 3rd Place, but Don’t Be Deceived!\nSurprisingly, Oregon ranks 3rd in healthcare spending. However, let’s not overlook the undeniable fact that California claims the top spot. The massive population size of California is a significant contributing factor to its high expenditure. Although this report doesn’t delve into the specific reasons, it’s plausible that further analysis would align the Far West region more closely with the spending patterns observed in the Plains or New England regions.\n\n\n\n\n\n\n\n\n\n[1] \"$1.41M\"  \"$53.64M\" \"$1.9M\"   \"$3.41M\"  \"$5.81M\"  \"$10.16M\"\n[1] \"$5.81M\"\n\n\n\n\nInappropriate Model: Linear Fit Inadequate for the Data\nAt first glance, the model may seem impressive with an adjusted R-squared value of 0.8572. However, this is deceptive. It’s crucial to note that this model is highly inaccurate and strongly discouraged. The analysis reveals no correlation between Cost and Region_Name per Year, a finding consistent with the filtered dataset covering the years 1980 to 2014.\nThe residual plots provide clear evidence against a linear fit. The Residuals vs Fitted plot indicates a clear quadratic relationship rather than a linear one. The Q-Q plot deviates from linearity, exhibiting multiple curves along the fitted line. Additionally, the scale-location plot highlights that this model is fundamentally unsuitable for the data.\nIt is evident that a linear fit is not the appropriate choice for accurately modeling this dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      1 \n11072.4 \n\n\n\nCall:\nlm(formula = Cost ~ Region_Name + Year, data = regionHealthCareSince2005)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2683.8  -782.9  -279.8   597.7  4139.3 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                -680650.41   24479.29 -27.805  &lt; 2e-16 ***\nRegion_NameGreat Lakes        1438.65     368.55   3.904 0.000130 ***\nRegion_NameMideast            1657.50     368.55   4.497 1.17e-05 ***\nRegion_NameNew England       -3909.68     368.55 -10.608  &lt; 2e-16 ***\nRegion_NamePlains            -3830.75     368.55 -10.394  &lt; 2e-16 ***\nRegion_NameRocky Mountains   -5106.26     368.55 -13.855  &lt; 2e-16 ***\nRegion_NameSoutheast         -1231.18     368.55  -3.341 0.000998 ***\nRegion_NameSouthwest          -849.93     368.55  -2.306 0.022133 *  \nYear                           344.83      12.29  28.069  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1329 on 199 degrees of freedom\nMultiple R-squared:   0.88, Adjusted R-squared:  0.8752 \nF-statistic: 182.4 on 8 and 199 DF,  p-value: &lt; 2.2e-16\n\n\n             Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \nRegion_Name   7 1.185e+09 1.693e+08    95.9 &lt;2e-16 ***\nYear          1 1.391e+09 1.391e+09   787.9 &lt;2e-16 ***\nResiduals   199 3.514e+08 1.766e+06                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nSignificant Differences in Means among Regions\nMy objective was to investigate whether there were significant differences in the means of each region’s spending over the years. To start, I utilized the LeveneTest to examine the importance of variance (i.e., the spread of spending) across regions. Both tests yielded remarkably small p-values, indicating that three regions had substantially different variances compared to the others.\nBuilding on this, I employed the TukeyHsd test to confirm if these differing variances were reflected in the means. As anticipated from the LeveneTest results, the means of these regions indeed exhibited significant differences. Notably, New England, Plains, and Rocky Mountains had considerably lower average spending. However, despite their comparatively lower spending, these regions still followed the overall growth trend, with an increase of 18% since 2005.\n\n\n[1] \"The Average Spending In The Expensive Regions since 2005 = $5333.29\"\n\n\n[1] \"The Average Spending In The Expensive Regions since 2014= $7724.45\"\n\n\n[1] \"Difference: +$2391.16 | Percentage Increase: +18.31%\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2005 = $2173.22\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2014= $3142.21\"\n\n\n[1] \"Difference: +$968.99 | Percentage Increase: 18.23%\"\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(&gt;F)    \ngroup   7   12.03 8.727e-13 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   7  11.462 3.292e-12 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(&gt;F)    \ngroup   7  19.546 &lt; 2.2e-16 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   7  12.828 3.075e-14 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n             Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \nRegion_Name   7 1.185e+09 169337440   19.43 &lt;2e-16 ***\nResiduals   200 1.743e+09   8712933                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Cost ~ Region_Name, data = regionHealthCareSince2005)\n\n$Region_Name\n                                   diff         lwr           upr     p adj\nGreat Lakes-Far West         1438.64777 -1069.19906  3946.4945990 0.6495584\nMideast-Far West             1657.50100  -850.34583  4165.3478291 0.4679930\nNew England-Far West        -3909.68132 -6417.52815 -1401.8344885 0.0000930\nPlains-Far West             -3830.75287 -6338.59970 -1322.9060420 0.0001417\nRocky Mountains-Far West    -5106.25783 -7614.10466 -2598.4109954 0.0000001\nSoutheast-Far West          -1231.18113 -3739.02796  1276.6657036 0.8046614\nSouthwest-Far West           -849.92577 -3357.77260  1657.9210559 0.9679983\nMideast-Great Lakes           218.85323 -2288.99360  2726.7000602 0.9999950\nNew England-Great Lakes     -5348.32909 -7856.17592 -2840.4822574 0.0000000\nPlains-Great Lakes          -5269.40064 -7777.24747 -2761.5538109 0.0000000\nRocky Mountains-Great Lakes -6544.90559 -9052.75242 -4037.0587643 0.0000000\nSoutheast-Great Lakes       -2669.82890 -5177.67573  -161.9820653 0.0279871\nSouthwest-Great Lakes       -2288.57354 -4796.42037   219.2732870 0.1019616\nNew England-Mideast         -5567.18232 -8075.02915 -3059.3354875 0.0000000\nPlains-Mideast              -5488.25387 -7996.10070 -2980.4070410 0.0000000\nRocky Mountains-Mideast     -6763.75882 -9271.60565 -4255.9119944 0.0000000\nSoutheast-Mideast           -2888.68213 -5396.52896  -380.8352954 0.0119419\nSouthwest-Mideast           -2507.42677 -5015.27360     0.4200569 0.0500724\nPlains-New England             78.92845 -2428.91838  2586.7752767 1.0000000\nRocky Mountains-New England -1196.57651 -3704.42334  1311.2703233 0.8267302\nSoutheast-New England        2678.50019   170.65336  5186.3470223 0.0270977\nSouthwest-New England        3059.75554   551.90871  5567.6023746 0.0058329\nRocky Mountains-Plains      -1275.50495 -3783.35178  1232.3418768 0.7745369\nSoutheast-Plains             2599.57175    91.72492  5107.4185757 0.0361922\nSouthwest-Plains             2980.82710   472.98027  5488.6739280 0.0081616\nSoutheast-Rocky Mountains    3875.07670  1367.22987  6382.9235291 0.0001119\nSouthwest-Rocky Mountains    4256.33205  1748.48522  6764.1788814 0.0000135\nSouthwest-Southeast           381.25535 -2126.59148  2889.1021825 0.9997829"
  },
  {
    "objectID": "Content/Projects/US_HealthCare_Spending/index.html#results",
    "href": "Content/Projects/US_HealthCare_Spending/index.html#results",
    "title": "Healthcare Spending Analysis",
    "section": "Results",
    "text": "Results\nThe cost of healthcare in the United States has increased more than fivefold between 1980 and 2014. Across regions, there is a consistent upward trend in healthcare spending with no clear indications of a decrease. Although some regions are less expensive than others, their growth rates align with the national average. Personal health care spending, which averaged around $10,000 in 1980, has significantly risen to nearly $80,000 in 2014.\nAs of 2014, the Mideast, Great Lakes, and Far West regions rank as the top three most expensive regions, while the Rocky Mountains, New England, and Plains regions are the least expensive.\nWithin the Far West region, Oregon stands out as the third most expensive state."
  },
  {
    "objectID": "Content/Projects/US_HealthCare_Spending/index.html#conclusion",
    "href": "Content/Projects/US_HealthCare_Spending/index.html#conclusion",
    "title": "Healthcare Spending Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThe United States continues to experience escalating healthcare expenditures, raising concerns about the affordability of personal health care. The substantial increase of approximately $70,000 over a span of 35 years far exceeds inflation expectations. It would have been beneficial to have inflation-adjusted values in the dataset, allowing for a more comprehensive analysis and deeper insights.\nFurther exploration can be done to investigate potential statistical significance between individual states and their spending patterns. This avenue remains open for future researchers to delve into for a more in-depth understanding of healthcare expenditure variations."
  },
  {
    "objectID": "Content/Projects/Build-a-Dashboard/index.html",
    "href": "Content/Projects/Build-a-Dashboard/index.html",
    "title": "Olympic Dashboard",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gt)\nlibrary(scales)\nlibrary(plotly)\nlibrary(forcats)\nolympics_full &lt;- read_csv(\"https://raw.githubusercontent.com/bcervantesalvarez/Portfolio/main/Assets/Datasets/olympics.csv\")\nolympics &lt;- olympics_full |&gt;\n  filter(!is.na(medal)) |&gt;\n  separate_wider_delim(cols = team, names = c(\"team\", \"suffix\"), delim = \"-\", too_many = \"merge\", too_few = \"align_start\") |&gt;\n  select(-suffix) |&gt;\n  mutate(medal = fct_relevel(medal, \"Bronze\", \"Silver\", \"Gold\"))"
  },
  {
    "objectID": "Content/Projects/Build-a-Dashboard/index.html#column---medals-by-sport-and-year",
    "href": "Content/Projects/Build-a-Dashboard/index.html#column---medals-by-sport-and-year",
    "title": "Olympic Dashboard",
    "section": "Column - Medals by sport and year",
    "text": "Column - Medals by sport and year\n\nRow - Medals by sport\n\nplot_data &lt;- summer_olympics |&gt;\n  mutate(\n    sport = fct_lump_n(sport, n = 15),\n    sport = fct_infreq(sport),\n    sport = fct_rev(sport),\n    sport = fct_relevel(sport, \"Other\", after = 0)\n  ) |&gt;\n  count(sport, medal)\n\nplot_ly(\n  plot_data,\n  x = ~n,\n  y = ~sport,\n  color = ~medal,\n  colors = c(\"Gold\" = \"#d4af37\", \"Silver\" = \"#c0c0c0\", \"Bronze\" = \"#cd7f32\"),\n  type = 'bar',\n  orientation = 'h'\n) |&gt;\n  layout(\n    barmode = 'stack',\n    xaxis = list(\n      title = \"Medals\",\n      tickvals = seq(0, 7000, 1000),\n      ticktext = format(seq(0, 7000, 1000), big.mark = \",\")\n    ),\n    yaxis = list(title = \"Sport\"),\n    legend = list(\n      x = 0.8, y = 0.2,\n      orientation = 'h',\n      bgcolor = 'white',\n      bordercolor = 'gray',\n      borderwidth = 1\n    ),\n    margin = list(l = 60, r = 30, t = 20, b = 30)\n  )\n\n\n\n\n\n\n\nRow - Medals by year\n\nDue to World War II, no olympic games were held in 1940 and 1944.\n\n# Prepare the data\nplot_data &lt;- summer_olympics |&gt;\n  count(year, medal)\n\n# Create the plotly plot\nplot_ly(\n  plot_data,\n  x = ~year,\n  y = ~n,\n  type = 'scatter',\n  mode = 'lines+markers',\n  color = ~medal,\n  colors = c(\"Gold\" = \"#d4af37\", \"Silver\" = \"#c0c0c0\", \"Bronze\" = \"#cd7f32\"),\n  marker = list(size = 8, color = 'white', symbol = 'circle')\n) |&gt;\n  layout(\n    xaxis = list(\n      title = \"Year\",\n      tickvals = seq(1896, 2020, 8)\n    ),\n    yaxis = list(title = \"Medals\"),\n    legend = list(\n      x = 0.8, y = 0.2,\n      orientation = 'h',\n      bgcolor = 'white',\n      bordercolor = 'gray',\n      borderwidth = 1\n    ),\n    margin = list(l = 60, r = 30, t = 20, b = 30)\n  )"
  },
  {
    "objectID": "Content/Projects/Build-a-Dashboard/index.html#column---medals-by-country",
    "href": "Content/Projects/Build-a-Dashboard/index.html#column---medals-by-country",
    "title": "Olympic Dashboard",
    "section": "Column - Medals by country",
    "text": "Column - Medals by country\n\nRow - Value boxes\n\nsummer_most_golds &lt;- summer_olympics |&gt;\n  filter(medal == \"Gold\") |&gt;\n  count(team, sort = TRUE) |&gt;\n  slice_head(n = 1)\n\nsummer_most_silvers &lt;- summer_olympics |&gt;\n  filter(medal == \"Silver\") |&gt;\n  count(team, sort = TRUE) |&gt;\n  slice_head(n = 1)\n\nsummer_most_bronzes &lt;- summer_olympics |&gt;\n  filter(medal == \"Bronze\") |&gt;\n  count(team, sort = TRUE) |&gt;\n  slice_head(n = 1)\n\n\nMost golds:\n2363\nUnited States\n\n\nMost silvers:\n1251\nUnited States\n\n\nMost bronzes:\n1126\nUnited States\n\n\n\nRow - Tabsets of tables\n\nTeams sorted in descending order of total medals.\n\nsummer_olympics |&gt;\n  count(team, medal) |&gt;\n  pivot_wider(names_from = medal, values_from = n, values_fill = 0) |&gt;\n  mutate(total = Bronze + Gold + Silver) |&gt;\n  arrange(desc(total), team) |&gt;\n  slice_head(n = 30) |&gt;\n  mutate(Rank = c(1:30)) |&gt;\n  select(Team = team, Gold, Silver, Bronze) |&gt;\n  gt() |&gt;\n  cols_align(align = \"left\", columns = Team) |&gt;\n  data_color(\n    method = \"numeric\",\n    palette = \"nord::aurora\"\n  ) |&gt;\n  opt_interactive(use_compact_mode = TRUE)\n\n\n\n\n\n\n\n\n\n\nTeams sorted in ascending order of total medals.\n\nsummer_olympics |&gt;\n  count(team, medal) |&gt;\n  pivot_wider(names_from = medal, values_from = n, values_fill = 0) |&gt;\n  mutate(total = Bronze + Gold + Silver) |&gt;\n  arrange(total, team) |&gt;\n  slice_head(n = 30) |&gt;\n  select(Team = team, Gold, Silver, Bronze) |&gt;\n  gt() |&gt;\n  cols_align(align = \"left\", columns = Team) |&gt;\n  data_color(\n    method = \"numeric\",\n    palette = \"nord::frost\"\n  ) |&gt;\n  opt_interactive(use_compact_mode = TRUE,)"
  },
  {
    "objectID": "Content/Projects/Build-a-Dashboard/index.html#column---medals-by-sport-and-year-1",
    "href": "Content/Projects/Build-a-Dashboard/index.html#column---medals-by-sport-and-year-1",
    "title": "Olympic Dashboard",
    "section": "Column - Medals by sport and year",
    "text": "Column - Medals by sport and year\n\nRow - Medals by sport\n\n# Prepare the data\nplot_data &lt;- winter_olympics |&gt;\n  mutate(\n    sport = fct_lump_n(sport, n = 15),\n    sport = fct_infreq(sport),\n    sport = fct_rev(sport),\n    sport = fct_relevel(sport, \"Other\", after = 0)\n  ) |&gt;\n  count(sport, medal)\n\n# Create the plotly plot\nplot_ly(\n  plot_data,\n  x = ~n,\n  y = ~sport,\n  color = ~medal,\n  colors = c(\"Gold\" = \"#d4af37\", \"Silver\" = \"#c0c0c0\", \"Bronze\" = \"#cd7f32\"),\n  type = 'bar',\n  orientation = 'h'\n) |&gt;\n  layout(\n    barmode = 'stack',\n    xaxis = list(\n      title = \"Medals\",\n      tickvals = seq(0, 1500, 250),\n      ticktext = format(seq(0, 1500, 250), big.mark = \",\")\n    ),\n    yaxis = list(title = \"Sport\"),\n    legend = list(\n      x = 0.7, y = 0.2,\n      orientation = 'h',\n      bgcolor = 'white',\n      bordercolor = 'gray',\n      borderwidth = 1\n    ),\n    margin = list(l = 60, r = 30, t = 20, b = 30)\n  )\n\n\n\n\n\n\n\nRow - Medals by year\n\nDue to World War II, no olympic games were held in 1940 and 1944.\n\nlibrary(dplyr)\nlibrary(plotly)\n\n# Prepare the data\nplot_data &lt;- winter_olympics |&gt;\n  count(year, medal)\n\n# Create the plotly plot\nplot_ly(\n  plot_data,\n  x = ~year,\n  y = ~n,\n  type = 'scatter',\n  mode = 'lines+markers',\n  color = ~medal,\n  colors = c(\"Gold\" = \"#d4af37\", \"Silver\" = \"#c0c0c0\", \"Bronze\" = \"#cd7f32\"),\n  marker = list(size = 8, color = 'white', symbol = 'circle')\n) |&gt;\n  layout(\n    xaxis = list(\n      title = \"Year\",\n      tickvals = seq(1896, 2020, 8)\n    ),\n    yaxis = list(title = \"Medals\"),\n    legend = list(\n      x = 0.2, y = 0.8,\n      orientation = 'h',\n      bgcolor = 'white',\n      bordercolor = 'gray',\n      borderwidth = 1\n    ),\n    margin = list(l = 60, r = 30, t = 20, b = 30)\n  )"
  },
  {
    "objectID": "Content/Projects/Build-a-Dashboard/index.html#column---medals-by-country-1",
    "href": "Content/Projects/Build-a-Dashboard/index.html#column---medals-by-country-1",
    "title": "Olympic Dashboard",
    "section": "Column - Medals by country",
    "text": "Column - Medals by country\n\nRow - Value boxes\n\nwinter_most_golds &lt;- winter_olympics |&gt;\n  filter(medal == \"Gold\") |&gt;\n  count(team, sort = TRUE) |&gt;\n  slice_head(n = 1)\n\nwinter_most_silvers &lt;- winter_olympics |&gt;\n  filter(medal == \"Silver\") |&gt;\n  count(team, sort = TRUE) |&gt;\n  slice_head(n = 1)\n\nwinter_most_bronzes &lt;- winter_olympics |&gt;\n  filter(medal == \"Bronze\") |&gt;\n  count(team, sort = TRUE) |&gt;\n  slice_head(n = 1)\n\n\nMost golds:\n305\nCanada\n\n\nMost silvers:\n308\nUnited States\n\n\nMost bronzes:\n215\nFinland\n\n\n\nRow - Tabsets of tables\n\nTeams sorted in descending order of total medals.\n\nwinter_olympics |&gt;\n  count(team, medal) |&gt;\n  pivot_wider(names_from = medal, values_from = n, values_fill = 0) |&gt;\n  mutate(total = Bronze + Gold + Silver) |&gt;\n  arrange(desc(total), team) |&gt;\n  slice_head(n = 30) |&gt;\n  select(Team = team, Gold, Silver, Bronze) |&gt;\n  gt() |&gt;\n  cols_align(align = \"left\", columns = Team) |&gt;\n  data_color(\n    method = \"numeric\",\n    palette = \"nord::aurora\"\n  ) |&gt;\n  opt_interactive(use_compact_mode = TRUE,)\n\n\n\n\n\n\n\n\n\n\nTeams sorted in ascending order of total medals.\n\nwinter_olympics |&gt;\n  count(team, medal) |&gt;\n  pivot_wider(names_from = medal, values_from = n, values_fill = 0) |&gt;\n  mutate(total = Bronze + Gold + Silver) |&gt;\n  arrange(total, team) |&gt;\n  slice_head(n = 30) |&gt;\n  select(Team = team, Gold, Silver, Bronze) |&gt;\n  gt() |&gt;\n  cols_align(align = \"left\", columns = Team) |&gt;\n  data_color(\n    method = \"numeric\",\n    palette = \"nord::frost\"\n  ) |&gt;\n  opt_interactive(use_compact_mode = TRUE,)"
  },
  {
    "objectID": "Content/Projects/index.html",
    "href": "Content/Projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Applied Multivariate Analysis\n\n\nExploring wine attributes through EDA, MANOVA, and classification models reveals significant chemical differences and predictors of wine type and…\n\n\n\nBrian Cervantes Alvarez\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealthcare Spending Analysis\n\n\nThis report examines the surging healthcare costs in the U.S. from 1980 to 2014, revealing the factors behind its status as one of the world’s most…\n\n\n\nBrian Cervantes Alvarez\n\n\nDec 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Dashboard\n\n\nThe 100m Dashboard! Dash your way to victory and look beyond! Take your gold medal and observe that this dashboard was built using just quarto.\n\n\n\nPosit::conf2024\n\n\nSep 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting $50K+ Incomes\n\n\nExpertly predicting $50K+ incomes through ML, our project highlights the synergy of data science skills and team collaboration.\n\n\n\nBrian Cervantes Alvarez, Willa Van Liew\n\n\nApr 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Customer Returns\n\n\nLeveraging logistic regression and random forest to advance the prediction of retail item returns with increased precision.\n\n\n\nBrian Cervantes Alvarez\n\n\nMar 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Wine Province Origin\n\n\nOur project sets a new bar in wine origin identification, transforming how industry professionals use critic data.\n\n\n\nBrian Cervantes Alvarez\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymptom Severity Analysis\n\n\nUsing symptom analysis, this study employs a Random Forest approach to predict severity levels in patients, aiming to enhance healthcare…\n\n\n\nBrian Cervantes Alvarez\n\n\nJun 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Accident Time Series\n\n\nThis accident time series app allows you to explore and analyze data related accidents in UK. Feel free to download the data and explore other…\n\n\n\nBrian Cervantes Alvarez\n\n\nSep 24, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Content/Talks/TeachingwithWebR/index.html",
    "href": "Content/Talks/TeachingwithWebR/index.html",
    "title": "Interactive Teaching with webR",
    "section": "",
    "text": "webR is a version of R that runs directly in a web browser.\nIt eliminates the need for local installation or server setup.\nIt just works (even on your phone). Can be easily implemented using quarto."
  },
  {
    "objectID": "Content/Talks/TeachingwithWebR/index.html#what-is-webr",
    "href": "Content/Talks/TeachingwithWebR/index.html#what-is-webr",
    "title": "Interactive Teaching with webR",
    "section": "",
    "text": "webR is a version of R that runs directly in a web browser.\nIt eliminates the need for local installation or server setup.\nIt just works (even on your phone). Can be easily implemented using quarto."
  },
  {
    "objectID": "Content/Talks/TeachingwithWebR/index.html#slides",
    "href": "Content/Talks/TeachingwithWebR/index.html#slides",
    "title": "Interactive Teaching with webR",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "Content/Blog/FraudML/index.html#abstract",
    "href": "Content/Blog/FraudML/index.html#abstract",
    "title": "Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Abstract",
    "text": "Abstract\nFraud detection is a challenging task, but advancements are being made each year. This project aims to improve fraud detection using a Gradient Boosting Machine (GBM) model. The dataset is prepared, and the model is trained and evaluated on validation and test sets. Performance metrics such as accuracy, precision, recall, AUC, and Kappa coefficient are computed. A comparison is made with the existing ‘isFlaggedFraud’ column model. The GBM model demonstrates higher performance in various aspects, highlighting its superiority in classifying fraudulent transactions. However, further improvements are needed to enhance fraud detection while maintaining precision."
  },
  {
    "objectID": "Content/Blog/FraudML/index.html#introduction",
    "href": "Content/Blog/FraudML/index.html#introduction",
    "title": "Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Introduction",
    "text": "Introduction\nFraudulent activities pose a significant threat in various industries, including financial transactions. Detecting fraudulent transactions is a complex task, but advancements in machine learning techniques have shown promise in improving accuracy and precision. In this project, we focus on enhancing fraud detection using a Gradient Boosting Machine (GBM) model."
  },
  {
    "objectID": "Content/Blog/FraudML/index.html#methodology",
    "href": "Content/Blog/FraudML/index.html#methodology",
    "title": "Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Methodology",
    "text": "Methodology\nFirst, we set up the environment by importing necessary libraries and loading the dataset. The dataset, named ‘Card-Transaction_log.csv’, contains relevant information for training and evaluation. We explore the dataset by printing the first few rows and examining its shape.\nNext, we perform data wrangling by applying one-hot encoding to the features, excluding ‘isFraud’, ‘nameOrig’, ‘nameDest’, and ‘isFlaggedFraud’ columns. This process prepares the data for training the GBM model.\nTo evaluate the model’s performance, we split the data into training, validation, and test sets. The random seed is set for reproducibility. The GBM model is trained on the training set using the specified parameters, such as loss function, learning rate, number of estimators, and maximum depth. The model’s training time is recorded for analysis.\nWe then evaluate the GBM model on the validation set to measure its accuracy, precision, recall, confusion matrix, AUC, and Kappa coefficient. These metrics provide insights into the model’s performance in classifying fraudulent and non-fraudulent transactions.\nFurther, we assess the GBM model’s performance on the test set and compute the corresponding metrics. The test accuracy, precision, recall, confusion matrix, AUC, and Kappa coefficient provide a comprehensive evaluation of the model’s ability to classify transactions accurately.\nTo compare the GBM model with the existing ‘isFlaggedFraud’ column model, we calculate the accuracy, precision, and recall scores for the ‘isFlaggedFraud’ column. This analysis allows us to assess the GBM model’s superiority in detecting fraudulent transactions.\nBased on the results, we discuss the GBM model’s performance, highlighting its accuracy, precision, recall, AUC, and Kappa coefficient. We analyze the confusion matrix to identify areas where the model misclassifies transactions. While the GBM model outperforms the ‘isFlaggedFraud’ column model in several aspects, it still has room for improvement in detecting fraudulent instances while maintaining high precision.\n\n\n\n\n\n\nCaution\n\n\n\nEnsure you are using the correct Python version: Python 3.10.9 (‘base’) ~/anaconda3/bin/python\n\n\n\nSet up\n\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.ensemble import GradientBoostingClassifier  # For classification tasks\nfrom sklearn.ensemble import GradientBoostingRegressor  # For regression tasks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n    roc_curve,\n    cohen_kappa_score,\n)\n\nds = pd.read_csv('../../../Assets/Datasets/Card-Transaction_log.csv')\n\n# Print the first 5 rows\nprint(ds.head(5))\n\n# Print the shape of the frame\nprint(ds.shape)\n\n   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n\n      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n0  M1979787155             0.0             0.0        0               0  \n1  M2044282225             0.0             0.0        0               0  \n2   C553264065             0.0             0.0        1               0  \n3    C38997010         21182.0             0.0        1               0  \n4  M1230701703             0.0             0.0        0               0  \n(6362620, 11)\n\n\n\n\nData Wrangling\n\nX = pd.get_dummies(ds.drop(['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1))   # Perform one-hot encoding on the features\ny = ds['isFraud']\n\n\n\n\n\n\n\nNote\n\n\n\nThis dataset was already cleaned and tidy, so no additional wrangling was needed.\n\n\n\n\nSplit the data into training, validation, and test sets\n\n# Set the random seed\nnp.random.seed(57)\n\n# Training split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n\n\n# Train the model\nstart_time = time.time()  # Start the timer\n\ngbm = GradientBoostingClassifier(loss='log_loss', learning_rate=0.1, n_estimators=100, max_depth=3)\ngbm.fit(X_train, y_train)\n\nend_time = time.time()  # Stop the timer\n\nelapsed_time = end_time - start_time\nprint(f\"Elapsed Time: {elapsed_time} seconds\")\n\nElapsed Time: 1081.4407150745392 seconds\n\n\n\n\nEvaluate on the validation set\n\ny_pred_val = gbm.predict(X_val)\naccuracy_val = accuracy_score(y_val, y_pred_val)\nprecision_val = precision_score(y_val, y_pred_val)\nrecall_val = recall_score(y_val, y_pred_val)\nconfusion_matrix_val = confusion_matrix(y_val, y_pred_val)\nauc_val = roc_auc_score(y_val, y_pred_val)\nfpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_pred_val)\nkappa_val = cohen_kappa_score(y_val, y_pred_val)\n\nprint(f\"Validation Accuracy: {accuracy_val}\")\nprint(f\"Validation Precision: {precision_val}\")\nprint(f\"Validation Recall: {recall_val}\")\nprint(\"Validation Confusion Matrix:\")\nprint(confusion_matrix_val)\nprint(f\"Validation AUC: {auc_val}\")\nprint(f\"Validation Kappa: {kappa_val}\")\n\nValidation Accuracy: 0.9993241777758219\nValidation Precision: 0.9693396226415094\nValidation Recall: 0.4963768115942029\nValidation Confusion Matrix:\n[[635421     13]\n [   417    411]]\nValidation AUC: 0.7481781765679447\nValidation Kappa: 0.6562465275067701\n\n\n\n\nEvaluate on the test set\n\ny_pred_test = gbm.predict(X_test)\naccuracy_test = accuracy_score(y_test, y_pred_test)\nprecision_test = precision_score(y_test, y_pred_test)\nrecall_test = recall_score(y_test, y_pred_test)\nconfusion_matrix_test = confusion_matrix(y_test, y_pred_test)\nauc_test = roc_auc_score(y_test, y_pred_test)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_test)\nkappa_test = cohen_kappa_score(y_test, y_pred_test)\n\nprint(f\"Test Accuracy: {accuracy_test}\")\nprint(f\"Test Precision: {precision_test}\")\nprint(f\"Test Recall: {recall_test}\")\nprint(\"Test Confusion Matrix:\")\nprint(confusion_matrix_test)\nprint(f\"Test AUC: {auc_test}\")\nprint(f\"Test Kappa: {kappa_test}\")\n\nTest Accuracy: 0.9992958875431819\nTest Precision: 0.9455958549222798\nTest Recall: 0.46085858585858586\nTest Confusion Matrix:\n[[635449     21]\n [   427    365]]\nTest AUC: 0.7304127697259946\nTest Kappa: 0.6193839067580158\n\n\n\n\nHow does this compare to the current model?\n\n# Comparison with 'isFlaggedFraud' column\nis_flagged_fraud_accuracy = accuracy_score(ds['isFraud'], ds['isFlaggedFraud'])\nis_flagged_fraud_precision = precision_score(ds['isFraud'], ds['isFlaggedFraud'])\nis_flagged_fraud_recall = recall_score(ds['isFraud'], ds['isFlaggedFraud'])\n\nprint(f\"Accuracy (isFlaggedFraud): {is_flagged_fraud_accuracy}\")\nprint(f\"Precision (isFlaggedFraud): {is_flagged_fraud_precision}\")\nprint(f\"Recall (isFlaggedFraud): {is_flagged_fraud_recall}\")\n\nAccuracy (isFlaggedFraud): 0.9987116942391656\nPrecision (isFlaggedFraud): 1.0\nRecall (isFlaggedFraud): 0.0019481310118105442"
  },
  {
    "objectID": "Content/Blog/FraudML/index.html#results",
    "href": "Content/Blog/FraudML/index.html#results",
    "title": "Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Results",
    "text": "Results\nBased on the evaluation of the GBM (Gradient Boosting Machine) model on the test set, the following performance metrics were obtained:\n\nTest Accuracy: 0.9992958875431819\nTest Precision: 0.9455958549222798\nTest Recall: 0.46085858585858586\nTest AUC: 0.7304127697259946\nTest Kappa: 0.6193839067580158\n\nThe GBM model achieved a high accuracy score of 0.999, indicating a strong ability to correctly classify transactions in the test set. With a precision of 0.946, the GBM model accurately identified approximately 94.6% of the predicted fraudulent transactions. However, the recall score of 0.461 suggests that the GBM model only captured around 46.1% of the actual fraudulent transactions.\nAnalyzing the confusion matrix, the GBM model correctly classified a large number of non-fraudulent transactions (true negatives) and a significant portion of fraudulent transactions (true positives). However, there were some instances where the GBM model misclassified non-fraudulent transactions as fraudulent (false positives) and failed to identify certain fraudulent transactions (false negatives).\nThe AUC score of 0.730 indicates a moderate level of discrimination between fraudulent and non-fraudulent transactions for the GBM model. Although the GBM model demonstrates substantial agreement beyond chance with a Kappa coefficient of 0.619, there is room for improvement in its ability to detect fraudulent instances while maintaining high precision.\n\nComparison with Original Model\nComparing the GBM model with the ‘isFlaggedFraud’ column model, the GBM model outperforms in several key aspects. It achieves higher accuracy, precision, recall, and AUC scores, indicating superior overall performance in classifying fraudulent transactions. The ‘isFlaggedFraud’ column model, although having perfect precision, only detects a very small number of actual fraud cases, resulting in low recall."
  },
  {
    "objectID": "Content/Blog/FraudML/index.html#limitations-and-further-study",
    "href": "Content/Blog/FraudML/index.html#limitations-and-further-study",
    "title": "Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Limitations and Further Study",
    "text": "Limitations and Further Study\nIt is important to consider the specific goals and requirements of the application. While the GBM model provides better overall performance, it may have a higher number of false positives compared to the ‘isFlaggedFraud’ column model. Further enhancements can be made to improve the GBM model’s ability to detect fraudulent transactions while maintaining high precision.\nThese results provide valuable insights into the GBM model’s performance on unseen data, indicating its generalization capability. It is crucial to consider the test set performance as the final evaluation of the GBM model’s effectiveness in real-world scenarios."
  },
  {
    "objectID": "Content/Blog/US_HealthIns_Costs/index.html#abstract",
    "href": "Content/Blog/US_HealthIns_Costs/index.html#abstract",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Abstract:",
    "text": "Abstract:\nThis study confirms the significant impact of smoking on the escalation of health insurance premiums. Male and female smokers with a body mass index (BMI) of 30 or higher face additional charges, compounding their financial burden. Male smokers experience a 52% increase, while female smokers face a 49% rise in insurance charges, in addition to the base premium for smokers.\nThe severity of the situation is evident, as male smokers pay 408.57% more than non-smokers, and female smokers pay 350.12% more. The data unequivocally supports the notion that unhealthy lifestyle choices, such as smoking and high BMI, result in higher health insurance premiums. It is important to note that premiums also increase gradually over time with age.\nWhile this project provides valuable insights, further exploration opportunities exist. Applying machine learning techniques to assess the representativeness of the sample could enhance the accuracy of conclusions and foster advancements in the field of health insurance."
  },
  {
    "objectID": "Content/Blog/US_HealthIns_Costs/index.html#data-setup-and-import",
    "href": "Content/Blog/US_HealthIns_Costs/index.html#data-setup-and-import",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Data Setup and Import",
    "text": "Data Setup and Import\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n#read in the csv file\ndf = pd.read_csv(\"../../../Assets/Datasets/insurance.csv\")\nprint(df.head())\n\n   age     sex     bmi  children smoker     region      charges\n0   19  female  27.900         0    yes  southwest  16884.92400\n1   18    male  33.770         1     no  southeast   1725.55230\n2   28    male  33.000         3     no  southeast   4449.46200\n3   33    male  22.705         0     no  northwest  21984.47061\n4   32    male  28.880         0     no  northwest   3866.85520"
  },
  {
    "objectID": "Content/Blog/US_HealthIns_Costs/index.html#analysis-of-factors-affecting-insurance-costs",
    "href": "Content/Blog/US_HealthIns_Costs/index.html#analysis-of-factors-affecting-insurance-costs",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Analysis of Factors Affecting Insurance Costs:",
    "text": "Analysis of Factors Affecting Insurance Costs:\nThe objective of this analysis is to identify the factors influencing insurance costs and determine their impact on individuals’ financial burden. Specifically, we investigate the relationship between smoking habits and increased medical insurance expenses.\nInitially, we examine the prevalence of smoking within the dataset. Subsequently, we conduct a comprehensive assessment of the distribution of charges among smokers, employing visual aids like box plots to illustrate quartiles and identify any outliers. Statistical measures, including minimum, maximum, mean, and median, are then employed to provide a more comprehensive understanding of the data.\nFurthermore, we conduct a gender-based analysis to ascertain if there are notable differences in expenses between male and female smokers. This allows us to evaluate any significant variations in insurance costs based on gender.\n\nHistogram Plot of Smokers vs Nonsmokers\n\nSmoker Count: 274\nNonsmoker Count: 1064\n\nTo visually illustrate the distribution of smokers and nonsmokers within the dataset, a histogram plot was generated. The plot showcases the count of individuals categorized as smokers or nonsmokers. The number of smokers is 274, while the count of nonsmokers is 1064.\n\n#while a histogram is not necessary, it can really show in the visual count of smokers vs nonsmokers\nplt.figure(figsize = (8, 6))\nsns.set(font_scale = 1.5)\nsns.histplot(df.smoker)\nplt.title('Smoker vs Nonsmoker Count')\n\nplt.tight_layout(pad = 2)\nplt.show()\nplt.clf()\n\n#num_smokers = df.smoker.value_counts()['yes']\n#num_nonsmokers = df.smoker.value_counts()['no']\n#print(\"Exact number of smokers: {num_smokers}\".format(num_smokers = num_smokers))\n#print(\"Exact number of nonsmokers: {num_nonsmokers}\".format(num_nonsmokers = num_nonsmokers))\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\nBoxplot and Histogram Distribution of Insurance Costs for Smokers\nUpon analyzing the insurance costs specifically for smokers, a boxplot and histogram were constructed. The interquartile range indicates a substantial spread of $20,125.33 between the first and third quartiles.\nNotably, the distribution of insurance costs for smokers exhibits a bimodal pattern, suggesting the presence of a contributing factor that significantly impacts insurance expenses. Further investigation is required to identify the specific variable responsible for the observed increase in insurance costs among smokers. Potential factors to consider include region, BMI, sex, number of children, and age.\n\ndf_smokers = df[df.smoker == 'yes']\n#print(df_smokers.head())\nsns.set(font_scale = 0.8)\nIQR = stats.iqr(df_smokers.charges, interpolation = 'midpoint')\n#print(round(IQR,2))\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,6))\n#boxplot\nsns.boxplot(y = df_smokers.charges, ax = ax1)\nax1.set_ylabel('Insurance cost (USD)')\nax1.set_title('Insurance Cost Boxplot for Smokers')\n\n#histogram\nsns.histplot(df_smokers.charges, bins = 40, ax = ax2)\nax2.set_xlabel('Insurance Cost (USD)')\nax2.set_title('Insurance Cost Distribution for Smokers')\n\nplt.tight_layout(pad = 3)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\nInsurance Cost Statistics for Smokers\nSummary Stats:\n\nMean: $32,050.23\nMedian: $34,456.35\nMax: $63,770.43\nMin: $12,829.46\nStandard Deviation: $11,520.47\nVariance: 132,721,153.14\n\n\n#find mean of insurance cost for smokers\nsmoker_mean = round(np.mean(df_smokers.charges), 2)\n#print('The average insurance cost for being a smoker: ${mean}'.format(mean = smoker_mean))\n\n#find median of insurance cost for smokers\nsmoker_median = round(np.median(df_smokers.charges), 2)\n#print('The median insurance cost for being a smoker: ${median}'.format(median = smoker_median))\n\n#find standard deviation...\nsmoker_std = round(np.std(df_smokers.charges),2)\n#print('The standard deviation is: ${std}'.format(std = smoker_std))\n\n#let's see the variance\nsmoker_var = round(np.var(df_smokers.charges),2)\n#print('The variance is: {var}'.format(var = smoker_var))\n\n#maximum cost\nsmoker_max = round(np.max(df_smokers.charges), 2)\n#print('The maximum insurance cost for being a smoker is: ${max}'.format(max = smoker_max))\n\n#minimum cost\nsmoker_min = round(np.min(df_smokers.charges), 2)\n#print('The minimum insurance cost for being a smoker is: ${min}'.format(min = smoker_min))\n\nThe summary statistics provide valuable insights into the distribution of insurance costs for smokers. The mean insurance cost for smokers is calculated to be $32,050.23, representing the average expense in this category. The median insurance cost, at $34,456.35, serves as the central value around which the data is evenly distributed.\nAdditionally, the maximum observed insurance cost for smokers is $63,770.43, indicating the highest expense recorded within this group. Conversely, the minimum cost stands at $12,829.46, representing the lowest recorded expense.\nThe standard deviation of $11,520.47 measures the degree of variability or dispersion among the insurance costs for smokers. Furthermore, the variance of 132,721,153.14 quantifies the average squared deviation from the mean.\nThese statistical measures provide a comprehensive overview of the distribution of insurance costs for smokers, enabling a better understanding of the financial implications associated with smoking.\n\n\nSeparation of Male and Female Smokers:\nTo examine potential differences in insurance costs, a segmentation analysis was conducted, specifically focusing on male and female smokers. This approach aims to discern any notable variations in expenses between the two genders.\nThe analysis reveals significant insights, as demonstrated in the accompanying graphs. It becomes evident that the primary contributor to the observed bimodal distribution is the BMI (Body Mass Index). Notably, when the BMI reaches or exceeds 30, which falls within the obese category, a substantial increase in insurance costs occurs. This finding highlights the compounding effect of both obesity and smoking on medical insurance expenses.\nFurthermore, it is worth noting that insurance costs gradually increase as age advances. This correlation aligns with the expected progression of health issues that typically arise with aging. Consequently, there is a gradual rise in insurance costs as individuals age, reflecting the natural occurrence of age-related health conditions.\nThis segmentation analysis underscores the interplay between gender, BMI, age, and insurance costs, providing valuable insights for the medical industry in understanding the multifaceted factors that influence insurance expenses.\n\n\nAnalysis of Male Smokers:\nGraph One: Cost Distribution The distribution of insurance costs for male smokers exhibits a bimodal pattern, indicating the presence of two distinct clusters. This suggests the influence of an underlying variable that contributes to the observed cost disparity.\nGraph Two: Average Cost per Child The bar plot showcasing the average insurance costs per child does not reveal any clear trends. However, it is noteworthy that individuals with four children tend to have a slightly lower average insurance cost.\nGraph Three: Cost vs BMI with Region The scatterplot depicting insurance cost against BMI, considering different regions, demonstrates a visible linear relationship. The plot shows two clusters of individuals, but both clusters follow the same linear pattern. Notably, when the BMI reaches or exceeds 30 (indicating obesity), there is a significant and distinct increase in insurance costs for male smokers. It is important to note that BMI is not always an ideal measurement; however, in this analysis, it serves as an indicator of obesity. No notable trends were observed concerning the region.\nGraph Four: Cost vs Age with Children The scatterplot illustrating insurance cost against age, accounting for the presence of children, exhibits a linear trend split into two distinct clusters. This finding suggests that the observed difference is likely attributed to variations in BMI among individuals. As expected, insurance costs tend to increase gradually over time.\nGraph Five: Average Cost per Region The bar plot representing the average insurance costs per region does not yield any significant insights.\nGraph Six: Boxplot of Male Smoker Insurance Cost The boxplot provides valuable visual information regarding the spread of insurance costs among male smokers. It allows for a better understanding of the distribution and variability within this group.\nThese analyses shed light on the various factors impacting insurance costs for male smokers, including BMI, age, and the presence of children. By comprehensively examining these relationships, the medical industry can gain valuable insights to inform decision-making and enhance understanding of cost dynamics.\n\n#seperate all male smokers from the main data set\ndf_male_smoker = df_smokers[df_smokers.sex == 'male']\ndf_nonsmokers = df[df.smoker == 'no']\n#print(df_nonsmokers.head())\n#print(df_male_smoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male smoker\nsns.histplot(df_male_smoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Male Smoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of male smokers per child\ndf_children = df_male_smoker.groupby('children').mean().reset_index()\n#print(df_children.head())\n\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Male Smoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of male smokers by BMI\nsns.scatterplot(x = df_male_smoker.bmi, y = df_male_smoker.charges,\n                hue = df_male_smoker.region,\n                size = df_male_smoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Male Smoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of male smokers by age\nsns.scatterplot(x = df_male_smoker.age, y = df_male_smoker.charges,\n                hue = df_male_smoker.children,\n                size = df_male_smoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Male Smoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of male smokers per region\ndf_region = df_male_smoker.groupby('region').mean().reset_index()\n#print(df_region.head())\n\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Male Smoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of male smoker boxplot\nsns.boxplot(x = df_male_smoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Smoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\nAnalysis of Male Smokers: Highlighting BMI\nAs previously mentioned, the presence of two clusters in the cost distribution of male smokers can be attributed to variations in BMI. Notably, male smokers with a BMI greater than or equal to 30 experience a significant increase in expenses, approximately 47%, compared to their counterparts with a lower BMI. This finding underscores the impact of obesity on insurance costs for male smokers in the medical industry.\n\n#highlight BMI graph\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = df_male_smoker.bmi, y = df_male_smoker.charges,\n                hue = df_male_smoker.region,\n                size = df_male_smoker.region,\n                sizes = (7.5, 25))\nplt.axvline(x = 30, color = 'gray', label = 'Obese BMI')\nplt.title('Male Smoker Insurance Cost vs BMI')\nplt.xlabel('BMI')\nplt.ylabel('Insurance Costs (USD)')\n\nText(0, 0.5, 'Insurance Costs (USD)')\n\n\n\n\n\n\n\n\n\n\n#analyze BMI &lt; 30\n\nmale_df_bmi = df_male_smoker[df_male_smoker.bmi &lt; 30]\nmale_avg_cost = round(np.mean(male_df_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for male smokers with a BMI of less than 30 equates to: ${charge}\".format(charge = male_avg_cost))\n\n#analyze BMI &gt;= 30\n\nmale_df2_bmi = df_male_smoker[df_male_smoker.bmi &gt;= 30]\nmale_avg_cost_2 = round(np.mean(male_df2_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for male smokers with a BMI of greater than or equal to 30 equates to: ${charge}\".format(charge = male_avg_cost_2))\n\n#Difference of Insurance Cost\nmale_diff_cost = male_avg_cost_2 - male_avg_cost\n#print(\"The difference in insurance cost from being at or over 30 BMI compared to being under 30 BMI for males is: ${diff}\".format(diff = male_diff_cost))\n\nmale_diff_cost_percent = 100*round(male_diff_cost / male_avg_cost_2, 2)\n#print('A male smoker will suffer an increase of {percent}% in their insurance cost by being obese (BMI &gt;= 30)'.format(percent = male_diff_cost_percent))\n\n\n\nThe analysis of female smokers reveals the following insights:\nCost Distribution: Similar to the previous analysis, there is a bimodal distribution in the insurance cost for female smokers, indicating the presence of underlying factors contributing to the cost variations.\nAverage Cost per Child: The barplot does not show a clear trend between the number of children and the average insurance cost for female smokers. However, it is worth noting that having five children seems to be associated with a slightly lower average insurance cost.\nScatterplot of Cost vs BMI with Region: The scatterplot demonstrates a visible linear relationship between BMI and insurance cost for female smokers. There are two clusters of individuals, but they both follow the linear relationship. Notably, when the BMI is greater than or equal to 30 (indicating obesity), there is a significant increase in insurance cost for female smokers. It is important to consider that BMI may not be a perfect measurement of obesity (as discussed in the results). No specific trends are observed with respect to the region.\nScatterplot of Cost vs Age with Children: The scatterplot shows a linear trend in insurance cost based on age, but it is split into two linear clusters. This finding aligns with the earlier observation that differences in BMI among individuals may contribute to the clusters. As expected, insurance cost gradually increases over time.\nAverage Cost per Region: The barplot of average cost per region does not provide any valuable insights for female smokers.\nBoxplot of Female Smoker Insurance Cost: The boxplot displays the spread of insurance cost among female smokers, highlighting the range of variation in costs.\nOverall, the analysis of female smokers provides insights into the relationship between factors such as BMI, age, and insurance cost.\n\n#seperate all female smokers from the main data set\ndf_female_smoker = df_smokers[df_smokers.sex == 'female']\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male smoker\nsns.histplot(df_female_smoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Female Smoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of female smokers per child\ndf_children = df_female_smoker.groupby('children').mean().reset_index()\n#print(df_children.head())\n\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Female Smoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of female smokers by BMI\nsns.scatterplot(x = df_female_smoker.bmi, y = df_female_smoker.charges,\n                hue = df_female_smoker.region,\n                size = df_female_smoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Female Smoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of female smokers by age\nsns.scatterplot(x = df_female_smoker.age, y = df_female_smoker.charges,\n                hue = df_female_smoker.children,\n                size = df_female_smoker.children,\n                sizes = (12.5, 25), ax = ax[1,0])\nax[1,0].set_title('Female Smoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of female smokers per region\ndf_region = df_female_smoker.groupby('region').mean().reset_index()\n#print(df_region.head())\n\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average. Female Smoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of female smoker age vs bmi\nsns.boxplot(x = df_female_smoker.charges, ax = ax[1,2])\nax[1,2].set_title('Female Smoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nThe analysis of female smokers reveals findings that align with the observations made for male smokers. Specifically, the relationship between insurance cost and BMI for female smokers follows a similar pattern as observed in male smokers. Therefore, the conclusions drawn from the previous analysis regarding the impact of BMI on insurance cost can be applied to female smokers as well.\n\n#analyze BMI &lt; 30\n\nfemale_df_bmi = df_female_smoker[df_female_smoker.bmi &lt; 30]\nfemale_avg_cost = round(np.mean(female_df_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for female smokers with a BMI of less than 30 equates to: ${charge}\".format(charge = female_avg_cost))\n\n#analyze BMI &gt;= 30\n\nfemale_df2_bmi = df_female_smoker[df_female_smoker.bmi &gt;= 30]\nfemale_avg_cost_2 = round(np.mean(female_df2_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for female smokers with a BMI of greater than or equal to 30 equates to: ${charge}\".format(charge = female_avg_cost_2))\n\n#Difference of Insurance Cost\nfemale_diff_cost = female_avg_cost_2 - female_avg_cost\n#print(\"The difference in cost from being at or over 30 BMI compared to being under 30 BMI for females is: ${diff}\".format(diff = female_diff_cost))\n\n\n\nComparing Average Insurance Costs: Male vs. Female Smokers\nWhen comparing the average insurance costs of male and female smokers, some notable observations emerge. Among smokers with a BMI below 30, male smokers pay approximately 1.34% more than their female counterparts. However, for smokers with a BMI of 30 or higher, male smokers pay approximately 1.42% less than their female counterparts. These findings indicate a gender-based variation in insurance costs, influenced by both BMI and smoking status.\n\n#Plot average insurance cost for male and female \ndf_bmi_less_than_30 = df_smokers[df_smokers.bmi &lt; 30]\ndf_one = df_bmi_less_than_30.groupby('sex').mean().reset_index()\n\n#Get value\nmale_bmi_less_than_30 = df_one[df_one.sex == 'male'].charges.sum()\nfemale_bmi_less_than_30 = df_one[df_one.sex == 'female'].charges.sum()\n#print value\n\nmale_female_sum = male_bmi_less_than_30 + female_bmi_less_than_30\npor_male_less_30 = male_bmi_less_than_30 / male_female_sum\npor_female_less_30 = female_bmi_less_than_30 / male_female_sum\n\n\ndiff_por = round((por_male_less_30 - por_female_less_30), 4) * 100\n#print(\"Male smokers are expected to pay approximately {diff_por}% more than their female counterparts when their BMI &lt; 30 \".format(diff_por = diff_por))\n\ndf_bmi_greater_than_or_equal_30 = df_smokers[df_smokers.bmi &gt;= 30]\ndf_two = df_bmi_greater_than_or_equal_30.groupby('sex').mean().reset_index()\n\nmale_bmi_greq_30 = df_two[df_two.sex == 'male'].charges.sum()\nfemale_bmi_greq_30 = df_two[df_two.sex == 'female'].charges.sum()\n\n\nmale_female_sum = male_bmi_greq_30 + female_bmi_greq_30\npor_male_greq_30 = male_bmi_greq_30 / male_female_sum\npor_female_greq_30 = female_bmi_greq_30 / male_female_sum\n\ndiff_por_greq = round((por_male_greq_30 - por_female_greq_30), 4) * 100\ndiff_por_greq = abs(round(diff_por_greq, 4))\n#print(\"Male smokers are expected to pay approximately {diff_por_greq}% less than their female counterparts when their BMI &gt;= 30\".format(diff_por_greq = diff_por_greq))\n\n#print(df_one)\n#print(df_two)\n#df3 = pd.concat([df_one, df_two])\n#print(df3)\n\nfig, ax = plt.subplots(1,2,figsize=(8,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.8)\n\n#BMI is less than 30\nsns.set_palette('bright')\nsns.barplot(x = df_one.sex, y = df_one.charges, ax = ax[0])\nax[0].set_xlabel('Sex')\nax[0].set_ylabel('Insurance Cost (USD)')\nax[0].set_title('Average Smoker Insurance Cost with BMI &lt; 30')\n\n\n#BMI is greater than or equal to 30\nsns.set_palette('dark')\nsns.barplot(x = df_two.sex, y = df_two.charges, ax = ax[1])\nax[1].set_xlabel('Sex')\nax[1].set_ylabel('Insurance Cost (USD)')\nax[1].set_title('Average Smoker Insurance Cost with BMI &gt;= 30')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf"
  },
  {
    "objectID": "Content/Blog/US_HealthIns_Costs/index.html#approaching-the-analysis-of-nonsmokers",
    "href": "Content/Blog/US_HealthIns_Costs/index.html#approaching-the-analysis-of-nonsmokers",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Approaching the Analysis of Nonsmokers",
    "text": "Approaching the Analysis of Nonsmokers\nConsidered Factors: To understand the underlying causes of outliers or extreme values, several key factors warrant examination, including region, BMI, sex, children, and age. However, it is important to acknowledge that identifying the precise factors contributing to these outliers may present challenges and require further investigation.\n\nAnalysis of Insurance Costs for Nonsmokers: Boxplot and Histogram Distribution\nThe distribution of insurance costs for nonsmokers exhibits a right-skewed pattern, indicating a higher concentration of lower-cost cases. The interquartile range suggests a relatively narrow spread between the first and third quartiles.\n\nIQR = stats.iqr(df_nonsmokers.charges, interpolation = 'midpoint')\nIQR = round(IQR, 2)\nprint(IQR)\n\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,6))\n#boxplot\nsns.boxplot(y = df_nonsmokers.charges, ax = ax1)\nax1.set_ylabel('Insurance cost (USD)')\nax1.set_title('Insurance Cost Boxplot for Nonsmokers')\n\n#histogram\nsns.histplot(df_nonsmokers.charges, bins = 40, ax = ax2)\nax2.set_xlabel('Insurance Cost (USD)')\nax2.set_title('Insurance Cost Distribution for Nonsmokers')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n#right skewed distribution...explain potential causes:  Male and Female? BMI?\n\n7378.07\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\nAnalysis of Insurance Costs for Nonsmokers\nThe interquartile range of insurance costs for nonsmokers is $7,378.07, indicating a relatively narrow spread of data between the first and third quartiles.\n\n#find mean of insurance cost for nonsmokers\nnonsmoker_mean = round(np.mean(df_nonsmokers.charges), 2)\n#print('The nonsmoker average insurance cost for being a smoker: ${mean}'.format(mean = nonsmoker_mean))\n\n#find median of insurance cost for nonsmokers\nnonsmoker_median = round(np.median(df_nonsmokers.charges), 2)\n#print('The nonsmoker median insurance cost for being a smoker: ${median}'.format(median = nonsmoker_median))\n\n#find standard deviation...\nnonsmoker_std = round(np.std(df_nonsmokers.charges),2)\n#print('The nonsmoker standard deviation is: ${std}'.format(std = nonsmoker_std))\n\n#let's see the variance\nnonsmoker_var = round(np.var(df_nonsmokers.charges),2)\n#print('The nonsmoker variance is: {var}'.format(var = nonsmoker_var))\n\n#maximum cost\nnonsmoker_max = round(np.max(df_nonsmokers.charges), 2)\n#print('The nonsmoker maximum insurance cost is: ${max}'.format(max = nonsmoker_max))\n\n#minimum cost\nnonsmoker_min = round(np.min(df_nonsmokers.charges), 2)\n#print('The nonsmoker minimum insurance cost is: ${min}'.format(min = nonsmoker_min))\n\n#For explaination a high standard deviation tells us that here are other reasons the cost is so high since it varies signifcantly\n\n\n\nAnalyzing Male Nonsmokers: Key Findings\nGraph One: Cost Distribution The cost distribution for male nonsmokers exhibits a right-skewed pattern. It is important to consider the median as the most appropriate measure of central tendency, given the presence of skewness in the distribution.\nGraph Two: Average Cost per Child There is a notable trend indicating that having more children is associated with higher average insurance costs for male nonsmokers. However, an interesting exception is observed for individuals with five or more children, as their insurance costs show a significant decrease. The lowest average cost is observed for individuals with no children.\nGraph Three: Cost vs BMI with Region No correlation is found between the insurance cost and BMI of male nonsmokers. This finding highlights the absence of a relationship between these two variables. Similarly, no discernible trends are observed with respect to different regions.\nGraph Four: Cost vs Age with Children A strong linear relationship is observed between insurance cost and age for male nonsmokers. Additionally, the number of children shows a secondary trend, where fewer children correspond to lower insurance costs, while more children align with higher insurance costs. It is worth noting that some outliers deviate from the overall trend line.\nGraph Five: Average Cost per Region No significant insights can be drawn from the barplot of average cost per region for male nonsmokers. The data does not reveal any distinct variations among different regions in terms of insurance costs.\nGraph Six: Boxplot of Male Nonsmoker Insurance Cost The boxplot provides valuable information about the spread of insurance costs for male nonsmokers. It allows for visualizing the range, quartiles, and potential outliers within the data.\nOverall, these analyses shed light on various factors influencing insurance costs for male nonsmokers, such as the number of children and age. However, the relationship between BMI and insurance cost appears to be inconclusive, while regional differences do not show significant variations.\n\n#seperate all male nonsmokers from the main data set\ndf_male_nonsmoker = df_nonsmokers[df_nonsmokers.sex == 'male']\n\n#print(male_nonsmoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male nonsmoker\nsns.histplot(df_male_nonsmoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Male Nonsmoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of male nonsmokers per child\ndf_children = df_male_nonsmoker.groupby('children').mean().reset_index()\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Male Nonsmoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of male nonsmokers by BMI\nsns.scatterplot(x = df_male_nonsmoker.bmi, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.region,\n                size = df_male_nonsmoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Male Nonsmoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of male nonsmokers by age\nsns.scatterplot(x = df_male_nonsmoker.age, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.children,\n                size = df_male_nonsmoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Male Nonsmoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of male nonsmokers per region\ndf_region = df_male_nonsmoker.groupby('region').mean().reset_index()\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Male Nonsmoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of male nonsmoker boxplot\nsns.boxplot(x = df_male_nonsmoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Nonsmoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n#highlight scatter plot for linear trend\n\nsns.set(font_scale = 0.8)\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = df_male_nonsmoker.age, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.children,\n                size = df_male_nonsmoker.children,\n                sizes = (7.5, 20))\nplt.title('Male Nonsmoker Insurance Cost vs Age')\nplt.xlabel('Age')\nplt.ylabel('Insurance Costs (USD)')\nplt.legend(bbox_to_anchor = (1.0, 1))\n\nplt.show()\nplt.clf()\n#highlight this\n#solid evidence that as age increases, your insurance cost increases as well.\n#people with more children see a slightly higher cost overall \n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\nAnalyzing Female Nonsmokers: A Comparison with Male Nonsmokers\nGraph One: Cost Distribution Similar to male nonsmokers, the cost distribution for female nonsmokers exhibits a right-skewed pattern. This suggests that the median is a more appropriate measure of central tendency than the mean, considering the skewness in the distribution.\nGraph Two: Average Cost per Child Similarly to male nonsmokers, there is a discernible trend indicating that having more children is associated with higher average insurance costs for female nonsmokers. However, consistent with the findings among smokers, females with five or more children experience a decrease in average costs. Notably, the lowest average cost is observed among females with no children.\nGraph Three: Cost vs BMI with Region As with male nonsmokers, there is no correlation between the insurance cost and BMI among female nonsmokers. This lack of relationship remains consistent, and no significant trends are observed across different regions.\nGraph Four: Cost vs Age with Children In line with the observations for male nonsmokers, there is a strong linear relationship between insurance cost and age among female nonsmokers. A secondary trend is also evident, indicating that insurance costs tend to be lower for females with fewer children and higher for those with more children. It is worth noting that outliers do not conform to the overall trend line.\nGraph Five: Average Cost per Region There is a slight trend suggesting regional differences in insurance costs among female nonsmokers. On average, the northeast region exhibits higher costs compared to the southwest region.\nGraph Six: Boxplot of Male Smoker Insurance Cost While not directly related to the analysis of female nonsmokers, it is noteworthy to examine the spread of insurance costs among male smokers for comparative purposes\n\n#seperate all female nonsmokers from the main data set\ndf_female_nonsmoker = df_nonsmokers[df_nonsmokers.sex == 'female']\n\n#print(df_female_nonsmoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram Female nonsmoker\nsns.histplot(df_female_nonsmoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Female Nonsmoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of Female nonsmokers per child\ndf_children = df_female_nonsmoker.groupby('children').mean().reset_index()\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Female Nonsmoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of Female nonsmokers by BMI\nsns.scatterplot(x = df_female_nonsmoker.bmi, y = df_female_nonsmoker.charges,\n                hue = df_female_nonsmoker.region,\n                size = df_female_nonsmoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Female Nonsmoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\nax[0,2].legend(bbox_to_anchor = (1.0, 1))\n\n#scatterplot for insurance cost of Female nonsmokers by age\nsns.scatterplot(x = df_female_nonsmoker.age, y = df_female_nonsmoker.charges,\n                hue = df_female_nonsmoker.children,\n                size = df_female_nonsmoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Female Nonsmoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of Female nonsmokers per region\ndf_region = df_female_nonsmoker.groupby('region').mean().reset_index()\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Female Nonsmoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of Female nonsmoker boxplot\nsns.boxplot(x = df_male_nonsmoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Nonsmoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "Content/Blog/US_HealthIns_Costs/index.html#nonsmokers-vs-smokers",
    "href": "Content/Blog/US_HealthIns_Costs/index.html#nonsmokers-vs-smokers",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Nonsmokers vs Smokers",
    "text": "Nonsmokers vs Smokers\nBased on the analysis conducted on smokers and nonsmokers, it can be observed that smoking status significantly impacts health insurance costs. Both male and female smokers experience higher insurance costs compared to their nonsmoking counterparts. Furthermore, nonsmokers tend to have a more favorable cost distribution, characterized by lower median costs and less variability. This highlights the importance of smoking cessation interventions and promoting a nonsmoking lifestyle to mitigate the financial burden associated with health insurance for both individuals and the healthcare system.\n\n#Nonsmokers\ndf_avg = df_nonsmokers.groupby('sex').mean().reset_index()\n\n\nnonsmoker_male =  round((df_avg[df_avg.sex == 'male'].charges.sum()),2)\nnonsmoker_female = round((df_avg[df_avg.sex == 'female'].charges.sum()),2)\ndiff = round((nonsmoker_female - nonsmoker_male), 2)\ntotal = nonsmoker_male + nonsmoker_female\nnonsmoker_percent_diff = round((((nonsmoker_female - nonsmoker_male) / total) * 100),2) \n\n\nprint(\"The average insurance cost for a nonsmoker male is ${avg}.\".format(avg = nonsmoker_male))\nprint(\"The average insurance cost for a nonsmoker female is ${avg}.\".format(avg = nonsmoker_female))\nprint(\"The average difference in insurance cost for nonsmoker male and female is ${diff}.\\n\".format(diff = diff))\nprint(\"The nonsmoker males are charged {percent}% less than their female counterpart.\\n\".format(percent = nonsmoker_percent_diff))\n\n#Smokers\ndf_avg2 = df_smokers.groupby('sex').mean().reset_index()\n\n\nsmoker_male =  round((df_avg2[df_avg2.sex == 'male'].charges.sum()),2)\nsmoker_female = round((df_avg2[df_avg2.sex == 'female'].charges.sum()),2)\ndiff2 = round((smoker_male - smoker_female), 2)\ntotal = smoker_male + smoker_female\nsmoker_percent_diff = round((((smoker_male - smoker_female) / total) * 100),2) \n\n\nprint(\"The average insurance cost for a smoker male is ${avg}.\".format(avg = smoker_male))\nprint(\"The average insurance cost for a smoker female is ${avg}.\".format(avg = smoker_female))\nprint(\"The average difference in insurance cost for smoker male and female is ${diff}.\\n\".format(diff = diff2))\nprint(\"The smoker males are charged {percent}% more than their female counterpart.\\n\".format(percent = smoker_percent_diff))\n\n#Comparision\n\nins_cost_diff_male = smoker_male - nonsmoker_male\nins_cost_diff_female = smoker_female - nonsmoker_female\nins_cost_avg_diff = diff2 - diff\n\n\nins_cost_percent_male = round((smoker_male / nonsmoker_male) * 100, 2)\nins_cost_percent_female = round((smoker_female / nonsmoker_female) * 100, 2)\n\n\n\nprint(\"On average, a male nonsmoker pays ${nonsmoker}, while a female smoker pays ${smoker}.\".format(nonsmoker = nonsmoker_male, smoker = smoker_male))\nprint(\"The difference in their insurance charges is ${diff_male}.\".format(diff_male = ins_cost_diff_male))\nprint(\"A male smoker will get charged approxmiately {percent}% more than a nonsmoker.\\n\".format(percent = ins_cost_percent_male))\n\nprint(\"On average, a female nonsmoker pays ${nonsmoker}, while a female smoker pays ${smoker}.\".format(nonsmoker = nonsmoker_female, smoker = smoker_female))\nprint(\"The difference in their insurance charges is ${diff_female}.\".format(diff_female = ins_cost_diff_female))\nprint(\"A female smoker will get charged approxmiately {percent}% more than a nonsmoker.\".format(percent = ins_cost_percent_female))\n\nThe average insurance cost for a nonsmoker male is $8087.2.\nThe average insurance cost for a nonsmoker female is $8762.3.\nThe average difference in insurance cost for nonsmoker male and female is $675.1.\n\nThe nonsmoker males are charged 4.01% less than their female counterpart.\n\nThe average insurance cost for a smoker male is $33042.01.\nThe average insurance cost for a smoker female is $30679.0.\nThe average difference in insurance cost for smoker male and female is $2363.01.\n\nThe smoker males are charged 3.71% more than their female counterpart.\n\nOn average, a male nonsmoker pays $8087.2, while a female smoker pays $33042.01.\nThe difference in their insurance charges is $24954.81.\nA male smoker will get charged approxmiately 408.57% more than a nonsmoker.\n\nOn average, a female nonsmoker pays $8762.3, while a female smoker pays $30679.0.\nThe difference in their insurance charges is $21916.7.\nA female smoker will get charged approxmiately 350.12% more than a nonsmoker."
  },
  {
    "objectID": "Content/Blog/US_HealthIns_Costs/index.html#results",
    "href": "Content/Blog/US_HealthIns_Costs/index.html#results",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Results",
    "text": "Results\nThe impact of smoking on health insurance premiums is undeniable, as demonstrated by the substantial increases in charges for both male and female smokers. When coupled with a BMI of 30 or higher, smokers face an additional financial burden. Male smokers experience a significant 52% increase in insurance charges, while female smokers face a 49% rise. It is important to note that these increases are in addition to the base premium already charged to smokers.\nThe severity of the situation becomes apparent when comparing the costs borne by smokers and nonsmokers. Male smokers pay a staggering 408.57% more than their nonsmoking counterparts, while female smokers face an astronomical 350.12% increase. This data reinforces the notion that unhealthy lifestyle choices, such as smoking and an unhealthy BMI, result in punitive measures in the form of higher health insurance premiums. It is worth mentioning that insurance premiums gradually increase over time in line with the aging process.\nWhile this project provides valuable insights, there are still numerous avenues for further exploration. One potential direction involves applying machine learning techniques to assess the representativeness of the sample and refine the accuracy of the conclusions. Such analyses have the potential to drive advancements in the field of health insurance."
  },
  {
    "objectID": "Content/Blog/US_HealthIns_Costs/index.html#further-study",
    "href": "Content/Blog/US_HealthIns_Costs/index.html#further-study",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Further Study",
    "text": "Further Study\nAccording to a reputable source (Medical News Today), BMI, which relies on height and weight measurements, is an inadequate indicator of body fat content. It fails to consider important factors such as muscle mass, bone density, overall body composition, and variations based on race and sex.\nFurthermore, health insurance companies employ a practice known as tobacco rating, as outlined in the Affordable Care Act (ACA). Smokers can be charged a tobacco surcharge of up to 50% (or premiums 1.5 times higher) compared to non-smokers. This allows insurance providers to adjust prices based on tobacco usage and the associated health risks."
  },
  {
    "objectID": "Content/Blog/US_HealthIns_Costs/index.html#references",
    "href": "Content/Blog/US_HealthIns_Costs/index.html#references",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "References:",
    "text": "References:\nMedical News Today: “BMI (body mass index): What is it and is it useful?”\nHealthMarkets: “Smoking and Health Insurance”"
  },
  {
    "objectID": "Content/Blog/Param_Report_Retail/index.html#objective",
    "href": "Content/Blog/Param_Report_Retail/index.html#objective",
    "title": "Parameterized Reports",
    "section": "Objective",
    "text": "Objective\nThis project was aimed at streamlining the report generation process by employing parameterized R Markdown. The core idea was to automate the update of report parameters, thus eliminating the manual task of adjusting values for daily or periodic reports. This innovation is designed to significantly reduce time and effort for data scientists, analysts, and engineers, facilitating the production of consistent and timely reports. The automation feature introduced through parameterized R Markdown enhances both productivity and accuracy, offering considerable benefits to business operations."
  },
  {
    "objectID": "Content/Blog/Param_Report_Retail/index.html#approach",
    "href": "Content/Blog/Param_Report_Retail/index.html#approach",
    "title": "Parameterized Reports",
    "section": "Approach",
    "text": "Approach\nThe process involved the following key steps:\n\nLibrary Utilization:\n\nEssential R libraries were loaded, including tidyverse for data manipulation, plotly and ggplotly for interactive visualizations, and rmarkdown for rendering parameterized reports.\n\nData Preparation:\n\nThe dataset was loaded and processed. This included renaming columns, converting dates, and filtering data based on the year specified through parameters.\n\nCustom Theme Development:\n\nA custom theme function was created to standardize the appearance of plots across all reports, ensuring a consistent and professional aesthetic.\n\nData Visualization:\n\nInteractive visualizations were crafted using ggplotly to display monthly sales data, highlighting key trends and insights.\n\nAutomated Report Generation:\n\nA loop was implemented to automate the generation of reports for each year, utilizing a function that leverages parameterized R Markdown to render individual reports dynamically.\n\n\n\nHighlights\n\nThe project underscores the efficiency and scalability of automated report generation.\nThe approach allows for the seamless creation of visually appealing and informative reports.\nAutomation minimizes errors and saves considerable time, enhancing decision-making processes with timely and accurate data insights.\n\n\n\nTechnical Notes\n\nThe script included the loading of required libraries, data wrangling steps, and the use of ggplotly for creating dynamic visualizations.\nThe custom theme function (myTheme) ensures a uniform look across all visualizations.\nThe renderReport function and subsequent loop for rendering reports underscore the automation aspect, showcasing the project’s capacity to produce multiple reports efficiently.\n\n\n\nConclusion\nThis project exemplifies the power of automation in report generation using parameterized R Markdown. It offers a scalable solution for producing detailed and aesthetically consistent reports, providing valuable time savings and accuracy for businesses and their data teams."
  },
  {
    "objectID": "Content/Blog/Param_Report_Retail/index.html#required-libraries",
    "href": "Content/Blog/Param_Report_Retail/index.html#required-libraries",
    "title": "Parameterized Reports",
    "section": "Required Libraries",
    "text": "Required Libraries\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)\nlibrary(rmarkdown)\nlibrary(purrr)"
  },
  {
    "objectID": "Content/Blog/Param_Report_Retail/index.html#load-dataset-wrangle",
    "href": "Content/Blog/Param_Report_Retail/index.html#load-dataset-wrangle",
    "title": "Parameterized Reports",
    "section": "Load Dataset & Wrangle",
    "text": "Load Dataset & Wrangle\n\nds &lt;- read_csv(\"../../../Assets/Datasets/retail.csv\")\n\n#head(ds)\n\nds &lt;- ds %&gt;% \n  rename(ID = ...1) %&gt;%\n  mutate(Month = lubridate::floor_date(Date, 'month')) %&gt;%\n  filter(year(Month) == params$year)\n\nglimpse(ds)\n\nRows: 10,042\nColumns: 9\n$ ID         &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ DocumentID &lt;dbl&gt; 716, 716, 716, 716, 716, 716, 716, 460, 461, 462, 463, 464,…\n$ Date       &lt;date&gt; 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23…\n$ SKU        &lt;dbl&gt; 1039, 853, 862, 868, 2313, 2355, 2529, 2361, 2723, 655, 254…\n$ Price      &lt;dbl&gt; 381.78, 593.22, 423.73, 201.70, 345.76, 406.78, 542.38, 139…\n$ Discount   &lt;dbl&gt; 67.37254, 0.00034, -0.00119, 35.58814, 61.01966, 101.69458,…\n$ Customer   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 460, 479, 26, 580, 311, 311, 311, 311,…\n$ Quantity   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 4,…\n$ Month      &lt;date&gt; 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01…"
  },
  {
    "objectID": "Content/Blog/Param_Report_Retail/index.html#visualize-the-report",
    "href": "Content/Blog/Param_Report_Retail/index.html#visualize-the-report",
    "title": "Parameterized Reports",
    "section": "Visualize The Report",
    "text": "Visualize The Report\nI utilized ggplotly, a graphical representation tool, to create an interactive visualization of monthly sales time series data for “CRM and Invoicing system,” which is a wholesale company owned by Sadi Evren. The data for this analysis was obtained from the following Kaggle dataset: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv.\nThe resulting plot provided an insightful representation of the monthly sales data, showcasing trends and patterns in the data that could potentially provide useful information for decision making in the business.\nIn addition to the initial plot, I implemented a for loop to automatically generate multiple reports based on the time series data for each year. This approach eliminated the need for manual report generation, thereby saving time and reducing the risk of errors. The loop enabled the automated generation of separate reports for each year, which provided a comprehensive view of the sales trends over time.\nOverall, the use of ggplotly for data visualization and automation of report generation using a for loop demonstrated an effective approach for efficiently analyzing and presenting data.\n\np &lt;- ds %&gt;%\n  group_by(Month) %&gt;%\n  summarize(AvgSales = round(mean(Price * Quantity),2) ) %&gt;%\n  ggplot(aes(x = Month, \n             y = AvgSales,\n             group = 1,                 #Necessary or else line plot disappears\n             text = paste0(\"Monthly Sales: $\", (round(AvgSales/1000,2)),\"K\" ))) +\n  geom_line(size = 1) + \n  scale_y_continuous(labels = scales::dollar_format(scale = .001, suffix = \"K\")) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%B\") + \n  labs(title = paste0(\"CRM and Invoicing System Sales For FY: \", params$year),\n       caption = \"Source: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv\",\n       x = NULL,\n       y = NULL) +\n  myTheme()\n\nggplotly(p, tooltip = c(\"text\")) %&gt;% \n  layout(hovermode = \"x unified\")"
  },
  {
    "objectID": "Content/Blog/Param_Report_Retail/index.html#function-to-run-parameterized-reports",
    "href": "Content/Blog/Param_Report_Retail/index.html#function-to-run-parameterized-reports",
    "title": "Parameterized Reports",
    "section": "Function To Run Parameterized Reports",
    "text": "Function To Run Parameterized Reports\n\nrenderReport &lt;- function(year) {\n  quarto::quarto_render(\n    input = \"index.qmd\",\n    output_file = paste0(year, '.html'),\n    execute_params = list(year = year)\n  )\n}"
  },
  {
    "objectID": "Content/Blog/Param_Report_Retail/index.html#render-all-reports",
    "href": "Content/Blog/Param_Report_Retail/index.html#render-all-reports",
    "title": "Parameterized Reports",
    "section": "Render All Reports",
    "text": "Render All Reports\n\n# Renders all 4 Reports (dates range from 2019-2022)\nfor (year in 2019:2022) {\n    renderReport(year)\n}"
  },
  {
    "objectID": "Content/Blog/index.html",
    "href": "Content/Blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Interactive Learning with WebR\n\n\nLab Demonstration\n\n\n\n\n\n\n\n\nAugust 12, 2024\n\n\nBrian Cervantes Alvarez\n\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\n\nFraud Detection: Enhancing Accuracy with GBM\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\nGradient Boosting Machines\n\n\n\nI developed a model that enhances fraud detection in this dataset, contributing to ongoing advancements in this challenging task\n\n\n\n\n\nMay 30, 2023\n\n\nBrian Cervantes Alvarez\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding A Quick Dashboard For Amazon Products (EN.)\n\n\n\n\n\n\nR\n\n\nShiny\n\n\nCSS\n\n\nData Tables\n\n\nData Visualization\n\n\nDashboard\n\n\n\nExplore Amazon products in India with our Shiny app. Discover categories with the highest ratings and reviews. Efficient, informative, and visually…\n\n\n\n\n\nApril 21, 2023\n\n\nBrian Cervantes Alvarez\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports\n\n\n\n\n\n\nR\n\n\nPlotly\n\n\nReports\n\n\nTime Series\n\n\nData Visualization\n\n\n\nAchieving peak productivity in data analysis with automated R Markdown reports, minimizing effort and errors.\n\n\n\n\n\nApril 3, 2023\n\n\nBrian Cervantes Alvarez\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nApple’s Journey In The Stock Market\n\n\n\n\n\n\nR\n\n\nTime Series\n\n\nData Visualization\n\n\n\nExplore the dynamic world of Time Series Graphs with R’s plotly. Unveil trends in Apple’s stock market and the impact of groundbreaking innovations\n\n\n\n\n\nMarch 29, 2023\n\n\nBrian Cervantes Alvarez\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nDo You Like Stretching? I Would Reconsider!\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nPlotly\n\n\n\nStretching enhances flexibility but lacks muscle-building benefits. Weight training and resistance exercises stimulate muscle growth. Supplement…\n\n\n\n\n\nFebruary 24, 2023\n\n\nBrian Cervantes Alvarez\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nRating Pinot Wines: Is More Expensive Better?\n\n\n\n\n\n\nR\n\n\nData Tables\n\n\n\nRevolutionizing Data Tables: Visual Appeal & Comprehension. Explore techniques for aesthetically pleasing & informative tables. Enhance data…\n\n\n\n\n\nFebruary 20, 2023\n\n\nBrian Cervantes Alvarez\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nPokédex Database\n\n\n\n\n\n\nR\n\n\nPostgreSQL\n\n\nData Visualization\n\n\nData Engineering\n\n\n\nUnveil the power of PostgreSQL! Explore the intricate ETL process, advanced tools, and meticulous schema design that created a functional database.…\n\n\n\n\n\nDecember 5, 2022\n\n\nBrian Cervantes Alvarez\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nCollege Debt Shiny App\n\n\n\n\n\n\nR\n\n\nShiny\n\n\nData Visualization\n\n\nDashboard\n\n\n\nCollaborating with Corey Cassell, we’ve developed an interactive tool aiding students in career and financial planning, empowering informed…\n\n\n\n\n\nDecember 2, 2022\n\n\nBrian Cervantes Alvarez, Corey Cassell\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs\n\n\n\n\n\n\nPython\n\n\nStatistics\n\n\nJupyter Notebook\n\n\nData Visualization\n\n\n\nIn this captivating investigation, we dive deep into the impact of smoking habits on medical insurance expenses, carefully examining the nuanced…\n\n\n\n\n\nJuly 14, 2022\n\n\nBrian Cervantes Alvarez\n\n\n25 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#abstract",
    "href": "Content/Blog/AmazonProductsEN/index.html#abstract",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Abstract",
    "text": "Abstract\nThis project focuses on the development of a Shiny application to analyze Amazon products in India. Using a dataset sourced from Kaggle, the application aims to create an informative dashboard showcasing categories with the highest average ratings and reviews. Despite encountering challenges with product categorization, the project prioritizes efficiency and design improvements."
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#introduction",
    "href": "Content/Blog/AmazonProductsEN/index.html#introduction",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Introduction",
    "text": "Introduction\nThe objective of this project is to create a Shiny application that explores the Amazon products dataset from India. The dataset, obtained from Kaggle, presented challenges during development, particularly regarding product categories. However, the project’s primary focus was on creating a concise and informative dashboard displaying categories with the highest average ratings and reviews.\nCompared to previous Shiny app development experiences, this project demonstrated improved efficiency, taking significantly less time to complete. Furthermore, advanced CSS theming was implemented to enhance the overall design of the application. While additional features and visualizations could be incorporated, the decision was made to leave them for future exploration."
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#questions",
    "href": "Content/Blog/AmazonProductsEN/index.html#questions",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average rating for each category and subcategory?\nWhat is the average review for each category and subcategory?\n\n\n\nFullscreen"
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#load-libraries",
    "href": "Content/Blog/AmazonProductsEN/index.html#load-libraries",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\nlibrary(plotly)\nlibrary(DT)\noptions(scipen = 999)"
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#part-1-data-wrangling-multiple-datasets",
    "href": "Content/Blog/AmazonProductsEN/index.html#part-1-data-wrangling-multiple-datasets",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 1: Data Wrangling Multiple Datasets",
    "text": "Part 1: Data Wrangling Multiple Datasets\n\n# ORIGINAL DATA WRANGLING \n\ndatasets &lt;- as.data.frame(list.files(path = \"~/Documents/ShinyApps/AmazonProducts/AmazonProductApp\", pattern = \"csv\"))\ncolnames(datasets) &lt;- \"Datasets\"\n\n# Combine all the datasets\nfor (i in length(nrow(datasets))){\n combinedDs &lt;- read_csv(datasets[[i]])\n}\n\namazonProducts &lt;- combinedDs %&gt;%\n mutate(Name = name,\n       MainCategory = factor(str_to_title((sort(main_category)))),\n       SubCategory = factor(sort(sub_category)),\n       ProductImage = image,\n       ProductRating = as.numeric(ratings),\n       NumberOfRatings = as.numeric(gsub(\"\\\\,\",\"\",no_of_ratings)),\n       DiscountPrice = round(as.numeric(gsub(\"[\\\\₹,]\", \"\", discount_price)) / 81.85, 2), # convert from Rupee to USD\n       Price = round(as.numeric(gsub(\"[\\\\₹,]\", \"\", actual_price)) / 81.85, 2),           # convert from Rupee to USD\n       ProductLink = link) %&gt;%\n select(-c(name, \n           main_category, \n           sub_category, \n           image, \n           ratings, \n           no_of_ratings, \n           discount_price, \n           actual_price,\n           link)) %&gt;% \n drop_na() %&gt;%\n filter(!str_detect(SubCategory, \"^All \"))\n\n# amazonProducts %&gt;%\n# write_csv(\"amazonProducts.csv\")\n\n\nReload Data\n\nproducts &lt;- read_csv(\"../../../Assets/Datasets/amazonProducts.csv\")"
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#part-2-data-wrangling-for-visualization",
    "href": "Content/Blog/AmazonProductsEN/index.html#part-2-data-wrangling-for-visualization",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 2: Data Wrangling For Visualization",
    "text": "Part 2: Data Wrangling For Visualization\n\nproducts &lt;- products %&gt;% mutate(DiscountPrice = round(DiscountPrice82.04,2), Price = round(Price82.04,2))\n\n#For Plot 1\nratings &lt;- products %&gt;% select(-c(ProductImage, Name, ProductLink)) %&gt;% group_by(MainCategory,SubCategory) %&gt;% summarise(AverageRating = mean(ProductRating)) %&gt;% ungroup()\n\nreviews &lt;- products %&gt;% select(-c(ProductImage, Name, ProductLink)) %&gt;% group_by(MainCategory,SubCategory) %&gt;% summarise(AverageReview = mean(NumberOfRatings)) %&gt;% ungroup()\n\n#For Plot 2 \ntop10Products &lt;- products %&gt;% filter(ProductRating &gt; 4.5, NumberOfRatings &gt; 50) %&gt;% group_by(SubCategory) %&gt;% arrange(desc(ProductRating)) %&gt;% slice(1:10) %&gt;% select(-c(ProductImage, Name, ProductLink))\n\n#unique(products$MainCategory)\n\n\nColor Theming\n\n#Plot 1 \nnum_colors &lt;- 21 \ncolors &lt;- c(\"#f2f2f2\", \"#ff9900\") \npal1 &lt;- colorRampPalette(colors)(num_colors)\n\nprint(pal1)\n\n#Plot 2 \nnum_colors &lt;- 21 \ncolors &lt;- colors &lt;- c(\"#f2f2f2\",\"#00a8e1\") \npal2 &lt;- colorRampPalette(colors)(num_colors)"
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#part-1-shiny-ui",
    "href": "Content/Blog/AmazonProductsEN/index.html#part-1-shiny-ui",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 1: Shiny UI",
    "text": "Part 1: Shiny UI\n\n# Define UI for application \nui &lt;- fluidPage(\n  \n  #Background CSS\n  tags$head(tags$style(HTML('\n  @import url(https://fonts.googleapis.com/css?family=Montserrat&display=swap);\n    body {\n      font-family: Montserrat, sans-serif;\n      background-color: #FF9900;\n    }\n    .dataTables_wrapper {\n      background-color: #fff;\n    }\n    .sidebar {\n      background-color: #fff;\n      width: 3/12;\n      height: 2/12;\n    }\n    .nav-tabs &gt; li &gt; a {\n      color: black;\n      background-color: #00a8e1;\n      border-color: #00a8e1;\n    }'))),\n  \n  # Application title\n  titlePanel(\"Amazon Inc. Product Dashboard (EN.)\"),\n  \n  # Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"MainCategoryChoice\", \n                  label = h3(\"Select Category:\"), \n                  choices = unique(products$MainCategory), \n                  selected = \"Accessories\"),\n      \n      uiOutput(\"SubCategoryChoice\")\n    ),\n    \n    # Tabs\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Plot\", \n                 plotlyOutput(\"RatingsPlot\"),\n                 plotlyOutput(\"ReviewsPlot\")),\n        tabPanel(\"Data\", dataTableOutput(\"myDataTable\"))\n      )\n    )\n  )\n)"
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#part-2-shiny-server",
    "href": "Content/Blog/AmazonProductsEN/index.html#part-2-shiny-server",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 2: Shiny Server",
    "text": "Part 2: Shiny Server\n\n# Define server logic required to draw Dashboard\nserver &lt;- function(input, output) {\n  \n  # Subcategory choices\n  output$SubCategoryChoice &lt;- renderUI({\n    subcategories &lt;- unique(products$SubCategory[products$MainCategory == input$MainCategoryChoice])\n    checkboxGroupInput(\"SubCategoryChoice\", \n                       label = h3(\"Select Subcategories:\"), \n                       choices = subcategories, \n                       selected = subcategories)\n  })\n  \n  # Plot 1: Product Rating\n  output$RatingsPlot &lt;- renderPlotly({\n    ratings %&gt;%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %&gt;% \n      mutate(SubCategory_ordered = factor(SubCategory, \n                                          levels = unique(SubCategory[order(AverageRating)]))) %&gt;%\n      plot_ly(x = ~SubCategory_ordered, \n              y = ~round(AverageRating,2),\n              type = 'bar',\n              marker = list(color = ~pal1[SubCategory_ordered])) %&gt;%\n      add_annotations(x = ~SubCategory_ordered,\n                      y = ~round(AverageRating,2),\n                      text = ~paste0(round(AverageRating,2)),\n                      font = list(color = 'black', \n                                  size = 10),\n                      showarrow = FALSE,\n                      yshift = 5) %&gt;%\n      layout(title = paste0(\"Average Product Rating For \",input$MainCategoryChoice),\n             xaxis = list(title = \"\", tickangle = 45),\n             yaxis = list(title = \"\"),\n             showlegend = FALSE,\n             margin = list(t = 50,\n                           l = 50,\n                           r = 50,\n                           b = 50)) \n  })\n  \n  # Plot 2: Product Reviews\n  output$ReviewsPlot &lt;- renderPlotly({\n    reviews %&gt;%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %&gt;% \n      mutate(SubCategory_ordered = factor(SubCategory, \n                                          levels = unique(SubCategory[order(AverageReview)]))) %&gt;%\n      plot_ly(x = ~SubCategory_ordered, \n              y = ~round(AverageReview),\n              type = 'bar',\n              marker = list(color = ~pal2[SubCategory_ordered])) %&gt;%\n      add_annotations(x = ~SubCategory_ordered,\n                      y = ~round(AverageReview),\n                      text = ~paste0(round(AverageReview)),\n                      font = list(color = 'black', \n                                  size = 10),\n                      showarrow = FALSE,\n                      yshift = 5) %&gt;%\n      layout(title = paste0(\"Average Number of Reviews For \",input$MainCategoryChoice),\n             xaxis = list(title = \"\", tickangle = 45),\n             yaxis = list(title = \"\"),\n             showlegend = FALSE,\n             margin = list(t = 50,\n                           l = 50,\n                           r =50,\n                           b = 50)) \n  })\n  \n  output$myDataTable &lt;- DT::renderDataTable({\n    products %&gt;%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %&gt;%\n      mutate(ProductImage = sprintf('&lt;img src=\"%s\" width=\"75px\"/&gt;', ProductImage)) %&gt;%\n      DT::datatable(., escape = FALSE, options = list(\n        pageLength = 10,\n        lengthMenu = c(5, 10, 25),\n        scrollY = \"600px\",\n        scrollX = TRUE\n      )) %&gt;%\n      DT::formatStyle(columns = colnames(products), \n                      backgroundColor = styleEqual(c(\"green\", \"white\"), c(\"rgb(51, 102, 0)\", \"rgb(255, 255, 255)\")))\n  })\n  \n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#further-improvement-questions",
    "href": "Content/Blog/AmazonProductsEN/index.html#further-improvement-questions",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Further Improvement Questions",
    "text": "Further Improvement Questions\n\nIs there a correlation between the number of ratings and the product rating?\nWhat is the average discount percentage for each main category and subcategory?\nWhat is the price range for each main category and subcategory?\nWhich products have the highest ratings and how do they compare in terms of price and number of ratings?"
  },
  {
    "objectID": "Content/Blog/AmazonProductsEN/index.html#conclusion",
    "href": "Content/Blog/AmazonProductsEN/index.html#conclusion",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Conclusion",
    "text": "Conclusion\nThis project served as a rewarding experience, providing opportunities to enhance data analysis and Shiny app development skills. The Shiny application developed enables users to analyze Amazon products in India and offers insights into categories with the highest average ratings and reviews. Despite encountering challenges related to product categorization, the project prioritized efficiency and introduced improvements in design. Future exploration of additional features and visualizations remains open for further development and enhancement."
  },
  {
    "objectID": "Content/Blog/AppleInc_IncomeStatement/index.html#abstract",
    "href": "Content/Blog/AppleInc_IncomeStatement/index.html#abstract",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Abstract",
    "text": "Abstract\nThis project aims to utilize R’s plotly package to create dynamic and interactive visualizations of Time Series Graphs. The focus of the analysis is on the Apple stock prices from 1981 to the present, with a specific emphasis on identifying notable trends that have emerged over time. The study reveals a significant spike in the company’s stock prices following 2010, attributed to the success of Apple’s iPhone and related products. The research also highlights the impact of Apple’s marketing strategy in maintaining its market value and relevance to changing consumer needs. Overall, the project demonstrates the insightful use of R’s plotly for visualizing Time Series Graphs and providing meaningful explanations of stock market trends."
  },
  {
    "objectID": "Content/Blog/AppleInc_IncomeStatement/index.html#introduction",
    "href": "Content/Blog/AppleInc_IncomeStatement/index.html#introduction",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Introduction",
    "text": "Introduction\nVisualizing Time Series Graphs using R’s plotly package provides a powerful means to analyze and interpret complex data. In this project, the focus is on exploring the Apple stock prices over several decades and identifying key trends that have shaped the company’s market performance. The analysis uncovers a significant upsurge in Apple’s stock prices after 2010, which can be attributed to the tremendous success of the iPhone and Apple’s ability to stay in tune with evolving consumer demands.\nFurthermore, the research recognizes the role of Apple’s marketing strategy in maintaining the company’s market value and driving its growth. Apple’s innovative technology and ability to revolutionize the tech industry have contributed significantly to its economic impact. By employing R’s plotly package, this project aims to provide a more meaningful and interactive representation of the Time Series Graphs, enabling a comprehensive understanding of the trends in the stock market."
  },
  {
    "objectID": "Content/Blog/AppleInc_IncomeStatement/index.html#setup",
    "href": "Content/Blog/AppleInc_IncomeStatement/index.html#setup",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\n\n\nds &lt;- read_csv(\"../../../Assets/Datasets/AppleInc_Stocks.csv\")\n#head(ds)"
  },
  {
    "objectID": "Content/Blog/AppleInc_IncomeStatement/index.html#apply-custom-theme",
    "href": "Content/Blog/AppleInc_IncomeStatement/index.html#apply-custom-theme",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Apply Custom Theme",
    "text": "Apply Custom Theme\n\nmyTheme &lt;- function(){ \n    font &lt;- \"SF Mono\"   #assign font family up front\n    \n    theme_minimal() %+replace%    #replace elements we want to change\n    \n    theme(\n      \n      #grid elements\n      panel.grid.major.x = element_blank(),    #strip major gridlines\n      panel.grid.minor = element_blank(),    #strip minor gridlines\n      axis.ticks = element_blank(),          #strip axis ticks\n      \n      #since theme_minimal() already strips axis lines, \n      #we don't need to do that again\n      \n      #text elements\n      plot.title = element_text(             #title\n                   family = font,            #set font family\n                   size = 16,                #set font size\n                   face = 'bold',            #bold typeface\n                   hjust = 0,                #left align\n                   vjust = 2),               #raise slightly\n      \n      plot.subtitle = element_text(          #subtitle\n                   family = font,            #font family\n                   size = 12),               #font size\n      \n      plot.caption = element_text(           #caption\n                   family = font,            #font family\n                   size = 9,                 #font size\n                   hjust = 1),               #right align\n      \n      axis.title = element_text(             #axis titles\n                   family = font,            #font family\n                   size = 10),               #font size\n      \n      axis.text = element_text(              #axis text\n                   family = font,            #axis famuly\n                   size = 9),                #font size\n      \n      axis.text.x = element_text(            #margin for axis text\n                    margin=margin(5, b = 10))\n      \n      #since the legend often requires manual tweaking \n      #based on plot content, don't define it here\n    )\n}"
  },
  {
    "objectID": "Content/Blog/AppleInc_IncomeStatement/index.html#data-wrangling",
    "href": "Content/Blog/AppleInc_IncomeStatement/index.html#data-wrangling",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nyearlyDs &lt;- ds %&gt;% \n  drop_na() %&gt;%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %&gt;%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %&gt;%\n  summarize(Open = mean(Open),\n            High = mean(High),\n            Low = mean(Low),\n            Close = mean(Close),\n            AdjClose = mean(`Adj Close`),\n            Volume = mean(Volume))\n\n\nlog_yearlyDs &lt;- ds %&gt;% \n  drop_na() %&gt;%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %&gt;%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %&gt;%\n  summarize(Open = log(mean(Open)),\n            High = log(mean(High)),\n            Low = log(mean(Low)),\n            Close = log(mean(Close)),\n            AdjClose = log(mean(`Adj Close`)),\n            Volume = log(mean(Volume)))"
  },
  {
    "objectID": "Content/Blog/AppleInc_IncomeStatement/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "href": "Content/Blog/AppleInc_IncomeStatement/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Times Series Plots of Apple Inc. Stock Prices",
    "text": "Times Series Plots of Apple Inc. Stock Prices\n\np &lt;- yearlyDs %&gt;%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price Since 1981\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  myTheme()\n\nggplotly(p) %&gt;%\n  layout(hovermode = \"x unified\") %&gt;% \n  style(hovertext = paste0(\" High: $\", round(yearlyDs$High,2)),\n        traces = 1) %&gt;%\n  style(hovertext = paste0(\" Low: $\", round(yearlyDs$Low,2)),\n        traces = 2) %&gt;%\n  style(hovertext = paste0(\" AdjClose: $\", round(yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "Content/Blog/AppleInc_IncomeStatement/index.html#applying-log-norm",
    "href": "Content/Blog/AppleInc_IncomeStatement/index.html#applying-log-norm",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Applying Log-norm",
    "text": "Applying Log-norm\n\np2 &lt;- log_yearlyDs %&gt;%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price (Log-Normalized)\") +\n  myTheme()\n\nggplotly(p2, tooltip = \"text\") %&gt;%\n  layout(hovermode = \"x unified\", \n         hovertext = paste0(\" Year: \", log_yearlyDs$Year)) %&gt;% \n  style(hovertext = paste0(\" High: \", round(log_yearlyDs$High,2)),\n        traces = 1) %&gt;%\n  style(hovertext = paste0(\" Low: \", round(log_yearlyDs$Low,2)),\n        traces = 2) %&gt;%\n  style(hovertext = paste0(\" AdjClose: \", round(log_yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "Content/Blog/AppleInc_IncomeStatement/index.html#conclusion",
    "href": "Content/Blog/AppleInc_IncomeStatement/index.html#conclusion",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Conclusion",
    "text": "Conclusion\nThe utilization of R’s plotly package for visualizing Time Series Graphs in the context of Apple’s stock prices has yielded valuable insights. The analysis revealed a substantial increase in Apple’s stock prices following 2010, fueled by the success of the iPhone and the company’s adept marketing strategy. The study emphasizes the transformative impact of Apple’s technology and its contributions to the tech industry and economic growth.\nBy employing dynamic and interactive visualizations, this project has enhanced the interpretation of Time Series Graphs, enabling a deeper understanding of the trends in the stock market. The use of R’s plotly package has proven to be a valuable tool in visual data exploration and storytelling. This undertaking serves as a testament to the power of R’s plotly in uncovering meaningful patterns and explaining the dynamics of stock market trends in a more engaging and informative manner."
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#why-use-webr-in-education",
    "href": "Content/Blog/Teaching_With_WebR/index.html#why-use-webr-in-education",
    "title": "Interactive Learning with WebR",
    "section": "Why use webR in Education?",
    "text": "Why use webR in Education?\nwebR seamlessly brings the power of R to the browser, removing the need for complex local installations. This accessibility is crucial for modern classrooms, where students might be working on different devices and operating systems. By embedding R code directly into educational materials, webR facilitates instant feedback, interactive exercises, and dynamic visualizations, making data science education more intuitive and approachable."
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#key-features-of-webr",
    "href": "Content/Blog/Teaching_With_WebR/index.html#key-features-of-webr",
    "title": "Interactive Learning with WebR",
    "section": "Key Features of webR",
    "text": "Key Features of webR\n\nCross-Platform Compatibility: Students can run R code on any device with a web browser, ensuring a consistent learning experience.\nImmediate Feedback: webR allows for instant execution of code, enabling students to see the results of their actions right away.\nInteractive Visualizations: By integrating with packages like ggplot2, educators can create interactive plots that students can manipulate directly in their browser."
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#implementation-in-data-wrangling-labs",
    "href": "Content/Blog/Teaching_With_WebR/index.html#implementation-in-data-wrangling-labs",
    "title": "Interactive Learning with WebR",
    "section": "Implementation in Data Wrangling Labs",
    "text": "Implementation in Data Wrangling Labs\nFor example, when teaching data wrangling concepts such as filtering, selecting, mutating, and summarizing, webR allows students to experiment with real datasets interactively. They can instantly see the impact of different data manipulation techniques, reinforcing their understanding through practice.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#benefits-for-educators",
    "href": "Content/Blog/Teaching_With_WebR/index.html#benefits-for-educators",
    "title": "Interactive Learning with WebR",
    "section": "Benefits for Educators",
    "text": "Benefits for Educators\nwebR enables educators to:\n\nSimplify Setup: No more lengthy instructions on installing R and its dependencies.\nEnhance Engagement: Interactive content keeps students involved, making complex topics more digestible.\nFacilitate Learning: By allowing students to explore data manipulation and visualization in real time, webR helps solidify their understanding of essential concepts.\n\n\nSummary\nwebR transforms how data science is taught, making R’s powerful features accessible and interactive in the classroom. Whether you’re teaching undergraduates new to R or advanced students refining their skills, webR offers a dynamic platform that enhances learning and encourages exploration."
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#what-is-data-wrangling",
    "href": "Content/Blog/Teaching_With_WebR/index.html#what-is-data-wrangling",
    "title": "Interactive Learning with WebR",
    "section": "What is Data Wrangling?",
    "text": "What is Data Wrangling?\nData wrangling is the process of converting messy, untidy data into a tidy format, making it suitable for data visualization and analysis.\n\nData is often messy: Real-world data is rarely provided in a tidy format.\nIndustry challenges: Many industries have poorly designed data structures, requiring data preparation before visualization.\nRarely tidy datasets: It is uncommon to receive a dataset that is already tidy.\n\n\nWhat is Tidy Data?\nTidy data is a structured format that aligns the organization of a dataset with its underlying meaning. In tidy data:\n\nEach variable has its own column: Every column in the dataset corresponds to a specific variable or attribute.\nEach observation has its own row: Every row captures a single observation or data entry.\nEach cell contains a single value: Each cell holds one distinct piece of information for a particular variable and observation.\n\nIn most cases, data is often imported using SQL to create narrower datasets. While we won’t cover SQL in this course, it’s a valuable skill to learn in the future. For now, we’ll focus on using R to manipulate and create subset datasets from larger datasets for focused analysis.\n\n\nWhat Causes Untidy Data?\n\nIncorrect/Inconsistent Dates: Dates can be tricky because they might be formatted differently across datasets or have errors like typos or missing parts. For example, some data might use “MM/DD/YYYY” while others use “YYYY-MM-DD,” leading to confusion and potential errors when analyzing time-based data.\nWide Format Times: Time data is sometimes presented in a wide format, where each column represents a different time period. This structure can make it difficult to perform certain types of analysis, as many statistical and visualization tools prefer data in a long format, where each row represents a single observation at a specific time.\nVoid or Misspelled Descriptions: Descriptions and labels are often incomplete, missing, or contain typos. These errors can make it challenging to interpret the data correctly, especially when variables are not clearly defined or are inconsistent across different parts of the dataset.\nMissing Values: Missing data is common and can occur in any part of a dataset, leading to gaps that can skew analysis or result in errors. Handling these missing values is crucial for ensuring that any conclusions drawn from the data are accurate.\nCondensed or Incorrect Headers: Column names might be too short, unclear, or incorrectly labeled, leading to confusion about what the data actually represents. For example, a column labeled “Pop” might be ambiguous—does it refer to population, popularity, or something else?\nRow Content Split: Sometimes, a single column contains data that should be divided into multiple columns, such as when a “Location” column includes both city and state. This issue can make it difficult to analyze the data separately or perform operations that rely on more granular details. These common issues contribute to untidy data, which can complicate analysis and lead to inaccurate results.\n\nMastering data wrangling is crucial because you might have to handle datasets with millions of rows and hundreds of columns."
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#getting-started",
    "href": "Content/Blog/Teaching_With_WebR/index.html#getting-started",
    "title": "Interactive Learning with WebR",
    "section": "Getting Started",
    "text": "Getting Started\nFirst, ensure you have the necessary packages installed and loaded. We will use the dplyr, lubridate, readr, ggplot2, and tidyr packages for our examples.\n\n\n\n\n\n\nDownloading R-packages\n\n\n\nUse install.packages('Name of Package') to install an R package. Careful! Package names are case sensitive, so install.packages(‘GGplot2’) will not work, but install.packages('ggplot2') will.\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(lubridate)\n\n\nDownload the Data\n\n\n\n\n\n\nRunning Code Block Shortcuts\n\n\n\nMac users: Use ⌘ + return to run single or highlighted line(s). Use shift + return to run entire code block\nWindows users: Use ctrl + enter to run single or highlighted line(s). Use shift + enter to run entire code block\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVerify Datasets with head()\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nDescription of the Dataset\n\ncountry: Country name from a predefined list of 10 countries.\nyear: Years between 2010 and 2023.\npopulation: Real-world population size for each country and year.\ngdp: Gross Domestic Product (GDP) in USD millions for each country and year.\ngdp_per_capita: GDP per capita, calculated as GDP divided by population.\nlife_expectancy: Life expectancy of citzens\nbirth_rate: Birth rate for each country and year\ntemperature: Average temperature in celsius per country\nregion: Geographical region corresponding to each country.\ncategory: Classification of the country as “First World,” “Second World,” or “Third World.”"
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#tidy-data-wrangling",
    "href": "Content/Blog/Teaching_With_WebR/index.html#tidy-data-wrangling",
    "title": "Interactive Learning with WebR",
    "section": "Tidy Data Wrangling",
    "text": "Tidy Data Wrangling\nImportant Concepts\n\nFiltering: Filter data based on conditions such as year, country, or region.\nSelecting: Select specific columns for focused analysis.\nMutating: Create new columns, such as cases per 100,000 population.\nSummarizing: Aggregate data by country, year, or region to find totals and averages.\n\n\nFiltering\nFiltering is essential for narrowing down datasets to the most relevant information, making patterns easier to identify.\n\nExample 1\nYou are tasked with visualizing trends in life expectancy in Asian countries between 2010 and 2020.\n\nFiltering Process\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat did we do here?\nBy focusing on specific countries and years, filtering allows for more targeted and relevant visualizations, making it easier to analyze trends and patterns specific to the context.\n\n\n\nExample 2\nAnalyze the relationship between GDP per capita and life expectancy in European countries with a GDP per capita above $30,000 for the years 2015-2020.\n\nFiltering Process\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat did we do here?\nFiltering based on economic indicators allows for focused analysis on the relationship between wealth and life expectancy, removing noise from countries with different economic conditions.\n\n\n\nFiltering Challenges\n\nPopulation Size in Africa:\n\nFilter: Only countries from Africa where the population exceeds 50 million.\nPurpose: Isolate data for large African nations to analyze trends specific to highly populated areas.\n\nEconomic Data in Europe:\n\nFilter: Show data only for the years 2015-2020 for European countries with GDP per capita above $30,000.\nPurpose: Focus on wealthy European countries during a specific period to study economic outcomes.\n\nHigh Birth Rates in Asia:\n\nFilter: Data for Asian countries where the birth rate is above 2.5.\nPurpose: Analyze regions with high birth rates, possibly indicating population growth trends.\n\nCold Regions in Asia:\n\nFilter: Asian countries where the average temperature is below 10°C between 2010 and 2020.\nPurpose: Focus on colder regions in Asia to study how temperature may correlate with other demographic factors.\n\nGDP Data with Missing Values:\n\nFilter: Remove any entries with missing gdp_per_capita values for the years 2010-2020.\nPurpose: Ensure clean data for economic analysis, removing incomplete records that could skew results.\n\n\n\n\n\nSelecting\nSelecting allows you to focus on specific columns relevant to your analysis.\n\nExample 1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSelecting Challenges\n\nSelect columns related to economic indicators (e.g., country, gdp_per_capita, population) for further analysis.\nCreate a dataset with only the year, life_expectancy, and temperature columns for all countries and show the first 5 rows.\nChoose columns that exclude any geographical information and check the first 10 rows.\nSelect and rename the country and population columns to nation and pop_size, respectively.\nCreate a new dataset with only the year, population, and a newly created column, population_in_millions (which should be calculated as population / 1e6).\n\n\n\n\nMutating\nMutating helps create new columns based on existing data.\n\nExample 1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nMutating Challenges\n\nCreate a new column called gdp_total that multiplies gdp_per_capita by population.\nAdd a new column that indicates whether a country’s GDP per capita is above or below a certain threshold (e.g., $20,000).\nMutate the temperature column to create a new column, temperature_f, that converts Celsius to Fahrenheit.\nCreate a population_density column by dividing population by a given area (assuming you have area data).\nGenerate a column that calculates the ratio of birth rate to life expectancy for each country.\n\n\n\n\nSummarizing\nSummarizing aggregates data by country, year, or region to find totals and averages.\n\nExample 1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSummarizing Challenges\n\nSummarize the dataset by finding the average temperature for each region.\nAggregate the data to find the total population and average life expectancy for each continent.\nGroup the data by country and summarize to find the maximum and minimum GDP per capita for each country.\nSummarize by year to find the total population and average birth rate each year.\nCreate a summary that calculates the total population and average GDP per capita for countries classified as “First World.”"
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#untidy-data-wrangling",
    "href": "Content/Blog/Teaching_With_WebR/index.html#untidy-data-wrangling",
    "title": "Interactive Learning with WebR",
    "section": "Untidy Data Wrangling",
    "text": "Untidy Data Wrangling\n\nHandling Wide vs. Long Formats\nUntidy data often comes in a “wide” format, where multiple variables are stored across columns rather than in a long format where each observation is a row.\n\nCreating an Untidy Version\nTo demonstrate this, let’s take our dataset and convert it into a wide format, then back to a long format.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, let’s convert this wide data back to a long format.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis process shows how data can be reshaped for different analytical needs. Wide format is useful for certain analyses but often needs to be converted to long format for modeling and visualization.\n\n\n\nHandling Misspelled Header Column Names\nSometimes datasets come with misspelled or inconsistent column names, which can lead to errors in data manipulation.\n\nCreating Misspelled Header Names\nLet’s create a dataset with intentionally misspelled column names and then fix them.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFixing the Misspelled Column Names\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis illustrates how to identify and correct misspelled column names, which is a crucial step in data cleaning.\n\n\n\nHandling Row Content Split and Reverse\nSometimes, data stored in a single column needs to be split into multiple columns or vice versa.\n\nMerging Two Columns into One\nLet’s take the country and year columns and merge them into a single column.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSplitting the Merged Column Back into Two\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis demonstrates how to handle situations where data needs to be recombined or separated for different purposes.\n\n\n\nHandling Dates\nIn some cases, it might be necessary to convert a year column from a numeric format (double) into a proper date format for time series analysis or plotting purposes. Here’s how you can do that in R using the lubridate package.\n\nExample 1\n\nConverting year to a Date\nLet’s convert the year column into a date format, setting it as January 1st of that year.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat did we do here?\n\npaste0(year, \"-01-01\"): Combines the year with the string “-01-01” to create a date string like “2010-01-01”.\nymd(): Converts the resulting string into a date object in the “Year-Month-Day” format.\n\nThis creates a new column, year_date, which is now in the proper date format.\n\n\n\nChallenges [Solutions]\n\nConvert year to end of year date:\n\nTask: Convert the year column to a date format, but set it as December 31st of that year.\nPurpose: Useful for representing data that summarizes annual results.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCreate a quarterly date:\n\nTask: Convert the year column into a date representing the first quarter (e.g., “2010-03-31”).\nPurpose: Useful for quarterly analysis.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMid-year date conversion:\n\nTask: Convert the year column to a date format, setting it as June 30th of each year.\nPurpose: Represents mid-year data points.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUse year as a dynamic time period:\n\nTask: Convert the year column to represent the last day of a chosen month (e.g., November).\nPurpose: Allows for flexibility depending on the analysis context.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConvert year to fiscal year start date:\n\nTask: Convert the year column into a date representing the start of the fiscal year (e.g., April 1st).\nPurpose: Useful for financial and budgetary analyses.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nHandling Missing Data\nDealing with missing data is a crucial aspect of data wrangling. Missing data can occur for various reasons, and understanding the nature of these missing values is essential for appropriate handling.\n\nTypes of Missing Data\n\nMCAR (Missing Completely at Random): Data is missing entirely at random, with no relationship between the missing data and any other observed or unobserved data. The analysis remains unbiased if this missing data is ignored.\nMAR (Missing at Random): The likelihood of missing data on a variable is related to other observed variables but not to the value of the variable itself .\nMNAR (Missing Not at Random): The missingness is related to the unobserved data itself, meaning the missing values are related to the actual value that is missing.\n\n\n\nSimulating MAR Data\nWe’ll create a scenario where gdp_per_capita is more likely to be missing if the population is below a certain threshold, making it “missing at random” based on population size.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSimulating MNAR Data\nWe’ll simulate a scenario where the likelihood of life_expectancy being missing is higher if life expectancy is lower than 60 years, making it “missing not at random.”\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nMissing Data | Before Wrangling\n\nBefore Handling Missing Data\nLet’s visualize the data before handling missing values, focusing on the relationship between GDP per capita and life expectancy.\n\n\nVisualization with MAR Data\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualization with MNAR Data\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nMissing Data | How to Wrangle\nTo handle the missing data, we’ll apply a simple imputation strategy, filling in missing values with the median of the respective variable.\n\nImputing Missing Values for MAR Data\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nMissing Data | After Wrangling\nLet’s visualize the data again after imputing the missing values.\n\nVisualization with MAR Data (Imputed)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nImputing Missing Values for MNAR Data\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualization with MNAR Data (Imputed)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSummary\nUnderstanding the different types of missing data—MCAR, MAR, and MNAR—is crucial for choosing the right approach to handle them. We explored how to simulate and visualize MAR and MNAR scenarios in our dataset, highlighting the importance of addressing missing data for accurate analysis. By comparing visualizations before and after imputation, students can grasp the significant impact missing data can have on their results and learn effective strategies to mitigate these issues."
  },
  {
    "objectID": "Content/Blog/Teaching_With_WebR/index.html#creating-a-pseudo-publication-ready-visualization",
    "href": "Content/Blog/Teaching_With_WebR/index.html#creating-a-pseudo-publication-ready-visualization",
    "title": "Interactive Learning with WebR",
    "section": "Creating a Pseudo-Publication Ready Visualization",
    "text": "Creating a Pseudo-Publication Ready Visualization\nWe’ll combine all the data wrangling techniques you’ve learned—filtering, selecting, mutating, summarizing—to perform a detailed analysis and produce a polished, publication-ready visualization.\n\nCreating a Professional-Quality Visualization\nHere’s a step-by-step guide to transform and visualize data from 10 countries in the dataset:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nReview & Scrutinize\nWhat is this visualization?: Write a brief paragraph describing the design, purpose, and key message of the plot. Explain what the visualization is intended to show and how it effectively communicates the data.\nWhy is this visualization nearly publication-ready?: In 2-3 sentences, discuss what makes the plot polished and professional, highlighting any elements that could make it suitable for publication."
  },
  {
    "objectID": "Assets/Datasets/readme.html",
    "href": "Assets/Datasets/readme.html",
    "title": "List of Datasets",
    "section": "",
    "text": "Here is a list of datasets available in this repository. Click on each dataset name to access the file directly.\n\nall-ages.csv\namazonProducts.csv\nAppleInc_Stocks.csv\nCard-Transaction_log.csv\ncountries.csv\ncustomerReturnTest.csv\ncustomerReturnTrain.csv\nFinal_dev_pub.csv\nfraud.png\nhealthcareSpending.csv\ninsurance.csv\nmegaGymDataset.csv\nolympics.csv\nopenml_1590.csv\npinot.rds\nretail.csv\nsalary_and_stats.csv\nscuffed_pokedex.csv\nseverityLevels.csv\nsubmission.csv\ntuition_cost.csv\ntuition_income.csv\nwinequality-red.csv\nwinequality-white.csv\n\n\n\nUse read_csv(Assets/Datasets/data.csv)\n\n\n\nUse read_csv(\"../../../Assets/Datasets/data.csv\")\nReplace data.csv with the dataset of your choice"
  },
  {
    "objectID": "Assets/Datasets/readme.html#load-data-correctly-project-development-in-progress",
    "href": "Assets/Datasets/readme.html#load-data-correctly-project-development-in-progress",
    "title": "List of Datasets",
    "section": "",
    "text": "Use read_csv(Assets/Datasets/data.csv)"
  },
  {
    "objectID": "Assets/Datasets/readme.html#render-quarto-documents-deployment",
    "href": "Assets/Datasets/readme.html#render-quarto-documents-deployment",
    "title": "List of Datasets",
    "section": "",
    "text": "Use read_csv(\"../../../Assets/Datasets/data.csv\")\nReplace data.csv with the dataset of your choice"
  },
  {
    "objectID": "Assets/HTML/Blog/readme.html",
    "href": "Assets/HTML/Blog/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "Assets/Extensions/readme.html",
    "href": "Assets/Extensions/readme.html",
    "title": "Extensions",
    "section": "",
    "text": "Extensions\nHere are all the extensions that I have sused in my entire website application."
  },
  {
    "objectID": "Assets/Scripts/Resources/readme.html",
    "href": "Assets/Scripts/Resources/readme.html",
    "title": "Brian Cervantes Alvarez",
    "section": "",
    "text": "jdfkldjfl"
  },
  {
    "objectID": "Assets/Scripts/Blog/readme.html",
    "href": "Assets/Scripts/Blog/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-eda-on-red-white-wine-means",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-eda-on-red-white-wine-means",
    "title": "Applied Multivariate Analysis",
    "section": "Perform EDA on Red & White Wine Means:",
    "text": "Perform EDA on Red & White Wine Means:\nI conducted an exploratory data analysis in comparing means for the 11 chemical attributes in the red and white wines. Notably, ‘totalSulfurDioxide’ showed significant mean differences, followed by ‘freeSulfurDioxide’ and ‘residualSugar.’ These attributes currently stand out showing a clear difference between the means of white and red wines."
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-manova-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-red-and-white-wines-with-a-95-confidence-level.",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-manova-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-red-and-white-wines-with-a-95-confidence-level.",
    "title": "Applied Multivariate Analysis",
    "section": "Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.",
    "text": "Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.\n\\begin{align*}\n&H_0: \\bar{\\mu}_{\\text{red}} = \\bar{\\mu}_{\\text{white}} \\\\\n&H_A: \\bar{\\mu}_{\\text{red}} \\neq \\bar{\\mu}_{\\text{white}}\n\\end{align*}\n\nMANOVA Results\nThe MANOVA reveals a significant disparity in the means for specific chemical traits between red and white wines. Notably, Pillai’s Trace (0.86158) indicates a robust effect, accounting for approximately 86.128\\% of the variance. The p-value of 2.2 \\times 10^{-16} signifies significant differences in the mean vectors. Hence, the MANOVA decisively rejects the null hypothesis of no difference in means, and we have exceptionally high confidence in accepting the alternative-that there is a difference in means between each wine."
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-classification-modeling-on-red-white-wines",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-classification-modeling-on-red-white-wines",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Classification Modeling on Red & White Wines",
    "text": "Perform Classification Modeling on Red & White Wines\n\nTrain-Test Split & Cross-Validation Set up\nI performed a train-test split to ensure the models can handle new, unseen data. I allocated 70\\% to the training data to have a larger sample for the testing data (30\\%). To enhance reliability, I employed cross-validation, repeatedly splitting the data into different training and testing sets. This approach provides a more comprehensive evaluation of the model’s effectiveness.\n\n\nRandom Forest, SVM and Logistic Models\nFor my models, I’ve selected Random Forest, Support Vector Machine, and Logistic Regression as promising candidates for effective classification. Logistic Regression is particularly beneficial in binary classification scenarios due to its simplicity and interpretability. Meanwhile, Random Forest excels in capturing complex relationships through ensemble learning, and Support Vector Machine demonstrates proficiency in handling both linear and non-linear patterns. As the results will show later, Random Forest performed the best followed by SVM. Note, the models were not tuned to use their best hyperparameters.\n\n\nMetrics & Variable Importance\nThe confusion matrix for the red and white wine classification using the Random Forest model shows strong performance. The model correctly identified 474 red wines and 1464 white wines, with only 5 red wines and 5 white wines misclassified. This indicates high accuracy in both precision and recall. In comparison to the Support Vector Machine (SVM) and Logistic Regression models, the Random Forest performed better by minimizing misclassifications. The SVM model had slightly more misclassified instances (13 in total), while the Logistic Regression model had 17 misclassifications. The Random Forest’s performance makes it a better choice for this classifying red and white wines compared to SVM and Logistic Regression.\nThe variable importance analysis shows that “chlorides,” with a significance of 100%, is the most crucial feature for distinguishing red and white wines. Additionally, “totalSulfurDioxide” (96.92%) and “volatileAcidity” (43.47%) also played key roles, contributing to the model’s clear performance in wine classification.\n\n\nClassifying a New Red Wine Drawn From The Same Population\nTo estimate the probability of correctly classifying a new red wine drawn from the same population, we can use the concept of recall.\nIn our confusion matrix:\n \\text{Recall (for red wine)} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}} \nTo find the probability:\n \\text{Recall (for red wine)} = \\frac{474}{474 + 5} = \\frac{474}{479}  = 0.9896\nSo, the estimated probability of correctly classifying a new red wine, drawn from the same population, is approximately \\frac{474}{479}, or roughly 98.96\\%. This suggests a very high probability of correctly identifying red wines based on the model’s current performance."
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#k-means-clustering",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#k-means-clustering",
    "title": "Applied Multivariate Analysis",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nI chose k-means clustering with Euclidean distance for its efficiency with standardized numerical data. While k = 2 visually showed a clear distinction between red and white wines, higher k values (for example, 3 or 4) led to overlapping clusters, affecting the meaningful separation observed with k = 2."
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-manovas-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-wines-with-different-qualityquality-groups-with-a-95-confidence-level.",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-manovas-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-wines-with-different-qualityquality-groups-with-a-95-confidence-level.",
    "title": "Applied Multivariate Analysis",
    "section": "Perform MANOVAs To Determine If There Are A Significant Difference In Mean Vectors Between Wines With Different Quality/Quality Groups With A 95% Confidence Level.",
    "text": "Perform MANOVAs To Determine If There Are A Significant Difference In Mean Vectors Between Wines With Different Quality/Quality Groups With A 95% Confidence Level.\n\nMANOVA for Quality Levels 3, 4, 5, 6, 7, 8\n\\begin{align*}\n&H_0: \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 = \\mu_6 = \\mu_7 = \\mu_8 \\\\\n&H_A: \\text{At least one of } \\mu_2, \\mu_3, \\mu_4, \\mu_5, \\mu_6, \\mu_7, \\text{ or } \\mu_8 \\text{ is different}\n\\end{align*}\nIn the first analysis, we looked at the Original Quality Scores, and the results were highly significant. The p-value was super close to zero, less than 2.2 x 10^{-16}. This means there are big differences in the average chemical properties for different quality scores. Therefore, the mean vectors for each quality level varied, providing strong support for rejecting the null hypothesis.\n\n\nMANOVA for Quality Groups [Low, Medium, High]\n\\begin{align*}\n&H_0: \\mu_{\\text{Low}} = \\mu_{\\text{Medium}} = \\mu_{\\text{High}} \\\\\n&H_A: \\text{At least one of } \\mu_{\\text{Low}}, \\mu_{\\text{Medium}}, \\text{ or } \\mu_{\\text{High}} \\text{ is different}\n\\end{align*}\nIn the second analysis, we focused on Quality Groups (Low, Medium, High), and the results were also highly significant. The p-value was very close to zero, less than 2.2 x 10^{-16}. This indicates significant differences in the average chemical properties across different quality groups. We have strong evidence that the mean vectors between each quality group differed. Therefore, we have evidence to reject the null and be in favor of the alternative.\n\n\nOverall MANOVA Test Conclusion\nTo summarize, our MANOVA tests reveal significant differences in average values for both original quality scores and quality groups. For original scores, statistics like Pillai’s trace and Wilks’ lambda had extremely low p-values p &lt; 2.2 x 10^{-16}. Quality groups exhibited similar results."
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-classification-on-quality-for-red-wines",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-classification-on-quality-for-red-wines",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Classification on Quality for Red Wines",
    "text": "Perform Classification on Quality for Red Wines\n\nRandom Forest and SVM models\nI performed the same procedure from the previous classification of red and white wines. I’ve selected Random Forest & Support Vector Machine the top models for classification. Logistic Regression is not designed for multiple classes. Interestingly, the Random Forest performed the best again, followed by SVM. It’s important to note that the models were not fine-tuned for hyperparameters at this stage.\n\n\nMetrics & Variable Importance\nRandom Forest emerged as the top-performing model once again, with an accuracy of 69.2\\%. For instance, we can observe that quality level 5 has the highest number of correct predictions (163), while quality levels 4 and 6 have some misclassifications. Among the features, alcohol, total sulfur dioxide, and volatile acidity emerged as the top three influential variables, showcasing their significance in predicting wine quality in red wines."
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-principal-component-analysis",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-principal-component-analysis",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Principal Component Analysis",
    "text": "Perform Principal Component Analysis\n\nExplaining PC1 & PC2\nPC1 can be interpreted as representing “Wine Body.” Red wines with higher fixed acidity, citric acid, free sulfur dioxide, and total sulfur dioxide contribute positively to this component, indicating a fuller and more robust body. Hence, higher levels of volatile acidity, residual sugar, and alcohol contribute negatively to this component.\nPC2 can be labeled as “Fermentation Characteristics.” Additionally, red wines with elevated levels of free sulfur dioxide, total sulfur dioxide, and density contribute positively to this component, highlighting aspects related to the fermentation process. On the other end, higher alcohol content and volatile acidity contribute negatively to PC2.\n\n\nRandom Forest Model with 2 PCA\nIn the confusion matrix, it’s evident that the model struggled to accurately predict certain classes, particularly in categories 3, 4, and 7, where the predicted values differ from the actual values. To add, random forest model achieved an accuracy of 58.07% which is quite below the previous models. Despite its limitations, the model demonstrated some success in capturing patterns related to “Wine Body” and “Fermentation Characteristics.” And it’s with just 2 variables with linear combinations.\n\n\nRandom Forest Model with 11 PCAs\nThe random forest model attained an accuracy of 68.13%. Plus, it excelled in predicting class 5 but faced challenges in classes 3, 4, 6, and 7. Principal Component Analysis (PCA) highlights PC2 as the most influential (100%), followed by PC3 (80.49%), PC5 (32.47%), and others. This suggests a need for further analysis to enhance predictions in specific classes and leverage insights from key Principal Components for optimization.\n\n\nComparison between Random Forest Models (normal, 2PCs, 11PCs)\nIn comparing the Random Forest models, both the normal model and the 11 PCs model achieve an accuracy of approximately 68%, surpassing the 2 PCs model, which attains an accuracy of 58.07%. It’s noteworthy that the 2 PCs model demonstrates the potency of PCA, albeit with a trade-off in interpretability. Despite the challenges encountered, each model variant provides valuable insights for optimizing the predictive power. The room for improvement is wide open. Factors such as hyperparameter tuning, other models that were not explored, feature engineering, and delving further into factor analysis are instances that could be used to maximize performance."
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-eda-on-red-white-wine-means-1",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-eda-on-red-white-wine-means-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform EDA on Red & White Wine Means:",
    "text": "Perform EDA on Red & White Wine Means:\n\n# Calculate the mean vectors for red and white wines \n# separately for each of the 11 chemical attributes\nmeanVectors &lt;- wineDs %&gt;% \n  group_by(wineType) %&gt;%\n  summarize_all(mean)\n\n# Display the mean vectors\nhead(meanVectors, 2)\n\n# A tibble: 2 × 12\n  wineType chlorides density    pH sulphates alcohol fixedAcidity\n  &lt;fct&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 red         0.0875   0.997  3.31     0.658    10.4         8.32\n2 white       0.0458   0.994  3.19     0.490    10.5         6.85\n# ℹ 5 more variables: volatileAcidity &lt;dbl&gt;, citricAcid &lt;dbl&gt;,\n#   residualSugar &lt;dbl&gt;, freeSulfurDioxide &lt;dbl&gt;, totalSulfurDioxide &lt;dbl&gt;\n\n# Convert to long format for plotting\nmeanDs &lt;- tidyr::gather(meanVectors,\n                        key = \"attribute\", \n                        value = \"means\", -wineType)\n\n#meanDs\n\n# Plot\np1 &lt;- ggplot(meanDs, aes(x = means, y = attribute, fill = wineType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Mean Values for Red|White Wines\",\n       x = \"Mean Value\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\n\n# Plot\np2 &lt;- ggplot(meanDs, aes(x = log(means), y = attribute, fill = wineType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Log-Transformed Mean Values for Red|White Wines\",\n       x = \"Mean Value\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\nmeanDifDs &lt;- meanDs %&gt;%\n  spread(wineType, means) %&gt;%\n  mutate(meanDifference = red - white)\n\n# Plot the mean differences\np3 &lt;- ggplot(meanDifDs, aes(x = meanDifference, y = attribute)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", \n           fill = \"red\", color = \"black\") +\n  labs(title = \"Mean Differences between Red & White Wines\",\n       x = \"Mean Difference\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\n\n# Show plot 1 for report\np1\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\np3"
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-manova-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-red-and-white-wines-with-a-95-confidence-level.-1",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-manova-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-red-and-white-wines-with-a-95-confidence-level.-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.",
    "text": "Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.\n\nwineManova &lt;- manova(cbind(chlorides, density, pH, sulphates, \n                           alcohol, fixedAcidity, volatileAcidity, \n                           citricAcid, residualSugar, freeSulfurDioxide,\n                           totalSulfurDioxide) ~ wineType, data = wineDs) \n\n# Print the summary of the MANOVA\nsummary(wineManova)\n\n            Df  Pillai approx F num Df den Df                Pr(&gt;F)    \nwineType     1 0.86158   3669.6     11   6485 &lt; 0.00000000000000022 ***\nResiduals 6495                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nMANOVA Results"
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-classification-modeling-on-red-white-wines-1",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-classification-modeling-on-red-white-wines-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Classification Modeling on Red & White Wines",
    "text": "Perform Classification Modeling on Red & White Wines\n\nTrain-Test Split & Cross-Validation Set up\n\nsplitIndex &lt;- createDataPartition(wineDs$wineType, p = 0.7, list = FALSE)\ntrainData &lt;- wineDs[splitIndex, ]\ntestData &lt;- wineDs[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n\n\nRandom Forest, SVM and Logistic Models\n\nset.seed(2013)\n\n# Train Random Forest\nrfModel &lt;- train(wineType ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Train Logistic Regression\nlogisticModel &lt;- train(wineType ~ ., \n                       data = trainData,\n                       method = \"glm\",\n                       family = \"binomial\")\n\n# Train Support Vector Machine\nsvmModel &lt;- train(wineType ~ ., \n                  data = trainData,\n                  method = \"svmRadial\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\nlogisticPred &lt;- predict(logisticModel, newdata = testData)\nsvmPred &lt;- predict(svmModel, newdata = testData)\n\n\n\nMetrics & Variable Importance\n\n# Random Forest model metrics\nconfMatrixRF &lt;- confusionMatrix(rfPred, testData$wineType, \n                                dnn = c(\"Prediction\", \"Reference\"))\naccuracyRF &lt;- confMatrixRF$overall[\"Accuracy\"]\n\n# Logistic Regression model metrics\nconfMatrixLogistic &lt;- confusionMatrix(logisticPred, testData$wineType, \n                                      dnn = c(\"Prediction\", \"Reference\"))\naccuracyLogistic &lt;- confMatrixLogistic$overall[\"Accuracy\"]\n\n# SVM model metrics\nconfMatrixSVM &lt;- confusionMatrix(svmPred, testData$wineType, \n                                 dnn = c(\"Prediction\", \"Reference\"))\naccuracySVM &lt;- confMatrixSVM$overall[\"Accuracy\"]\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$wineType)) +\n    scale_y_discrete(labels = levels(testData$wineType))\n}\n\n#confMatrixRF\nconfMatrixLogistic\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  red white\n     red    472     2\n     white    7  1467\n                                             \n               Accuracy : 0.9954             \n                 95% CI : (0.9912, 0.9979)   \n    No Information Rate : 0.7541             \n    P-Value [Acc &gt; NIR] : &lt;0.0000000000000002\n                                             \n                  Kappa : 0.9875             \n                                             \n Mcnemar's Test P-Value : 0.1824             \n                                             \n            Sensitivity : 0.9854             \n            Specificity : 0.9986             \n         Pos Pred Value : 0.9958             \n         Neg Pred Value : 0.9953             \n             Prevalence : 0.2459             \n         Detection Rate : 0.2423             \n   Detection Prevalence : 0.2433             \n      Balanced Accuracy : 0.9920             \n                                             \n       'Positive' Class : red                \n                                             \n\n#confMatrixSVM\n\n# Plot Confusion Matrices for each model\nplotCM(confMatrixRF, \"Random Forest\")\n\n\n\n\n\n\n\nplotCM(confMatrixLogistic, \"Logistic Regression\")\n\n\n\n\n\n\n\nplotCM(confMatrixSVM, \"Support Vector Machine (SVM)\")\n\n\n\n\n\n\n\n# Print the metrics for each model\nprint(\"Random Forest Model Results\")\n\n[1] \"Random Forest Model Results\"\n\nprint(paste(\"Accuracy:\", round(accuracyRF, 4)))\n\n[1] \"Accuracy: 0.9959\"\n\nprint(\"Logistic Regression Model Results:\")\n\n[1] \"Logistic Regression Model Results:\"\n\nprint(paste(\"Accuracy:\", round(accuracyLogistic, 4)))\n\n[1] \"Accuracy: 0.9954\"\n\nprint(\"Support Vector Machine (SVM) Model Results:\")\n\n[1] \"Support Vector Machine (SVM) Model Results:\"\n\nprint(paste(\"Accuracy:\", round(accuracySVM, 4)))\n\n[1] \"Accuracy: 0.9964\"\n\n# Get variable importance from the best model\nvarImp(rfModel)\n\nrf variable importance\n\n                   Overall\nchlorides          100.000\ntotalSulfurDioxide  99.094\nvolatileAcidity     45.672\ndensity             26.868\nresidualSugar       20.723\nfreeSulfurDioxide   19.350\nsulphates           18.844\nfixedAcidity        17.806\ncitricAcid           7.336\npH                   4.955\nalcohol              0.000\n\n\n\n\nClassifying a New Red Wine Drawn From The Same Population"
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#clustering",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#clustering",
    "title": "Applied Multivariate Analysis",
    "section": "Clustering:",
    "text": "Clustering:\n\nset.seed(123)\n\nds &lt;- wineDs %&gt;%\n  select(-wineType)\n\nds &lt;- scale(ds)\n\nfviz_nbclust(ds, kmeans, method='silhouette')\n\n\n\n\n\n\n\nkm.final &lt;- kmeans(ds, 2, nstart = 30)\n\nfviz_cluster(km.final, data = ds, \n             geom = \"point\",\n             ellipse.type = \"convex\", \n             ggtheme = theme_bw())"
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-manovas-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-wines-with-different-qualityquality-groups-with-a-95-confidence-level",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-manovas-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-wines-with-different-qualityquality-groups-with-a-95-confidence-level",
    "title": "Applied Multivariate Analysis",
    "section": "Perform MANOVAs to determine if there are a significant difference in mean vectors between wines with different quality/quality groups with a 95% confidence level?",
    "text": "Perform MANOVAs to determine if there are a significant difference in mean vectors between wines with different quality/quality groups with a 95% confidence level?\n\nMANOVA for Quality Levels 3, 4, 5, 6, 7, 8\n\nredWineDs &lt;- wine %&gt;%\n  filter(wineType == \"red\") %&gt;%\n  select(-wineType)\n\n# Extracting Columns\ncolVars &lt;-  cbind(\n  redWineDs$chlorides, redWineDs$density, redWineDs$pH, redWineDs$sulphates, \n  redWineDs$alcohol, redWineDs$fixedAcidity, redWineDs$volatileAcidity, \n  redWineDs$citricAcid, redWineDs$residualSugar, redWineDs$freeSulfurDioxide,\n  redWineDs$totalSulfurDioxide\n)\n\n# MANOVA Analysis - Quality, levels = 3,4,5,6,7,8\nmanaovaTest &lt;- manova(colVars ~ quality, data = redWineDs)\nsummary(manaovaTest)\n\n            Df  Pillai approx F num Df den Df                Pr(&gt;F)    \nquality      1 0.36055   81.348     11   1587 &lt; 0.00000000000000022 ***\nResiduals 1597                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nMANOVA for Quality Groups [Low, Medium, High]\n\n# Adding qualityGroup\nredWineDs &lt;- redWineDs %&gt;% \n  mutate(\n    qualityGroup = case_when(\n      quality %in% 3:4 ~ \"Low\",\n      quality %in% 5:6 ~ \"Medium\",\n      quality %in% 7:8 ~ \"High\"\n    )\n  ) %&gt;%\n  mutate(qualityGroup = factor(qualityGroup, levels = c(\"Low\", \n                                                        \"Medium\", \n                                                        \"High\")))\ncolVarsCategorized &lt;-  cbind(\n  redWineDs$chlorides, redWineDs$density, redWineDs$pH, \n  redWineDs$sulphates, redWineDs$alcohol, redWineDs$fixedAcidity, \n  redWineDs$volatileAcidity, redWineDs$citricAcid, \n  redWineDs$residualSugar, redWineDs$freeSulfurDioxide,\n  redWineDs$totalSulfurDioxide\n)\n\n# MANOVA Analysis - QualityGroup, Levels = \"Low\", \"Medium\", \"High\"\nmanaovaTest &lt;- manova(colVars ~ qualityGroup, data = redWineDs)\nsummary(manaovaTest)\n\n               Df  Pillai approx F num Df den Df                Pr(&gt;F)    \nqualityGroup    2 0.30989   26.453     22   3174 &lt; 0.00000000000000022 ***\nResiduals    1596                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nOverall MANOVA Test Conclusion"
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-classification-on-quality-for-red-wines-1",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-classification-on-quality-for-red-wines-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Classification on Quality for Red Wines",
    "text": "Perform Classification on Quality for Red Wines\n\nqualityDs &lt;- redWineDs %&gt;%\n  mutate(quality = factor(quality, levels = c(\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"))) %&gt;%\n  select(-qualityGroup)\n\n\n# Split the dataset into training and testing sets\nsplitIndex &lt;- createDataPartition(qualityDs$quality, p = 0.7, list = FALSE)\ntrainData &lt;- qualityDs[splitIndex, ]\ntestData &lt;- qualityDs[-splitIndex, ]\n\n# Create a train control object for repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n\nRandom Forest and SVM models\n\nset.seed(2013)\n# Random Forest model\nrfModel &lt;- train(quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# (SVM) model\nsvmModel &lt;- train(quality ~ ., \n                  data = trainData,\n                  method = \"svmLinear\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\nsvmPred &lt;- predict(svmModel, newdata = testData)\n\n\n\nMetrics & Variable Importance\n\n# Random Forest confusion matrix\nrfConfMatrix &lt;- confusionMatrix(rfPred, testData$quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy &lt;- rfConfMatrix$overall[\"Accuracy\"]\n\n# SVM confusion matrix\nsvmConfMatrix &lt;- confusionMatrix(svmPred, testData$quality, \n                                 dnn = c(\"Prediction\", \"Reference\"))\nsvmAccuracy &lt;- svmConfMatrix$overall[\"Accuracy\"]\n\n# Print the results\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n\n[1] \"Random Forest Accuracy: 0.691823899371069\"\n\nprint(paste(\"SVM Accuracy:\", svmAccuracy))\n\n[1] \"SVM Accuracy: 0.59538784067086\"\n\nrfConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   0   1   0   0\n         5   3  10 163  46   5   0\n         6   0   4  40 137  24   4\n         7   0   1   1   7  30   1\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                               \n               Accuracy : 0.6918               \n                 95% CI : (0.6482, 0.733)      \n    No Information Rate : 0.4277               \n    P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022\n                                               \n                  Kappa : 0.4953               \n                                               \n Mcnemar's Test P-Value : NA                   \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000 0.000000   0.7990   0.7173  0.50847  0.00000\nSpecificity          1.000000 0.997835   0.7656   0.7483  0.97608  1.00000\nPos Pred Value            NaN 0.000000   0.7181   0.6555  0.75000      NaN\nNeg Pred Value       0.993711 0.968487   0.8360   0.7985  0.93364  0.98952\nPrevalence           0.006289 0.031447   0.4277   0.4004  0.12369  0.01048\nDetection Rate       0.000000 0.000000   0.3417   0.2872  0.06289  0.00000\nDetection Prevalence 0.000000 0.002096   0.4759   0.4382  0.08386  0.00000\nBalanced Accuracy    0.500000 0.498918   0.7823   0.7328  0.74228  0.50000\n\nsvmConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   1   0   0   0\n         4   0   0   0   0   0   0\n         5   2  11 159  66   6   0\n         6   1   4  44 125  53   5\n         7   0   0   0   0   0   0\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                            \n               Accuracy : 0.5954            \n                 95% CI : (0.5498, 0.6398)  \n    No Information Rate : 0.4277            \n    P-Value [Acc &gt; NIR] : 0.0000000000001356\n                                            \n                  Kappa : 0.3101            \n                                            \n Mcnemar's Test P-Value : NA                \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000  0.00000   0.7794   0.6545   0.0000  0.00000\nSpecificity          0.997890  1.00000   0.6886   0.6259   1.0000  1.00000\nPos Pred Value       0.000000      NaN   0.6516   0.5388      NaN      NaN\nNeg Pred Value       0.993697  0.96855   0.8069   0.7306   0.8763  0.98952\nPrevalence           0.006289  0.03145   0.4277   0.4004   0.1237  0.01048\nDetection Rate       0.000000  0.00000   0.3333   0.2621   0.0000  0.00000\nDetection Prevalence 0.002096  0.00000   0.5115   0.4864   0.0000  0.00000\nBalanced Accuracy    0.498945  0.50000   0.7340   0.6402   0.5000  0.50000\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$quality)) +\n    scale_y_discrete(labels = levels(testData$quality))\n}\n\nplotCM(rfConfMatrix, \"Random Forest\")\n\n\n\n\n\n\n\nplotCM(svmConfMatrix, \"SVM\")\n\n\n\n\n\n\n\nplot(varImp(rfModel))"
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#perform-principal-component-analysis-1",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#perform-principal-component-analysis-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Principal Component Analysis",
    "text": "Perform Principal Component Analysis\n\nPart 1\n\nds &lt;- select(qualityDs, -quality)\n\n# Perform PCA\npcaResults &lt;- prcomp(ds, scale = T, center = T)\n\n# Print PCA summary\nsummary(pcaResults)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     1.7604 1.3878 1.2452 1.1015 0.97943 0.81216 0.76406\nProportion of Variance 0.2817 0.1751 0.1410 0.1103 0.08721 0.05996 0.05307\nCumulative Proportion  0.2817 0.4568 0.5978 0.7081 0.79528 0.85525 0.90832\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.65035 0.58706 0.42583 0.24405\nProportion of Variance 0.03845 0.03133 0.01648 0.00541\nCumulative Proportion  0.94677 0.97810 0.99459 1.00000\n\n# Visualize PCA results\nfviz_eig(pcaResults, addlabels = TRUE, kaiser = TRUE)\n\n\n\n\n\n\n\nfviz_pca_ind(pcaResults, geom = \"point\", col.ind = qualityDs$quality, palette = \"jco\")\n\n\n\n\n\n\n\nfviz_pca_biplot(pcaResults, geom = \"arrow\", col.var = \"contrib\", palette = \"jco\", alpha = 0.7)\n\n\n\n\n\n\n\npcaDs &lt;- rownames_to_column(as.data.frame(pcaResults$rotation))\n\npcaDs %&gt;% \n  select(rowname, PC1, PC2)\n\n              rowname         PC1          PC2\n1           chlorides  0.21224658  0.148051555\n2             density  0.39535301  0.233575490\n3                  pH -0.43851962  0.006710793\n4           sulphates  0.24292133 -0.037553916\n5             alcohol -0.11323207 -0.386180959\n6        fixedAcidity  0.48931422 -0.110502738\n7     volatileAcidity -0.23858436  0.274930480\n8          citricAcid  0.46363166 -0.151791356\n9       residualSugar  0.14610715  0.272080238\n10  freeSulfurDioxide -0.03615752  0.513566812\n11 totalSulfurDioxide  0.02357485  0.569486959\n\n# Adding back the quality with principal components\nprc &lt;- select(qualityDs, quality) %&gt;%\n  bind_cols(as.data.frame(pcaResults$x)) %&gt;%\n  select(quality, PC1, PC2) \n\n# Rename columns\nprc &lt;- prc %&gt;%\n  rename(\n    \"Quality\" = quality,\n    \"Wine Body\" = PC1,\n    \"Fermentation Characteristics\" = PC2\n  )\n\nprc2 &lt;- select(qualityDs, quality) %&gt;%\n  bind_cols(as.data.frame(pcaResults$x)) %&gt;%\n  select(quality, PC1, PC2, PC3, PC4, PC5, PC6, PC7, PC8, PC9, PC10, PC11) \n\nprc2 &lt;- prc2 %&gt;%\n  rename(\"Quality\" = quality)\n\nprc2\n\n# A tibble: 1,599 × 12\n   Quality    PC1    PC2    PC3      PC4     PC5    PC6    PC7    PC8      PC9\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 5       -1.62   0.451 -1.77   0.0437  -0.0670 -0.914  0.161  0.282 -0.00510\n 2 5       -0.799  1.86  -0.911  0.548    0.0184  0.929  1.01  -0.762  0.521  \n 3 5       -0.748  0.882 -1.17   0.411    0.0435  0.401  0.539 -0.598  0.0868 \n 4 6        2.36  -0.270  0.243 -0.928    1.50   -0.131 -0.344  0.455 -0.0915 \n 5 5       -1.62   0.451 -1.77   0.0437  -0.0670 -0.914  0.161  0.282 -0.00510\n 6 5       -1.58   0.569 -1.54   0.0237   0.110  -0.993  0.110  0.314  0.0343 \n 7 5       -1.10   0.608 -1.08  -0.344    1.13    0.175 -0.261 -0.240  0.0273 \n 8 7       -2.25  -0.417 -0.987 -0.00120  0.780   0.286 -0.131 -0.119  0.614  \n 9 7       -1.09  -0.308 -1.52   0.00331  0.227  -0.512 -0.250 -0.439  0.399  \n10 5        0.655  1.66   1.21  -0.824   -1.72   -0.476 -0.230 -0.839 -1.27   \n# ℹ 1,589 more rows\n# ℹ 2 more variables: PC10 &lt;dbl&gt;, PC11 &lt;dbl&gt;\n\n\n\n\nPart 2\n\nset.seed(2013)\n\nsplitIndex &lt;- createDataPartition(prc$Quality, p = 0.7, list = FALSE)\ntrainData &lt;- prc[splitIndex, ]\ntestData &lt;- prc[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n# Train Random Forest\nrfModel &lt;- train(Quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\nnote: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\n\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$Quality)) +\n    scale_y_discrete(labels = levels(testData$Quality))\n}\n\n# Random Forest metrics\nrfConfMatrix &lt;- confusionMatrix(rfPred, testData$Quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy &lt;- rfConfMatrix$overall[\"Accuracy\"]\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n\n[1] \"Random Forest Accuracy: 0.580712788259958\"\n\nplotCM(rfConfMatrix, \"Random Forest\")\n\n\n\n\n\n\n\nrfConfMatrix$table\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   3   2   0   0\n         5   1   7 135  53  11   1\n         6   2   6  56 117  23   4\n         7   0   2  10  19  25   0\n         8   0   0   0   0   0   0\n\n# Variable Importance\nvarImp(rfModel)\n\nrf variable importance\n\n                               Overall\n`Fermentation Characteristics`     100\n`Wine Body`                          0\n\n\n\nset.seed(2013)\nsplitIndex &lt;- createDataPartition(prc2$Quality, p = 0.7, list = FALSE)\ntrainData &lt;- prc2[splitIndex, ]\ntestData &lt;- prc2[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n# Train Random Forest\nrfModel &lt;- train(Quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\n\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$Quality)) +\n    scale_y_discrete(labels = levels(testData$Quality))\n}\n\n# Random Forest metrics\nrfConfMatrix &lt;- confusionMatrix(rfPred, testData$Quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy &lt;- rfConfMatrix$overall[\"Accuracy\"]\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n\n[1] \"Random Forest Accuracy: 0.681341719077568\"\n\nplotCM(rfConfMatrix, \"Random Forest\")\n\n\n\n\n\n\n\nrfConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   1   0   0   0\n         5   3   9 161  47   4   0\n         6   0   6  39 134  25   3\n         7   0   0   3  10  30   2\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                               \n               Accuracy : 0.6813               \n                 95% CI : (0.6374, 0.723)      \n    No Information Rate : 0.4277               \n    P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022\n                                               \n                  Kappa : 0.4807               \n                                               \n Mcnemar's Test P-Value : NA                   \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000 0.000000   0.7892   0.7016  0.50847  0.00000\nSpecificity          1.000000 0.997835   0.7692   0.7448  0.96411  1.00000\nPos Pred Value            NaN 0.000000   0.7188   0.6473  0.66667      NaN\nNeg Pred Value       0.993711 0.968487   0.8300   0.7889  0.93287  0.98952\nPrevalence           0.006289 0.031447   0.4277   0.4004  0.12369  0.01048\nDetection Rate       0.000000 0.000000   0.3375   0.2809  0.06289  0.00000\nDetection Prevalence 0.000000 0.002096   0.4696   0.4340  0.09434  0.00000\nBalanced Accuracy    0.500000 0.498918   0.7792   0.7232  0.73629  0.50000\n\n# Variable Importance\nvarImp(rfModel)\n\nrf variable importance\n\n     Overall\nPC2  100.000\nPC3   80.491\nPC5   32.468\nPC9   30.188\nPC1   16.023\nPC4    7.563\nPC7    6.410\nPC8    5.631\nPC11   3.630\nPC10   3.489\nPC6    0.000"
  },
  {
    "objectID": "Content/Projects/Statistical Analysis Wines/index.html#repeat-for-grouped-quality-setting-low-medium-high",
    "href": "Content/Projects/Statistical Analysis Wines/index.html#repeat-for-grouped-quality-setting-low-medium-high",
    "title": "Applied Multivariate Analysis",
    "section": "Repeat for Grouped Quality Setting (Low, Medium, High)",
    "text": "Repeat for Grouped Quality Setting (Low, Medium, High)\n\nSetting up data for classification\n\ngroupedQualityDs &lt;- redWineDs %&gt;%\n  select(-quality)\n\n\n\nTrain-Test-Split & Cross-Validation Set up\n\n# Split the dataset into training and testing sets\nsplitIndex &lt;- createDataPartition(groupedQualityDs$qualityGroup, p = 0.7, list = FALSE)\ntrainData &lt;- groupedQualityDs[splitIndex, ]\ntestData &lt;- groupedQualityDs[-splitIndex, ]\n\n# Create a train control object for repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n\n\nRandom Forest and SVM models\n\nset.seed(2013)\n# Train Random Forest classifier using caret\nrfModel &lt;- train(qualityGroup ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Train Support Vector Machine (SVM) model using caret\nsvmModel &lt;- train(qualityGroup ~ ., \n                  data = trainData,\n                  method = \"svmRadial\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\nsvmPred &lt;- predict(svmModel, newdata = testData)\n\n\n\nMetrics\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$qualityGroup)) +\n    scale_y_discrete(labels = levels(testData$qualityGroup))\n}\n\n# Random Forest metrics\nrfConfMatrix &lt;- confusionMatrix(rfPred, testData$qualityGroup, \n                                dnn = c(\"Prediction\", \"Reference\"))\nsvmConfMatrix &lt;- confusionMatrix(svmPred, testData$qualityGroup, \n                                 dnn = c(\"Prediction\", \"Reference\"))\n\nrfConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low      0      0    0\n    Medium  18    388   39\n    High     0      7   26\n\nOverall Statistics\n                                          \n               Accuracy : 0.8661          \n                 95% CI : (0.8323, 0.8953)\n    No Information Rate : 0.8264          \n    P-Value [Acc &gt; NIR] : 0.01092         \n                                          \n                  Kappa : 0.395           \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity             0.00000        0.9823     0.40000\nSpecificity             1.00000        0.3133     0.98305\nPos Pred Value              NaN        0.8719     0.78788\nNeg Pred Value          0.96234        0.7879     0.91236\nPrevalence              0.03766        0.8264     0.13598\nDetection Rate          0.00000        0.8117     0.05439\nDetection Prevalence    0.00000        0.9310     0.06904\nBalanced Accuracy       0.50000        0.6478     0.69153\n\nsvmConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low      0      0    0\n    Medium  18    393   50\n    High     0      2   15\n\nOverall Statistics\n                                         \n               Accuracy : 0.8536         \n                 95% CI : (0.8186, 0.884)\n    No Information Rate : 0.8264         \n    P-Value [Acc &gt; NIR] : 0.06328        \n                                         \n                  Kappa : 0.2611         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity             0.00000        0.9949     0.23077\nSpecificity             1.00000        0.1807     0.99516\nPos Pred Value              NaN        0.8525     0.88235\nNeg Pred Value          0.96234        0.8824     0.89154\nPrevalence              0.03766        0.8264     0.13598\nDetection Rate          0.00000        0.8222     0.03138\nDetection Prevalence    0.00000        0.9644     0.03556\nBalanced Accuracy       0.50000        0.5878     0.61296\n\nplotCM(rfConfMatrix, \"Random Forest\")\n\n\n\n\n\n\n\nplotCM(svmConfMatrix, \"SVM\")\n\n\n\n\n\n\n\n\n\n\nLook at variable importance\n\nplot(varImp(rfModel))"
  },
  {
    "objectID": "Content/Projects/PredictPinotWine_ML/index.html#purpose",
    "href": "Content/Projects/PredictPinotWine_ML/index.html#purpose",
    "title": "Predicting Wine Province Origin",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of this project was to develop a predictive model for identifying the province of origin for wines based on descriptions provided by critics. To achieve this goal, a random forest model was built and evaluated for its performance, achieving a kappa score of 0.82. This project aimed to provide a useful tool for wine connoisseurs and industry professionals in identifying the origin of wines based on their sensory characteristics."
  },
  {
    "objectID": "Content/Projects/PredictPinotWine_ML/index.html#setup",
    "href": "Content/Projects/PredictPinotWine_ML/index.html#setup",
    "title": "Predicting Wine Province Origin",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)"
  },
  {
    "objectID": "Content/Projects/PredictPinotWine_ML/index.html#feature-engineering",
    "href": "Content/Projects/PredictPinotWine_ML/index.html#feature-engineering",
    "title": "Predicting Wine Province Origin",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nwine = read_rds(\"../../../Assets/Datasets/pinot.rds\") \n\nwine_words &lt;- function(df, j, stem = T){ \n  data(stop_words)\n  words &lt;- df %&gt;%\n    unnest_tokens(word, description) %&gt;%\n    anti_join(stop_words) %&gt;%\n    filter(str_detect(string = word, pattern = \"[a-z+]\")) %&gt;% # get rid weird non alphas \n    filter(str_length(word) &gt;= 3) %&gt;% # get rid of strings shorter than 3 characters \n    filter(!(word %in% c(\"wine\",\"pinot\", \"vineyard\"))) %&gt;%\n    group_by(word) %&gt;%\n    mutate(total=n()) %&gt;%\n    ungroup()\n  \n  if(stem){\n    words &lt;- words %&gt;% \n      mutate(word = wordStem(word))\n  }\n  \n  words &lt;- words %&gt;% \n    count(id, word) %&gt;% \n    group_by(id) %&gt;% \n    mutate(exists = (n&gt;0)) %&gt;% \n    ungroup %&gt;% \n    group_by(word) %&gt;% \n    mutate(total = sum(n)) %&gt;% \n    filter(total &gt; j) %&gt;% \n    pivot_wider(id_cols = id,\n                names_from = word,\n                values_from = exists,\n                values_fill = list(exists=0)) %&gt;% \n    right_join(select(df,id,province)) %&gt;% \n    select(-id) %&gt;% \n    mutate(across(-province, ~replace_na(.x, F)))\n}\n\nwino &lt;- wine_words(wine, j = 190, stem = T)\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(id)`"
  },
  {
    "objectID": "Content/Projects/PredictPinotWine_ML/index.html#specification",
    "href": "Content/Projects/PredictPinotWine_ML/index.html#specification",
    "title": "Predicting Wine Province Origin",
    "section": "Specification",
    "text": "Specification\n\nset.seed(504) \n\nctrl &lt;- trainControl(method = \"cv\", number = 3)\n\n\nwine_index &lt;- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain &lt;- wino[ wine_index, ]\ntest &lt;- wino[-wine_index, ]\n\nfit &lt;- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 100,\n             tuneLength = 15,\n             nodesize = 10,\n             verbose = TRUE,\n             trControl = ctrl,\n             metric = \"Kappa\")"
  },
  {
    "objectID": "Content/Projects/PredictPinotWine_ML/index.html#model-performance",
    "href": "Content/Projects/PredictPinotWine_ML/index.html#model-performance",
    "title": "Predicting Wine Province Origin",
    "section": "Model Performance",
    "text": "Model Performance\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               216          5                 0           0        1\n  California              13        758                14          23       19\n  Casablanca_Valley        0          0                10           0        0\n  Marlborough              0          0                 0          12        0\n  New_York                 0          0                 0           0        0\n  Oregon                   9         28                 2          10        6\n                   Reference\nPrediction          Oregon\n  Burgundy              10\n  California            62\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               475\n\nOverall Statistics\n                                          \n               Accuracy : 0.8793          \n                 95% CI : (0.8627, 0.8945)\n    No Information Rate : 0.4728          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8069          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9076            0.9583                 0.384615\nSpecificity                   0.9889            0.8515                 1.000000\nPos Pred Value                0.9310            0.8526                 1.000000\nNeg Pred Value                0.9847            0.9579                 0.990379\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1291            0.4531                 0.005977\nDetection Prevalence          0.1387            0.5314                 0.005977\nBalanced Accuracy             0.9482            0.9049                 0.692308\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.266667         0.00000        0.8684\nSpecificity                    1.000000         1.00000        0.9512\nPos Pred Value                 1.000000             NaN        0.8962\nNeg Pred Value                 0.980132         0.98446        0.9370\nPrevalence                     0.026898         0.01554        0.3270\nDetection Rate                 0.007173         0.00000        0.2839\nDetection Prevalence           0.007173         0.00000        0.3168\nBalanced Accuracy              0.633333         0.50000        0.9098"
  },
  {
    "objectID": "Content/Projects/PredictPinotWine_ML/index.html#re-fit-and-evaluation",
    "href": "Content/Projects/PredictPinotWine_ML/index.html#re-fit-and-evaluation",
    "title": "Predicting Wine Province Origin",
    "section": "Re-fit and evaluation",
    "text": "Re-fit and evaluation\n\nset.seed(1504)\n\nwine_index &lt;- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain &lt;- wino[ wine_index, ]\ntest &lt;- wino[-wine_index, ]\n\n# example spec for knn\nfit_final &lt;- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             tuneGrid = fit$bestTune) \n# The last line means we will fit a model using the best tune parameters your CV found above."
  },
  {
    "objectID": "Content/Projects/PredictPinotWine_ML/index.html#final-model-performance",
    "href": "Content/Projects/PredictPinotWine_ML/index.html#final-model-performance",
    "title": "Predicting Wine Province Origin",
    "section": "Final Model Performance",
    "text": "Final Model Performance\n\nconfusionMatrix(predict(fit_final, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               219          7                 0           0        0\n  California               7        752                12          13       20\n  Casablanca_Valley        0          0                12           0        0\n  Marlborough              0          1                 0          22        0\n  New_York                 0          0                 0           0        1\n  Oregon                  12         31                 2          10        5\n                   Reference\nPrediction          Oregon\n  Burgundy               7\n  California            55\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               485\n\nOverall Statistics\n                                          \n               Accuracy : 0.8912          \n                 95% CI : (0.8753, 0.9057)\n    No Information Rate : 0.4728          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8274          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9202            0.9507                 0.461538\nSpecificity                   0.9902            0.8787                 1.000000\nPos Pred Value                0.9399            0.8754                 1.000000\nNeg Pred Value                0.9868            0.9521                 0.991571\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1309            0.4495                 0.007173\nDetection Prevalence          0.1393            0.5134                 0.007173\nBalanced Accuracy             0.9552            0.9147                 0.730769\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                     0.48889       0.0384615        0.8867\nSpecificity                     0.99939       1.0000000        0.9467\nPos Pred Value                  0.95652       1.0000000        0.8899\nNeg Pred Value                  0.98606       0.9850478        0.9450\nPrevalence                      0.02690       0.0155409        0.3270\nDetection Rate                  0.01315       0.0005977        0.2899\nDetection Prevalence            0.01375       0.0005977        0.3258\nBalanced Accuracy               0.74414       0.5192308        0.9167"
  },
  {
    "objectID": "Content/Projects/PredictPinotWine_ML/index.html#conclusion",
    "href": "Content/Projects/PredictPinotWine_ML/index.html#conclusion",
    "title": "Predicting Wine Province Origin",
    "section": "Conclusion",
    "text": "Conclusion\nThe kappa value of 0.82 for our random forest model signifies a high level of precision and accuracy, reflecting a very good agreement with the actual outcomes. This statistical measure, important for assessing classification model performance, confirms the model’s efficacy in predicting the correct class labels. Thus, with a kappa value of 0.82, the model demonstrates reliable predictive performance."
  },
  {
    "objectID": "Content/Projects/UKAccidentShinyApp/index.html",
    "href": "Content/Projects/UKAccidentShinyApp/index.html",
    "title": "UK Accident Time Series",
    "section": "",
    "text": "This accident time series app allows you to explore and analyze data related accidents in UK. Feel free to download the data and explore other methods of analysis.\nHere is the link to the full page: UKAccidentTimeSeries"
  }
]