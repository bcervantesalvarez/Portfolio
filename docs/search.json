[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Apple’s Stock Journey:\n\n\n\n\n\n\n\nR\n\n\nfinance\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Customer Returns\n\n\n\n\n\n\n\nR\n\n\nmachine learning\n\n\nlogistical model\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nPredicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions\n\n\n\n\n\n\n\nR\n\n\nmachine learning\n\n\nrandom forest\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nInvestigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach\n\n\n\n\n\n\n\nreveal.js\n\n\nR\n\n\npresentation\n\n\ninformative\n\n\nsurvival analysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nBrian Cervantes Alvarez, Willa Van Liew, Hans Lehndorff\n\n\n\n\n\n\n  \n\n\n\n\nDo You Like Stretching? I Would Reconsider!\n\n\n\n\n\n\n\nR\n\n\ninformative\n\n\ninteractive\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nRating Pinot Wines: Is More Expensive Better?\n\n\n\n\n\n\n\nR\n\n\ninformative\n\n\ninteractive\n\n\ndatatables\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nHealthcare Spending Is Only Getting Worse\n\n\n\n\n\n\n\nR\n\n\nlinear regression\n\n\nstats\n\n\nanalysis\n\n\nhealthcare\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2022\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nPokédex Database\n\n\n\n\n\n\n\npostgreSQL\n\n\ndatabase\n\n\npresentation\n\n\nR\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nResources for Prospective College Students\n\n\n\n\n\n\n\nR\n\n\ninteractive\n\n\ninformative\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\nBrian Cervantes Alvarez, Corey Cassell\n\n\n\n\n\n\n  \n\n\n\n\nU.S. Medical Insurance Costs\n\n\n\n\n\n\n\npython\n\n\nhealthcare\n\n\nanalysis\n\n\nstats\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2022\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Premium Default (Insurance)\n\n\n\n\n\n\n\nR\n\n\ninformative\n\n\ninteractive\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brian",
    "section": "",
    "text": "Who am I?\nI am a next generation Data Scientist who is eager to learn and apply my knowledge to real-world problems. I have completed relevant coursework and gained proficiency in programming languages such as Python, R, and SQL. Additionally, I have experience working with data visualization tools and statistical analysis techniques. I am committed to continuous learning and staying up-to-date with the latest developments in the field of data science. I am excited to contribute my skills and learn from experienced professionals in the industry.\n\n\nWhere do I come from?\nI was born in Pasco, Washington, a small city in the southeastern part of the state. Pasco is part of the Tri-Cities area, along with Richland and Kennewick, and is known for its agricultural industry, particularly the cultivation of potatoes, apples, and wine grapes. I enjoyed the rural atmosphere of Pasco, with its open fields, orchards, and vineyards.\n\n\n\nPasco, Washington\n\n\nBefore finishing 1st grade, I moved to Salem, Oregon, a mid-sized city in the heart of the Willamette Valley. Salem is known for its farms and vineyards that produce some of the best wine and food in the state. Growing up in Salem, I was immersed in the outdoors and enjoyed activities such as hiking, camping, and running. Salem also has a rich history, with landmarks like the Oregon State Capitol and Willamette University, which is the oldest university in the west. However, as I got older, I also became aware of some of the social and economic challenges faced by many families in the area, such as poverty, limited job opportunities, and inadequate access to healthcare. My family and I suffered poverty and food instability before I graduated high school. Despite these challenges, I feel a strong connection to Salem and its people, and I am grateful for the experiences and values that I gained from growing up in this community.\n\n\n\nSalem, Oregon"
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#abstract",
    "href": "posts/US_HealthCare_Spending/index.html#abstract",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "ABSTRACT",
    "text": "ABSTRACT\nU.S. Healthcare costs are increasing in every category since 1980. There are an unfiltered number of reasons for the rise of these costs. From population increase to doctor’s wages, they are but a few examples of the unforgiving healthcare spending increases that are observed in a yearly basis. This report examines the expenditure trends from 1980 to 2005 and up to 2014. The results show that each sector in the United States is experiencing an unprecedented increase in health care costs. Hence, the findings revealed in this report serve to provide context behind the United States’ status as one the most expensive health care nations in the world."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#introduction",
    "href": "posts/US_HealthCare_Spending/index.html#introduction",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "INTRODUCTION",
    "text": "INTRODUCTION\nHealthcare is a vital component to our lives. Without it, we cannot get the necessary help to live happy lives. Or worse, we could not live long enough to enjoy it. However, Healthcare spending continues to skyrocket in a year to year basis. This report reveals the frightening truth about the healthcare spending and visualizes each component. From the overall spending nationally, to showing each category and their individual trends, the spending remains increasingly constant."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#background",
    "href": "posts/US_HealthCare_Spending/index.html#background",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "BACKGROUND",
    "text": "BACKGROUND\nNow, the data set that was used for this report is called US Healthcare Spending Per Capita (source: Kaggle). The format of this data set was quite tricky. Instead of being in the long format, with few columns and lengthy rows, it was in the wide format, lots of columns and very few rows. Specifically, the years were in the format, “Y####” which made it very difficult to do any sort of analysis at first. Though, with some pivoting and string manipulation, the updated data set was ready to do some quirky analysis. The full step process is coming up next."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#methodology",
    "href": "posts/US_HealthCare_Spending/index.html#methodology",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "METHODOLOGY",
    "text": "METHODOLOGY\nBefore jumping into the data, first start with seeing whether it is “wide” or “long”. Check out the number of rows and columns to allow some room to do some data wrangling.\n\n\n# A tibble: 5 × 42\n   Code Item    Group Regio…¹ Regio…² State…³  Y1980  Y1981  Y1982  Y1983  Y1984\n  <dbl> <chr>   <chr>   <dbl> <chr>   <chr>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1     1 Person… Unit…       0 United… <NA>    216977 251789 283073 311677 341645\n2     1 Person… Regi…       1 New En… <NA>     12960  14845  16759  18429  20253\n3     1 Person… Regi…       2 Mideast <NA>     43479  49604  55406  61165  68154\n4     1 Person… Regi…       3 Great … <NA>     40658  46668  51440  55967  60776\n5     1 Person… Regi…       4 Plains  <NA>     16980  19682  21919  23940  25684\n# … with 31 more variables: Y1985 <dbl>, Y1986 <dbl>, Y1987 <dbl>, Y1988 <dbl>,\n#   Y1989 <dbl>, Y1990 <dbl>, Y1991 <dbl>, Y1992 <dbl>, Y1993 <dbl>,\n#   Y1994 <dbl>, Y1995 <dbl>, Y1996 <dbl>, Y1997 <dbl>, Y1998 <dbl>,\n#   Y1999 <dbl>, Y2000 <dbl>, Y2001 <dbl>, Y2002 <dbl>, Y2003 <dbl>,\n#   Y2004 <dbl>, Y2005 <dbl>, Y2006 <dbl>, Y2007 <dbl>, Y2008 <dbl>,\n#   Y2009 <dbl>, Y2010 <dbl>, Y2011 <dbl>, Y2012 <dbl>, Y2013 <dbl>,\n#   Y2014 <dbl>, Average_Annual_Percent_Growth <dbl>, and abbreviated …\n\n\n [1] \"Code\"                          \"Item\"                         \n [3] \"Group\"                         \"Region_Number\"                \n [5] \"Region_Name\"                   \"State_Name\"                   \n [7] \"Y1980\"                         \"Y1981\"                        \n [9] \"Y1982\"                         \"Y1983\"                        \n[11] \"Y1984\"                         \"Y1985\"                        \n[13] \"Y1986\"                         \"Y1987\"                        \n[15] \"Y1988\"                         \"Y1989\"                        \n[17] \"Y1990\"                         \"Y1991\"                        \n[19] \"Y1992\"                         \"Y1993\"                        \n[21] \"Y1994\"                         \"Y1995\"                        \n[23] \"Y1996\"                         \"Y1997\"                        \n[25] \"Y1998\"                         \"Y1999\"                        \n[27] \"Y2000\"                         \"Y2001\"                        \n[29] \"Y2002\"                         \"Y2003\"                        \n[31] \"Y2004\"                         \"Y2005\"                        \n[33] \"Y2006\"                         \"Y2007\"                        \n[35] \"Y2008\"                         \"Y2009\"                        \n[37] \"Y2010\"                         \"Y2011\"                        \n[39] \"Y2012\"                         \"Y2013\"                        \n[41] \"Y2014\"                         \"Average_Annual_Percent_Growth\"\n\n\nAs explained earlier, the data set was evidently in the wide format. So, using the “pivot_longer” function with the “tidyverse” package, the years columns were placed into one column called “Year” with their values in the another column called “Cost”. I used the “gsub” function to fix up my Year column and factored a few columns for later use.\n\n\n# A tibble: 6 × 5\n  Item                 Region_Name   State_Name  Year   Cost\n  <fct>                <fct>         <fct>      <dbl>  <dbl>\n1 Personal Health Care United States <NA>        1980 216977\n2 Personal Health Care United States <NA>        1981 251789\n3 Personal Health Care United States <NA>        1982 283073\n4 Personal Health Care United States <NA>        1983 311677\n5 Personal Health Care United States <NA>        1984 341645\n6 Personal Health Care United States <NA>        1985 376376\n\n\n\nHealth Care Costs Have Increased\nDiving straight into the first visual. It appears very clearly that Healthcare spending is in an upward trend and does not appear to slow down. The graph was filter from the years 1980 to 2005 to highlight this era of healthcare costs.\n\n\n\n\n\n\n\nPersonal, Hospital and Physician & Clinical Care Are The Dominate Spending Categories\nYikes. Personal health care went from about $10K to nearly $80K in that short time span. Hospital and Clinical Care are heavy hitters for spending. Though this graph shows the same trend for each category. Not very pleasant information.\n\n\n\n\n\n\n\nEach Region Is Trending The Same\nTerrible news, each region has not stopped increasing spending. Their trend lines all appear to be similar and proportionally alike to one and another. The highlights are that the Mideast is the most expensive and the Rocky Mountains is the cheapest. Note, this was looking at the spending up to 2005. Maybe things will change by 2014.\n\n\n\n\n\n\n\nThe Trends Remain. Healthcare Spending Continues To Rise In Every Region In The U.S.\nIn the bar chart, you can distinguish more clearly each region and their overall spending. What stands out the most is that the Plains, New England and the Rocky Mountains are one of the lowest in terms of medical funding. In some cases, they are 1/3 the cost of the most expensive regions.\n\n\n\n\n\n\n\nThe Far West Ranks 3rd In Healthcare Spending\nFor 2014, that is quite the jump. It can be assumed that all those states are heavy hitters in terms of spending, but that is proven quite wrong in the next graphic.\n\n\n\n\n\n[1] \"$21.81M\"\n[1] \"$21.81M\"\n\n\n\n\nOregon Ranks 3rd, But Don’t Be Fooled!\nLook, California takes the #1 spot in spending. You cannot deny the underlying reasons behind that, such as their ridicously population size. While this report did not look into it, you would probably push the Far West region closer to the Plains or New England in terms of spending.\n\n\n\n\n\n[1] \"$1.41M\"  \"$53.64M\" \"$1.9M\"   \"$3.41M\"  \"$5.81M\"  \"$10.16M\"\n[1] \"$1.41M\"  \"$53.64M\" \"$1.9M\"   \"$3.41M\"  \"$5.81M\"  \"$10.16M\"\n[1] \"$5.81M\"\n[1] \"$5.81M\"\n\n\n\n\nA Linear Fit Is Not The Right Model For This Data\nRight off the bat, the model appears to have an adjusted R-Squared value of 0.8572. That looks wondeful, right? No! This model is not accurate at all and is highly discouraged. It becomes clear that their is no correlation between Cost and Region_Name per Year. While not shown, this was true for the filtered data set that went up from 1980 to 2014. The residual plots speak for themselves. The Residuals vs Fitted was showing clear signs of being a quadratic fit instead of a linear one. The Q-Q was short of linear but had a number of curves along the fitted line. The scale location evidently showed that this model was simple not the correct fit for this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      1 \n11072.4 \n\n\n\nCall:\nlm(formula = Cost ~ Region_Name + Year, data = regionHealthCareSince2005)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2683.8  -782.9  -279.8   597.7  4139.3 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -680650.41   24479.29 -27.805  < 2e-16 ***\nRegion_NameGreat Lakes        1438.65     368.55   3.904 0.000130 ***\nRegion_NameMideast            1657.50     368.55   4.497 1.17e-05 ***\nRegion_NameNew England       -3909.68     368.55 -10.608  < 2e-16 ***\nRegion_NamePlains            -3830.75     368.55 -10.394  < 2e-16 ***\nRegion_NameRocky Mountains   -5106.26     368.55 -13.855  < 2e-16 ***\nRegion_NameSoutheast         -1231.18     368.55  -3.341 0.000998 ***\nRegion_NameSouthwest          -849.93     368.55  -2.306 0.022133 *  \nYear                           344.83      12.29  28.069  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1329 on 199 degrees of freedom\nMultiple R-squared:   0.88, Adjusted R-squared:  0.8752 \nF-statistic: 182.4 on 8 and 199 DF,  p-value: < 2.2e-16\n\n\n             Df    Sum Sq   Mean Sq F value Pr(>F)    \nRegion_Name   7 1.185e+09 1.693e+08    95.9 <2e-16 ***\nYear          1 1.391e+09 1.391e+09   787.9 <2e-16 ***\nResiduals   199 3.514e+08 1.766e+06                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nWas there A Significant Difference In The Means Of Each Region?\nI wished to explore if it was statistically significant on where each region’s mean over the years mattered. I actually started off using a LeveneTest to see if the variance (the spread of the spending for each region) was important. Given both tests yielded an extremely small p-value, it is clear that there are 3 regions that have a completely different variance than the other regions. I was not done there, I now wanted to confirm this with the TukeyHsd test to see if their means differed as a result. Yes, their means differed as expected from the LeveneTest. To make clear, New England, Plains and Rocky Mountains spent much lower on average. Despite that fact, they are following the trend of growth with an 18% since 2005.\nI wished to explore if it was statistically significant on where each region’s mean over the years mattered. I actually started off using a LeveneTest to see if the variance (the spread of the spending for each region) was important. Given both tests yielded an extremely small p-value, it is clear that there are 3 regions that have a completely different variance than the other regions. I was not done there, I now wanted to confirm this with the TukeyHsd test to see if their means differed as a result. Yes, their means differed as expected from the LeveneTest. To make clear, New England, Plains and Rocky Mountains spent much lower on average. Despite that fact, they are following the trend of growth with an 18% since 2005.\n\n\n[1] \"The Average Spending In The Expensive Regions since 2005 = $5333.29\"\n\n\n[1] \"The Average Spending In The Expensive Regions since 2014= $7724.45\"\n\n\n[1] \"Difference: +$2391.16 | Percentage Increase: +18.31%\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2005 = $2173.22\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2014= $3142.21\"\n\n\n[1] \"Difference: +$968.99 | Percentage Increase: 18.23%\"\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(>F)    \ngroup   7   12.03 8.727e-13 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   7  11.462 3.292e-12 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(>F)    \ngroup   7  19.546 < 2.2e-16 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   7  12.828 3.075e-14 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n             Df    Sum Sq   Mean Sq F value Pr(>F)    \nRegion_Name   7 1.185e+09 169337440   19.43 <2e-16 ***\nResiduals   200 1.743e+09   8712933                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Cost ~ Region_Name, data = regionHealthCareSince2005)\n\n$Region_Name\n                                   diff         lwr           upr     p adj\nGreat Lakes-Far West         1438.64777 -1069.19906  3946.4945990 0.6495584\nMideast-Far West             1657.50100  -850.34583  4165.3478291 0.4679930\nNew England-Far West        -3909.68132 -6417.52815 -1401.8344885 0.0000930\nPlains-Far West             -3830.75287 -6338.59970 -1322.9060420 0.0001417\nRocky Mountains-Far West    -5106.25783 -7614.10466 -2598.4109954 0.0000001\nSoutheast-Far West          -1231.18113 -3739.02796  1276.6657036 0.8046614\nSouthwest-Far West           -849.92577 -3357.77260  1657.9210559 0.9679983\nMideast-Great Lakes           218.85323 -2288.99360  2726.7000602 0.9999950\nNew England-Great Lakes     -5348.32909 -7856.17592 -2840.4822574 0.0000000\nPlains-Great Lakes          -5269.40064 -7777.24747 -2761.5538109 0.0000000\nRocky Mountains-Great Lakes -6544.90559 -9052.75242 -4037.0587643 0.0000000\nSoutheast-Great Lakes       -2669.82890 -5177.67573  -161.9820653 0.0279871\nSouthwest-Great Lakes       -2288.57354 -4796.42037   219.2732870 0.1019616\nNew England-Mideast         -5567.18232 -8075.02915 -3059.3354875 0.0000000\nPlains-Mideast              -5488.25387 -7996.10070 -2980.4070410 0.0000000\nRocky Mountains-Mideast     -6763.75882 -9271.60565 -4255.9119944 0.0000000\nSoutheast-Mideast           -2888.68213 -5396.52896  -380.8352954 0.0119419\nSouthwest-Mideast           -2507.42677 -5015.27360     0.4200569 0.0500724\nPlains-New England             78.92845 -2428.91838  2586.7752767 1.0000000\nRocky Mountains-New England -1196.57651 -3704.42334  1311.2703233 0.8267302\nSoutheast-New England        2678.50019   170.65336  5186.3470223 0.0270977\nSouthwest-New England        3059.75554   551.90871  5567.6023746 0.0058329\nRocky Mountains-Plains      -1275.50495 -3783.35178  1232.3418768 0.7745369\nSoutheast-Plains             2599.57175    91.72492  5107.4185757 0.0361922\nSouthwest-Plains             2980.82710   472.98027  5488.6739280 0.0081616\nSoutheast-Rocky Mountains    3875.07670  1367.22987  6382.9235291 0.0001119\nSouthwest-Rocky Mountains    4256.33205  1748.48522  6764.1788814 0.0000135\nSouthwest-Southeast           381.25535 -2126.59148  2889.1021825 0.9997829"
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#results",
    "href": "posts/US_HealthCare_Spending/index.html#results",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "RESULTS",
    "text": "RESULTS\nU.S. Healthcare costs have more than quintupled from 1980 to 2014. There are trends throughout each region that show no clear indication of healthcare spending going down. While some regions are not as expensive as others, they are still growing at the same pace nationally. Following the trend, personal health care spending averaged approximately $10,000 in 1980, but has substantially increased to nearly $80,000 in 2014. The top 3 most expensive regions as of 2014 are the Mideast, Great Lakes, and Far West. The top 3 least cheapest regions as of 2014 are the Rocky Mountains, New England, and Plains. In the Far West region, Oregon ranks 3rd in being the most expensive State in the region."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#conclusion",
    "href": "posts/US_HealthCare_Spending/index.html#conclusion",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nThe United States continues to spend more in their healthcare system. Though, in some categories such as Personal Health Care, you could argue that it is becoming increasingly unaffordable. A $70K difference in 35 years is quite off from inflation expectations. Speaking of inflation, this dataset would have had great potential if inflation adjusted values were given. In that sense, a deeper analysis could be made and more insight would have been gained. Now, one could explore this with even more in depth see if there is any statistical significance between each individual state and their spending habits. That is left and open for anyone to undertake in the future."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#purpose",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#purpose",
    "title": "Resources for Prospective College Students",
    "section": "Purpose:",
    "text": "Purpose:\nHow can we provide a prospective college student with information regarding their potential income and student debt upon embarking on a career path? To address this issue, we have created an interactive tool. Although it is uncomplicated and subject to enhancement, it has enabled us to delve into interactive visualizations through the utilization of R Shiny."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#summary",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#summary",
    "title": "Resources for Prospective College Students",
    "section": "Summary:",
    "text": "Summary:\nThe interactive visualization is comprised of four distinct parts: salary estimator, tuition estimator, debt estimator, and debt calculator. The salary estimator allows prospective students to explore the potential salaries associated with different majors, thereby enabling them to make informed decisions about their future careers. The tuition estimator provides an estimate of the generalized tuition costs associated with pursuing a major in a particular state. The debt estimator calculates the potential four-year degree debt based on the student’s family income, utilizing an average of all students attending each university, and offers insight into the likely debt accumulation over the four years of study. The final tool analyzes the expected length of time a student will remain in debt, based on their chosen major category, and provides a visualization of the projected student debts. Overall, this interactive tool is an invaluable resource for high school students who are considering higher education, offering a comprehensive set of tools to help them make informed decisions about their future."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#shiny-interactive-tool",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#shiny-interactive-tool",
    "title": "Resources for Prospective College Students",
    "section": "Shiny Interactive Tool",
    "text": "Shiny Interactive Tool"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#setup",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#setup",
    "title": "Resources for Prospective College Students",
    "section": "Setup",
    "text": "Setup\n\n\nSetup + Wrangling\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(rsconnect)\nlibrary(colorspace)\nlibrary(plotly)\nlibrary(shinyWidgets)\nlibrary(scales)\nlibrary(ggplot2)\n\n#read the files in from github\nallAgesDf <- read_csv(\"all-ages.csv\")\ntuition_cost <- read_csv(\"tuition_income.csv\")\ntuition <- read_csv(\"tuition_cost.csv\")\nds4<-read_csv(\"salary_and_stats.csv\")\n\n#Wrangling Salary Potential\nsalary <- allAgesDf %>% \n  dplyr::select(Major, P25th, Median, P75th) %>% \n  pivot_longer(c(P25th, Median, P75th),\n               names_to = \"Percentile_Range\", values_to = \"Salary\") %>%\n  arrange(Major) %>%\n  mutate(Percentile_Range = as.factor(Percentile_Range),\n         Major = as.factor(Major))\n\n#Wrangling Potential Tuition Burden\n\n\ntuition_cost <- tuition_cost %>% \n  filter(year == 2018 & net_cost > 0) %>%\n  arrange(name) %>%\n  mutate(income_lvl = as.factor(income_lvl),\n         name = as.factor(name))\n  \n\ntuition_cost$income_lvl <- recode(tuition_cost$income_lvl, \n                                  \"0 to 30,000\" = \"$0 to $30,000\",\n                                  \"30,001 to 48,000\" = \"$30,001 to $48,000\",\n                                  \"48_001 to 75,000\" = \"$48,001 to $75,000\",\n                                  \"75,001 to 110,000\" = \"$75,001 to $110,000\",\n                                  \"Over 110,000\" = \"Over $110,000\")\nsalary$Percentile_Range <- factor(salary$Percentile_Range, levels = c(\"P25th\", \"Median\", \"P75th\"))\nsalary$Percentile_Range <- recode(salary$Percentile_Range, \n                                  \"P25th\" = \"Early Career\",\n                                  \"Median\" = \"Middle Career\",\n                                  \"P75th\" = \"Late Career\")\nsalary$Major <- str_to_title(salary$Major)\nsalary$Major <- gsub(\"And\", \"and\", salary$Major)\n\n\ndf <- tuition %>% \n  group_by(state, degree_length, type) %>% filter(!is.na(state) & degree_length != \"Other\") %>%\n  summarise(room_expenses = mean(room_and_board, na.rm = TRUE),\n            inStateTotal = mean(in_state_total, na.rm = TRUE),\n            outOfStateTotal = mean(out_of_state_total, na.rm = TRUE))\n\ndf$degree_length <- as.factor(df$degree_length)\ndf$type <- as.factor(df$type)\n\ndf <- df %>% rename(\"Room and Board\" = room_expenses,\n              \"In State Tuition\" = inStateTotal,\n              \"Out of State Tuition\" = outOfStateTotal)\n\n\n\n\nColor Theme\n#vars  \n  title = 25\n  subtitle = 20\n  facet_title = 25\n  axis_title = 18\n  tick_numbers = 13\n  title_color = \"black\"\n  background = \"gainsboro\"\n  plot_background = \"gainsboro\"\n  facet_header_background = \"gainsboro\"\n  line_type = \"solid\"\n\nCoreyPlotTheme <- theme(\n    text = element_text(family = \"Futura\"),\n    #background color of page\n    plot.background = element_rect(fill = background),\n    \n    #graph background and grid\n    panel.background = element_blank(),\n    panel.grid.major = element_line(size = .1, linetype = line_type, colour = \"gainsboro\"), \n    panel.grid.minor = element_line(size = .1, linetype = line_type, colour = \"black\"),\n    \n    #title/font/labels\n    plot.title = element_text(color = title_color, size = title,family = \"Futura\",hjust = 0.5),\n    plot.subtitle = element_text(color = title_color, size = subtitle,family = \"Futura\", hjust = 0.5),\n    #plot.caption = element_textbox_simple(halign = 0, size = tick_numbers, maxwidth = 30,family = \"Futura\"),\n    plot.caption = element_text(color = title_color, face = \"bold\", size = tick_numbers, family = \"Futura\", hjust=0),\n    strip.text = element_text(color = title_color,size = facet_title, family = \"Futura\"),\n    strip.background = element_rect(fill = facet_header_background),\n    \n    #tick marks\n    axis.text = element_text(color = title_color, size = tick_numbers, family = \"Futura\"),\n    axis.title = element_text(color = title_color, size = axis_title, family = \"Futura\"),\n    axis.ticks.x = element_blank(),\n    \n    #legend\n    legend.title = element_text(color = title_color,size =subtitle, family = \"Futura\"),\n    legend.background = element_rect(fill = plot_background),\n    legend.text = element_text(size = tick_numbers, family =\"Futura\" )\n  )"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#plot-sidebar-inputs",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#plot-sidebar-inputs",
    "title": "Resources for Prospective College Students",
    "section": "Plot Sidebar Inputs",
    "text": "Plot Sidebar Inputs\n\n\nSalary Estimator Selectors\n#INPUT FOR PLOT 1\n\ninput1 <- inputPanel(\n  selectInput(\"selectInput1\", label = \"Choose your major:\", \n              choices = unique(salary$Major),\n              selected = \"ART HISTORY AND CRITICISM\"),\n  checkboxGroupInput(\"percentile_choice\", label = \"Pick your career level:\", \n                     choices = list(\"Early Career \" = \"Early Career\",\n                                    \"Middle Career \" = \"Middle Career\",\n                                    \"Late Career \" = \"Late Career\"),\n                     selected = c(\"Early Career\", \"Middle Career\", \"Late Career\")),\n)\n\n\n\n\nTuition Estimator Options\n#INPUT FOR PLOT 2\n\ninput2 <- inputPanel(\n  selectInput(\"money\", label = \"Select the type of expense:\",\n              choices = c(\"Room and Board\" = \"Room and Board\",\n                          \"In State Tuition\" = \"In State Tuition\",\n                          \"Out of State Tuition\" = \"Out of State Tuition\"),\n              selected = \"In State Tuition\"),\n  selectInput(\"state\", label = \"Pick your State:\", \n              choices = unique(df$state),\n              selected = \"Oregon\"),\n)\n\n\n\n\nDebt Estimator Levels\n#INPUT FOR PLOT 3\n\ninput3 <- inputPanel(\n  selectInput(\"selectInput2\", \n              label = \"Select your university:\",\n              choices = unique(tuition_cost$name), \n              selected = \"Willamette University\"),\n  checkboxGroupInput(\"checkGroup\", \n                     label = \"Select your household income bracket:\", \n                     choices = list(\"$0 to $30,000\" = \"$0 to $30,000\",\n                                    \"$30,001 to $48,000\" = \"$30,001 to $48,000\",\n                                    \"$48,001 to $75,000\" = \"$48,001 to $75,000\",\n                                    \"$75,001 to $110,000\" = \"$75,001 to $110,000\",\n                                    \"Over $110,000\" = \"Over $110,000\"),\n                               selected = c(\"$0 to $30,000\",\n                                            \"$30,001 to $48,000\",\n                                            \"$48,001 to $75,000\",\n                                            \"$75,001 to $110,000\",\n                                            \"Over $110,000\")),\n)\n\n\n\n\nDebt Calculator Choices\n#INPUT FOR PLOT 4\n\ninput4 <- inputPanel(\n  selectInput(\"major_category\", \n              label = \"Pick a major category:\", \n              choices = unique(ds4$major_category),\n              selected = \"Computers & Mathematics\"),\n)"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#plots",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#plots",
    "title": "Resources for Prospective College Students",
    "section": "Plots",
    "text": "Plots\n\n\nSalary Estimator\n#PLOT1\nplot1 <- renderPlot({\n  salary %>% \n    filter((Major %in% input$selectInput1) & (Percentile_Range %in% input$percentile_choice)) %>% \n    ggplot(aes(x = Percentile_Range, y = Salary, fill = Percentile_Range)) +\n      geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n      geom_label(aes(y = Salary,\n                     label = print(paste0(\"$\", round(Salary/1000, 2), \"K\"))),\n                 show.legend = FALSE,\n                 size = 7,\n                 family = \"Futura\",\n                 fill = \"white\") +\n      scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n      labs(x = NULL,\n           y = NULL,\n           title = paste0(\"Estimated Salary for \", input$selectInput1),\n           caption = \"Source: TuitionTracker.org @ 2018\") + \n      CoreyPlotTheme +\n      scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nTuition Estimator\n#PLOT2\nplot2 <- renderPlot({\n  df %>% filter(state == input$state) %>%\n      ggplot(aes(x = degree_length, y = .data[[input$money]], fill = degree_length)) +\n      geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n      facet_wrap(~type) + \n      geom_label(aes(y = .data[[input$money]],\n                     label = print(paste0(\"$\", round(.data[[input$money]]/1000, 2), \"K\"))),\n                 family = \"Oswald\",\n                 size = 7,\n                 show.legend = FALSE,\n                 fill = \"white\") +\n      scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3),\n                         limits = c(0,55000)) +\n      labs(x = NULL,\n           y = NULL,\n           title = paste0(\"Average \", input$money, \" for \", input$state, \" Universities\"),\n           subtitle = \"For Undergraduate Degrees\",\n           caption = \"Source: TuitionTracker.org @ 2018\") + \n      CoreyPlotTheme +\n      scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nDebt Estimator\n#PLOT3\nplot3 <- renderPlot({\n  tuition_cost %>% \n      filter((income_lvl %in% input$checkGroup) & (name %in% input$selectInput2)) %>%\n      ggplot(aes(x = income_lvl, y = net_cost, fill = income_lvl)) +\n      geom_col(color = \"black\", width = 0.4, position = \"dodge\", show.legend = FALSE) +\n      geom_label(aes(y = net_cost,\n                     label = print(paste0(\"$\", round(net_cost/1000, 2), \"K\"))),\n                 family = \"Oswald\",\n                 size = 7,\n                 show.legend = FALSE,\n                 fill = \"white\") +\n      scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n      labs(x = NULL,\n           y = NULL,\n           title = paste0(\"Median Student Loan Debt for \", input$selectInput2),\n           subtitle = \"After Completing Their Undergraduate Degree\",\n           caption = \"Source: TuitionTracker.org @ 2018\") +\n      CoreyPlotTheme + \n      scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nDebt Calculator\n#PlOT4\nplot4 <- renderPlot({\n  ds4 %>% \n      filter(major_category == input$major_category) %>% \n      ggplot(aes(perfect_payback_period,reorder(major, perfect_payback_period), fill = perfect_payback_period))+\n      geom_col(show.legend = FALSE) +\n      geom_label(aes(label=paste(round(perfect_payback_period,2),\" yrs.\")), \n                 show.legend = FALSE, \n                 fill = \"white\", \n                 hjust = 1.1) +\n      theme(axis.title.y = element_blank(),\n            axis.text.x = element_blank()) +\n      labs(title = 'How Long Will You Be In Debt?',\n           subtitle = \"Based on Your Major\",\n           x = 'Time to pay off loans')+\n      CoreyPlotTheme +\n      theme(plot.title = element_text(hjust = 0.5)) +\n      scale_fill_continuous_sequential(\"PuBuGn\")\n})"
  },
  {
    "objectID": "posts/FancyTables/index.html#purpose",
    "href": "posts/FancyTables/index.html#purpose",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Purpose",
    "text": "Purpose\nThis project centers on the exploration of various techniques to design visually appealing and comprehensible data tables. It is a well-known fact that the conventional format of Excel spreadsheets can be tedious and challenging to read. As such, the project aims to investigate alternative ways of presenting data in tables that are both aesthetically pleasing and easy to understand.\nBy utilizing various design principles such as color theory, typography, and layout, we seek to create data tables that are visually striking and convey information effectively. Additionally, the project involves the evaluation of different software tools and platforms that offer innovative and user-friendly options for creating data tables.\nIt is crucial to recognize that the presentation of data plays a significant role in its interpretation and understanding. The traditional Excel format may not provide sufficient visual cues to highlight key data points or insights. Therefore, this project seeks to address this limitation by exploring new and innovative methods of presenting data tables that are both functional and visually appealing."
  },
  {
    "objectID": "posts/FancyTables/index.html#advanced-data-tables",
    "href": "posts/FancyTables/index.html#advanced-data-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Advanced Data Tables",
    "text": "Advanced Data Tables\n\n\n\n\n\nDT Table Code\nds_starter <- ds %>% \n  mutate(province = as.factor(province),\n         price = price,\n         thetaPointMean = mean(points),\n         thetaPriceMean = mean(price))\n\nds_starter %>% \n    arrange(province, year) %>%\n  select(Province = province, \n         Year = year, \n         Price = price, \n         Points = points, \n         Description = description) %>%\n  datatable(., \n            filter = \"bottom\", \n            extensions = 'Buttons', \n            options = list(dom = 'Bfrtip',\n                           buttons = c('copy', 'csv', 'excel'), \n                           initComplete = JS(\"function(settings, json) {\",\n                                             \"$(this.api().table().header()).css({'background-color': '#131F4F', 'color': '#fff'});\",\n                                             \"}\")))"
  },
  {
    "objectID": "posts/FancyTables/index.html#adding-statistical-visuals-to-summary-tables",
    "href": "posts/FancyTables/index.html#adding-statistical-visuals-to-summary-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Adding Statistical Visuals To Summary Tables",
    "text": "Adding Statistical Visuals To Summary Tables\n\n\nGT Table Code\nfancyTbl <- ds_summary %>%\n  gt() %>%\n# format the numeric output to 3 digit rounding  \n  fmt_number(columns = c(pointsMean, pointsSD, priceMean, priceSD),\n             decimals = 3) %>%\n# create nice labels for a few ugly variable names\n  cols_label(province = \"Province\",\n             pointsMean = \"Avg. Points\",\n             pointsSD = \"Std. Dev. Points\",\n             priceMean = \"Avg. Price\",\n             priceSD = \"Std. Dev. Price\",\n             points = \"Points Trend\",\n             price = \"Price Trend\",) %>%\n# Plot the sparklines from the list column\n  gt_plt_sparkline(points, \n                   type=\"ref_median\", \n                   same_limit = TRUE\n                   ) %>%\n  gt_plt_sparkline(price, \n                   type=\"ref_median\", \n                   same_limit = TRUE\n                   ) %>%\n# use the guardian's table theme\n  gt_theme_guardian() %>% \n# give hulk coloring to the Mean Human Rights Score\n  gt_hulk_col_numeric(pointsMean) %>%\n  gt_hulk_col_numeric(priceMean) %>%\n# create a header and subheader\n  tab_header(title=\"Province Pinot Wine Summary\", subtitle = \"Source: Dr. Hendrick\") %>%\n# attach excel file\n  tab_source_note(excel_file_attachment)\n# save the original as an image\n#gtsave(fancyTbl, \"table.png\")\n# show the table themed in accordance with the page\nfancyTbl\n\n\n\n\n\n\n  \n    \n      Province Pinot Wine Summary\n    \n    \n      Source: Dr. Hendrick\n    \n  \n  \n    \n      Province\n      Avg. Points\n      Std. Dev. Points\n      Avg. Price\n      Std. Dev. Price\n      Points Trend\n      Price Trend\n    \n  \n  \n    Burgundy\n90.438\n2.989\n98.035\n132.856\n          89.0\n          83.0\n    California\n90.517\n2.831\n47.465\n18.553\n          91.0\n          34.0\n    Casablanca_Valley\n86.282\n2.428\n21.107\n11.953\n          87.0\n          30.0\n    Marlborough\n87.550\n2.245\n27.668\n13.833\n          85.0\n          25.0\n    New_York\n87.748\n2.268\n25.679\n9.565\n          88.0\n          35.0\n    Oregon\n89.489\n2.663\n44.856\n20.209\n          90.0\n          22.0\n  \n  \n    \n      \n   Download Excel"
  },
  {
    "objectID": "posts/FancyTables/index.html#conclusion",
    "href": "posts/FancyTables/index.html#conclusion",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Conclusion",
    "text": "Conclusion\nThe objective of this endeavor is to create a thorough and all-encompassing compilation of principles and instructions for crafting data tables that are simple to interpret and comprehend. By employing these guidelines, our aim is to enhance the usability and accessibility of data tables across a broad spectrum of applications and sectors."
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html",
    "href": "posts/PredictPinotWine_ML/index.html",
    "title": "Predicting Pinot Wines From 6 Provinces Through Their Individual Descriptions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)\nwine = read_rds(\"pinot.rds\")"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#feature-engineering",
    "href": "posts/PredictPinotWine_ML/index.html#feature-engineering",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(id)`"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#specification",
    "href": "posts/PredictPinotWine_ML/index.html#specification",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Specification",
    "text": "Specification\n\nset.seed(504) \n\nctrl <- trainControl(method = \"cv\", number = 3)\n\n\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\ntest <- wino[-wine_index, ]\n\nfit <- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 100,\n             tuneLength = 15,\n             nodesize = 10,\n             verbose = TRUE,\n             trControl = ctrl,\n             metric = \"Kappa\")"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#re-fit-and-evaluation",
    "href": "posts/PredictPinotWine_ML/index.html#re-fit-and-evaluation",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Re-fit and evaluation",
    "text": "Re-fit and evaluation\n\nset.seed(1504)\n\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\ntest <- wino[-wine_index, ]\n\n# example spec for knn\nfit_final <- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             tuneGrid = fit$bestTune) \n# The last line means we will fit a model using the best tune parameters your CV found above."
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#final-model-performance",
    "href": "posts/PredictPinotWine_ML/index.html#final-model-performance",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Final Model Performance",
    "text": "Final Model Performance\n\nconfusionMatrix(predict(fit_final, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               219          7                 0           0        0\n  California               7        752                12          13       20\n  Casablanca_Valley        0          0                12           0        0\n  Marlborough              0          1                 0          22        0\n  New_York                 0          0                 0           0        1\n  Oregon                  12         31                 2          10        5\n                   Reference\nPrediction          Oregon\n  Burgundy               7\n  California            55\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               485\n\nOverall Statistics\n                                          \n               Accuracy : 0.8912          \n                 95% CI : (0.8753, 0.9057)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8274          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9202            0.9507                 0.461538\nSpecificity                   0.9902            0.8787                 1.000000\nPos Pred Value                0.9399            0.8754                 1.000000\nNeg Pred Value                0.9868            0.9521                 0.991571\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1309            0.4495                 0.007173\nDetection Prevalence          0.1393            0.5134                 0.007173\nBalanced Accuracy             0.9552            0.9147                 0.730769\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                     0.48889       0.0384615        0.8867\nSpecificity                     0.99939       1.0000000        0.9467\nPos Pred Value                  0.95652       1.0000000        0.8899\nNeg Pred Value                  0.98606       0.9850478        0.9450\nPrevalence                      0.02690       0.0155409        0.3270\nDetection Rate                  0.01315       0.0005977        0.2899\nDetection Prevalence            0.01375       0.0005977        0.3258\nBalanced Accuracy               0.74414       0.5192308        0.9167"
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#background",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#background",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Background",
    "text": "Background\nEchinostoma trivolvis is a species of trematode parasite that commonly infects birds, mammals, and some reptiles. It is part of the family Echinostomatidae, which includes several other species of trematodes that infect animals and humans. Echinostoma trivolvis has a complex life cycle that involves multiple hosts, including snails, birds, and mammals. The adult worms inhabit the small intestine of their hosts, where they feed on blood and nutrients. Infection with Echinostoma trivolvis can cause a range of symptoms, including abdominal pain, diarrhea, and malnutrition. The parasite is found throughout North America, particularly in wetlands and other aquatic environments where its intermediate hosts, freshwater snails, are abundant."
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#echinostoma-trivolvis-vs.-pesticides",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#echinostoma-trivolvis-vs.-pesticides",
    "title": "Does Temperature Kill Parasites Faster? Yes!",
    "section": "Echinostoma trivolvis Vs. Pesticides",
    "text": "Echinostoma trivolvis Vs. Pesticides"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html",
    "href": "posts/GymRat_Plotly/index.html",
    "title": "Do You Like Stretching? I would Reconsider!",
    "section": "",
    "text": "Stretching is not a good method of building muscle for several reasons. First, stretching is primarily focused on increasing flexibility and range of motion, rather than building muscle mass or strength. While stretching can help prepare the muscles for exercise and prevent injury, it is not a sufficient method of building muscle. Second, stretching alone does not provide enough resistance or tension on the muscles to stimulate muscle growth. To build muscle, the body needs to be challenged with weight or resistance training. Finally, stretching may actually reduce muscle strength by decreasing muscle activation and power output. While stretching can be a useful addition to a muscle-building routine, it should not be relied upon as the primary method for building muscle mass and strength."
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#general-setup",
    "href": "posts/GymRat_Plotly/index.html#general-setup",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "General Setup",
    "text": "General Setup\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(extrafont)\nlibrary(plotly)\nlibrary(htmlwidgets)\n#font_import()\nloadfonts()"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#data-wrangling-for-plot-1",
    "href": "posts/GymRat_Plotly/index.html#data-wrangling-for-plot-1",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Data Wrangling For Plot 1",
    "text": "Data Wrangling For Plot 1\n\ngymDs <- read_csv(\"megaGymDataset.csv\")\n\n#head(gymDs, 5)\n#names(gymDs)\n\nds <- gymDs %>%\n  mutate(ID = ...1,\n         Level = factor(Level, levels = c(\"Beginner\", \"Intermediate\", \"Expert\")),\n         Type = as.factor(Type)) %>%\n  select(-...1) %>% \n  drop_na()"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#using-plotly-for-plot-1",
    "href": "posts/GymRat_Plotly/index.html#using-plotly-for-plot-1",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Using Plotly for Plot 1",
    "text": "Using Plotly for Plot 1\n\nmyTheme <- theme(text = element_text(family = \"Futura Medium\"),\n                 plot.margin = margin(0.5, 0.5, 0.5, 0.5, unit = \"cm\"),\n                 plot.title = element_text(size = 15, family = \"Futura Condensed ExtraBold\"),\n                 plot.subtitle = element_text(size = 10, family = \"Futura Medium\"),\n                 strip.text.y = element_text(angle = 0, size = 10, family = \"Futura Condensed ExtraBold\"),\n                 strip.placement = \"inside\",\n                 axis.title.x = element_text(margin = margin(t = 0.5, b = 0.5, unit = \"cm\")),\n                 axis.title.y = element_blank(),\n                 axis.text = element_text(size = 9),\n                 legend.position = \"none\",\n                 panel.grid.major.y = element_blank())\n\n\nplotDs <- ds %>% \n  group_by(Type, Level) %>%\n  summarize(meanRating = mean(Rating)) %>%\n  arrange(Type, meanRating) %>%\n  ungroup()\n\np <- plotDs %>%\n  highlight_key(., ~reorder_within(Type, meanRating, Level)) %>%\n  ggplot(aes(x = meanRating, \n             y = reorder_within(Type, meanRating, Level),\n             fill = fct_reorder(Type, meanRating),\n             text = paste0(\"Rating: \", round(meanRating,2),\n                           \"<br>Type: \", Type))) +\n  geom_col(color = \"black\") + \n  facet_grid(rows = vars(Level), \n             scales = \"free_y\", \n             switch = \"y\", \n             space = \"free_y\") +\n  scale_y_reordered() +\n  scale_fill_brewer(palette = \"PuBuGn\") +\n  labs(title = \"Ranking Exercise Type According to Experience Level of Individuals\",\n       x = \"Average Rating of Each Exercise Type\",\n       fill = \"Workout Types\") + \n  theme_minimal() +\n  myTheme \n\n\nggplotly(p, tooltip = \"text\") %>%\n  config(displayModeBar = FALSE) %>%\n  highlight(on = \"plotly_hover\", off = \"plotly_doubleclick\") %>%\n  layout(\n    uniformtext=list(minsize=8, mode='hide'),\n    margin = list(b = 70, l = 140, r = 140)\n  )"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#background",
    "href": "posts/GymRat_Plotly/index.html#background",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Background",
    "text": "Background\nStretching is not a good method of building muscle for several reasons. First, stretching is primarily focused on increasing flexibility and range of motion, rather than building muscle mass or strength. While stretching can help prepare the muscles for exercise and prevent injury, it is not a sufficient method of building muscle. Second, stretching alone does not provide enough resistance or tension on the muscles to stimulate muscle growth. To build muscle, the body needs to be challenged with weight or resistance training. Finally, stretching may actually reduce muscle strength by decreasing muscle activation and power output. While stretching can be a useful addition to a muscle-building routine, it should not be relied upon as the primary method for building muscle mass and strength."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Background\nI come from a background in retail, where I have gained valuable experience in customer service, sales, and inventory management. However, I have always been fascinated by the power of data and its potential to inform business decisions and drive growth. As a result, I have pursued coursework in data science and gained proficiency in programming languages such as Python, R, and SQL. I have also completed projects that involved data cleaning, exploratory data analysis, and predictive modeling. My experience in retail has taught me the importance of understanding customer behavior and market trends, and I believe that these insights can be enhanced through the use of data-driven approaches. I am excited to apply my skills and knowledge to a career in data science and contribute to an organization that values innovation and data-driven decision-making.\n\n\nGoal\nMy goal as a Data Scientist is to leverage my skills and knowledge to contribute to a dynamic organization that values innovation and data-driven decision-making. I am seeking a challenging role where I can apply my expertise in statistical analysis, machine learning, and data visualization to solve complex problems and drive business growth. Through my academic coursework and hands-on experience, I have developed a strong foundation in programming languages such as Python, R, and SQL, as well as tools and frameworks like Tableau, Pandas, and Scikit-learn. I am excited to continue building my skills and learning from experienced professionals in the industry.\n\n\nEducation\nWillamette University, Salem, OR | M.S. in Data Science | Aug 2022 - Aug 2023\nLinfield University, McMinnville, OR | B.A. in Mathematics | Aug 2018 - May 2022\n\n\nProfessional Experience\nThe North Face, a VF Company | Retail Specialist | Oct 2019 - June 2021, Oct 2022 - present\n\nCustomer service: As a retail worker at The North Face, I have developed strong customer service skills, including active listening, problem-solving, and conflict resolution. I am able to provide product recommendations, answer questions about features and benefits, and ensure that customers have a positive experience in the store.\nSales: I am proficient in sales techniques, including suggestive selling, upselling, and cross-selling. I understand how to build rapport with customers and how to close a sale effectively. I am also comfortable with point-of-sale systems and can process transactions accurately and efficiently.\nProduct knowledge: I have in-depth knowledge of The North Face’s products, including their features, benefits, and intended uses. I am able to explain technical details, such as materials and construction, to customers in a clear and concise manner.\nInventory management: I am experienced in managing inventory, including receiving, organizing, and stocking products. I am able to track inventory levels and identify when reorders are necessary. I am also skilled in visual merchandising and can arrange products in an aesthetically pleasing and organized manner.\nTeamwork: I am able to work collaboratively with other team members to achieve common goals. I am comfortable with delegating tasks, taking direction from others, and providing constructive feedback. I am also able to work independently and take initiative when necessary.\nCommunication: I have excellent verbal and written communication skills. I am able to communicate effectively with customers, team members, and management. I am also comfortable with public speaking and can present information to groups in a clear and engaging manner.\n\n\n\nLanguages\n\nI have a deep understanding of the cultural nuances of both English and Spanish-speaking communities. I am able to navigate cultural differences with sensitivity and respect.\nI have basic verbal and written communication skills in French. I am able to introduce myself, ask and answer basic questions, and engage in simple conversations. I am also able to write simple sentences and paragraphs in French."
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html",
    "href": "posts/US_HealthIns_Costs/index.html",
    "title": "U.S. Medical Insurance Costs",
    "section": "",
    "text": "The focal point of this project pertains to the medical insurance industry. I showcase my proficiency in creating compelling visualizations and conducting statistical analyses on a refined data set. Prior to delving into the data set, I formulated several key questions to guide my investigation. These inquiries included identifying the principal driver of heightened insurance costs, evaluating the variables that contribute to rising or falling medical insurance expenses, and identifying the optimal individual profile that minimizes medical insurance costs.\nOf particular interest in this study was the analysis of smoking habits as a variable that may have a significant impact on medical insurance expenses. Moreover, a detailed exploration of the disparities in insurance costs between male and female individuals, both smokers and non-smokers, was conducted.\nTo accomplish these objectives, advanced analytical tools and methodologies were utilized. Rigorous data cleaning and wrangling were carried out to ensure the accuracy and integrity of the data. Subsequently, advanced statistical techniques such as linear regression and hypothesis testing were utilized to extract meaningful insights from the data.\nUltimately, this project provides valuable insights into the medical insurance industry and serves as a testament to the power of data analytics in unlocking actionable insights that can drive informed decision-making.\nIf you wish to skip towards the results portion, feel free to scroll down and have a quick read! Interesting results were found.\n\n\n\n\n   age     sex     bmi  children smoker     region      charges\n0   19  female  27.900         0    yes  southwest  16884.92400\n1   18    male  33.770         1     no  southeast   1725.55230\n2   28    male  33.000         3     no  southeast   4449.46200\n3   33    male  22.705         0     no  northwest  21984.47061\n4   32    male  28.880         0     no  northwest   3866.85520\n\n\n\n\n\n\nWe want to know what influences the cost of insurance. What causes an individual to pay more for their insurance? We can infer that being a smoker will drastically increase their out of pocket medical insurance expenses.\nI take a look into how many smokers we have in this data set. Then, I take a quick look into the smokers’ distribution of charges and show a boxplot to visualize the insurance cost quartiles. Next, I dive into using some statistics such as finding the min, max, mean, median, etc. I follow up by seperating male and female smokers and seeing if there’s a drastic change in their expenses.\n\n\n\nSmoker Count: 274\nNonsmoker Count: 1064\n\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nThe interquartile range is $20,125.33. Very high spread from the first and third interquartiles.\nNow, notice that we have a bimodal distribution. This means that there is a variable in the data set that is drastically affecting the insurance cost. Since there is two “humps” we must identify the component that causes a further jump in insurance costs for those who smoke.\nWhat could cause this? - Region? - Bmi? - Sex? - Children? - Age?\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nSummary Stats: - Mean: $32,050.23 - Median: $34,456.35 - Max: $63,770.43 - Min: $12,829.46 - Standard Deviation: $11,520.47 - Variance: 132721153.14\nGiven the distribution, we can say that the mean represents the true average insurance cost for smokers.\n\n\n\nI chose to seperate male and female smokers to see if there was a noticable difference in cost. Notice in the graphs below, that we can easily identify the culprit for the bimodal distribution. The BMI drastically influences the insurance cost when it is greater than or equal to 30 (the obese rating starts at 30 forward). This means that being obese AND being a smoker can sharply increase your medical insurance cost.\nNote: As Age slowly increases, the insurance cost also slowly increases. So, there is a gradual increase in insurance cost as you age. That is common sense since as one ages, one develops more health issues as they near death.\n\n\n\nGraph One: Cost Distribution - Bimodal Distribution is again present. - Two clusters of cost are due to an underlying variable.\nGraph Two: Barplot of Average Cost per Child - No clear trend. - Only valuable note is that having 4 children seems to have an overall lower average insurance cost.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is a visable linear relationship between BMI and Cost. - There are two clusters of individuals but follows the linear relationship. - At >=30 BMI, there is a clear signifcant increase in insurance cost for male smokers. - Note: a BMI of 30 or implies that the individual is “obese” (though, BMI is not necessarily a good measurement–see results). - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - There is a linear trend, but split into two linear clusters. - As found earlier, this is most likely due to BMI differences in the indivduals. - Overtime, an individual’s insurance cost will increase as expected.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nAs mentioned above, the reason why the cost distribution had two clusters was due to the difference in BMI in each male smoker. Male smokers with a BMI >= 30 will that their expense increase by approximately 47%.\n\n\nText(0, 0.5, 'Insurance Costs (USD)')\n\n\n\n\n\n\n\n\nGraph One: Cost Distribution - Bimodal Distribution is again present. - Two clusters of cost are due to an underlying variable.\nGraph Two: Barplot of Average Cost per Child - No clear trend. - Only valuable note is that having 5 children seems to have an overall lower average insurance cost.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is a visable linear relationship between BMI and Cost - There are two clusters of individuals but follows the linear relationship. - At >=30 BMI, there is a clear signifcant increase in insurance cost for female smokers. - Note: a BMI of 30 or implies that the individual is “obese” (though, BMI is not necessarily a good measurement–see results). - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - There is a linear trend, but split into two linear clusters. - As found earlier, this is most likely due to BMI differences in the indivduals. - Overtime, an individual’s insurance cost will increase as expected.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn.\nGraph Six: Boxplot of Female Smoker Insurance Cost - Noteworthy to see the spread of female smokers’ insurance cost.\nGiven that the Insurane Cost vs BMI is essentially the same as male smokers, the stated results will apply here.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nNoteworthy mention: - Male smokers pay approximately 1.34% more than their female counterparts when their BMI less than 30. - Male smokers pay approximately 1.42% less than their female counterparts when their BMI greater than or equal to 30.\n\n\n\n\n\n<function matplotlib.pyplot.clf()>\n\n\n\n\n\nThe interquartile range is $7,378.07. Spread is minimal from the first and third interquartiles.\nNow, notice that we have a right-skewed distribution.\nWhat could cause this? - Region? - Bmi? - Sex? - Children? - Age?\nKeep in mind, there is a chance that we may not be able to determine the cause of those outliers.\n\n\n7378.07\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\nNow let’s look at the nonsmokers.\nSummary Stats: - Mean: $8,434.27 - Median: $7,345.41 - Max: $39,910.61 - Min: $1,121.87 - Standard Deviation: $5,990.96 - Variance: 35891656\n\n\nGraph One: Cost Distribution - Right-skewed distribution is again present. - Knowing this, we should go with the median as the most appropriate statistic instead of the mean.\nGraph Two: Barplot of Average Cost per Child - There seems to be a trend that having more children will lead to higher average insurance costs. - Given the information from the smokers, when an individual has 5 children or more, their insurance costs drops signifcantly. - However, the lowest average comes from having no children at all.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is no correlation between cost and BMI of nonsmokers. - This is an interesting finding. - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - Strong linear relationship between insurance cost and age. - A secondary trend is shown with the number of children. - The less children, the insurance cost is on the lower end of the linear relationship. - The more children, the insurance cost is on the higher end of the linear relationship - The outliers do not follow the trend line.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn. - Some regions are simply cheaper than others.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nGraph One: Cost Distribution - Right-skewed distribution is again present. - Knowing this, we should go with the median as the most appropriate statistic instead of the mean.\nGraph Two: Barplot of Average Cost per Child - Similar to the male nonsmokers, there seems to be a trend that having more children will result in higher average insurance costs. - Given the information from the smokers, when an individual has 5 children or more, their insurance costs drops on average. - However, the lowest average comes from having no children at all.\nGraph Three: Scatterplot of Cost vs BMI with Region - Again, there is no correlation between cost and BMI of nonsmokers. - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - Strong linear relationship between insurance cost and age. - A secondary trend is shown with the number of children. - The less children, the insurance cost is on the lower end of the linear relationship. - The more children, the insurance cost is on the higher end of the linear relationship - The outliers do not follow the trend line.\nGraph Five: Barplot of Average Cost per Region - There is a slight trend of region importance. - The northeast is more expensive on average than the southwest.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#purpose-of-this-project",
    "href": "posts/US_HealthIns_Costs/index.html#purpose-of-this-project",
    "title": "U.S. Medical Insurance Costs",
    "section": "Purpose of this Project",
    "text": "Purpose of this Project\nThe focal point of this project pertains to the medical insurance industry. I showcase my proficiency in creating compelling visualizations and conducting statistical analyses on a refined data set. Prior to delving into the data set, I formulated several key questions to guide my investigation. These inquiries included identifying the principal driver of heightened insurance costs, evaluating the variables that contribute to rising or falling medical insurance expenses, and identifying the optimal individual profile that minimizes medical insurance costs.\nOf particular interest in this study was the analysis of smoking habits as a variable that may have a significant impact on medical insurance expenses. Moreover, a detailed exploration of the disparities in insurance costs between male and female individuals, both smokers and non-smokers, was conducted.\nTo accomplish these objectives, advanced analytical tools and methodologies were utilized. Rigorous data cleaning and wrangling were carried out to ensure the accuracy and integrity of the data. Subsequently, advanced statistical techniques such as linear regression and hypothesis testing were utilized to extract meaningful insights from the data.\nUltimately, this project provides valuable insights into the medical insurance industry and serves as a testament to the power of data analytics in unlocking actionable insights that can drive informed decision-making.\nIf you wish to skip towards the results portion, feel free to scroll down and have a quick read! Interesting results were found.\n\nSet Up and Importing the Data\n\n\n   age     sex     bmi  children smoker     region      charges\n0   19  female  27.900         0    yes  southwest  16884.92400\n1   18    male  33.770         1     no  southeast   1725.55230\n2   28    male  33.000         3     no  southeast   4449.46200\n3   33    male  22.705         0     no  northwest  21984.47061\n4   32    male  28.880         0     no  northwest   3866.85520"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#analyzing-smoker-insurance-costs",
    "href": "posts/US_HealthIns_Costs/index.html#analyzing-smoker-insurance-costs",
    "title": "U.S. Medical Insurance Costs",
    "section": "Analyzing Smoker Insurance Costs",
    "text": "Analyzing Smoker Insurance Costs\nWe want to know what influences the cost of insurance. What causes an individual to pay more for their insurance? We can infer that being a smoker will drastically increase their out of pocket medical insurance expenses.\nI take a look into how many smokers we have in this data set. Then, I take a quick look into the smokers’ distribution of charges and show a boxplot to visualize the insurance cost quartiles. Next, I dive into using some statistics such as finding the min, max, mean, median, etc. I follow up by seperating male and female smokers and seeing if there’s a drastic change in their expenses.\n\nHistogram Plot of Smokers vs Nonsmokers\n\nSmoker Count: 274\nNonsmoker Count: 1064\n\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nBoxplot and Histogram Distribution of Insurance Costs for Smokers\nThe interquartile range is $20,125.33. Very high spread from the first and third interquartiles.\nNow, notice that we have a bimodal distribution. This means that there is a variable in the data set that is drastically affecting the insurance cost. Since there is two “humps” we must identify the component that causes a further jump in insurance costs for those who smoke.\nWhat could cause this? - Region? - Bmi? - Sex? - Children? - Age?\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nInsurance Cost Statistics for Smokers\nSummary Stats: - Mean: $32,050.23 - Median: $34,456.35 - Max: $63,770.43 - Min: $12,829.46 - Standard Deviation: $11,520.47 - Variance: 132721153.14\nGiven the distribution, we can say that the mean represents the true average insurance cost for smokers.\n\n\nSeperating Male and Female Smokers\nI chose to seperate male and female smokers to see if there was a noticable difference in cost. Notice in the graphs below, that we can easily identify the culprit for the bimodal distribution. The BMI drastically influences the insurance cost when it is greater than or equal to 30 (the obese rating starts at 30 forward). This means that being obese AND being a smoker can sharply increase your medical insurance cost.\nNote: As Age slowly increases, the insurance cost also slowly increases. So, there is a gradual increase in insurance cost as you age. That is common sense since as one ages, one develops more health issues as they near death.\n\n\nAnalyzing Male Smokers\nGraph One: Cost Distribution - Bimodal Distribution is again present. - Two clusters of cost are due to an underlying variable.\nGraph Two: Barplot of Average Cost per Child - No clear trend. - Only valuable note is that having 4 children seems to have an overall lower average insurance cost.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is a visable linear relationship between BMI and Cost. - There are two clusters of individuals but follows the linear relationship. - At >=30 BMI, there is a clear signifcant increase in insurance cost for male smokers. - Note: a BMI of 30 or implies that the individual is “obese” (though, BMI is not necessarily a good measurement–see results). - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - There is a linear trend, but split into two linear clusters. - As found earlier, this is most likely due to BMI differences in the indivduals. - Overtime, an individual’s insurance cost will increase as expected.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nBMI Graph Highlighted\nAs mentioned above, the reason why the cost distribution had two clusters was due to the difference in BMI in each male smoker. Male smokers with a BMI >= 30 will that their expense increase by approximately 47%.\n\n\nText(0, 0.5, 'Insurance Costs (USD)')\n\n\n\n\n\n\n\nAnalyzing Female Smokers\nGraph One: Cost Distribution - Bimodal Distribution is again present. - Two clusters of cost are due to an underlying variable.\nGraph Two: Barplot of Average Cost per Child - No clear trend. - Only valuable note is that having 5 children seems to have an overall lower average insurance cost.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is a visable linear relationship between BMI and Cost - There are two clusters of individuals but follows the linear relationship. - At >=30 BMI, there is a clear signifcant increase in insurance cost for female smokers. - Note: a BMI of 30 or implies that the individual is “obese” (though, BMI is not necessarily a good measurement–see results). - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - There is a linear trend, but split into two linear clusters. - As found earlier, this is most likely due to BMI differences in the indivduals. - Overtime, an individual’s insurance cost will increase as expected.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn.\nGraph Six: Boxplot of Female Smoker Insurance Cost - Noteworthy to see the spread of female smokers’ insurance cost.\nGiven that the Insurane Cost vs BMI is essentially the same as male smokers, the stated results will apply here.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nAverage Insurance Cost Comparision of Male and Female Smokers\nNoteworthy mention: - Male smokers pay approximately 1.34% more than their female counterparts when their BMI less than 30. - Male smokers pay approximately 1.42% less than their female counterparts when their BMI greater than or equal to 30.\n\n\n\n\n\n<function matplotlib.pyplot.clf()>\n\n\n\n\nBoxplot and Histogram Distribution of Insurance Costs for Nonsmokers\nThe interquartile range is $7,378.07. Spread is minimal from the first and third interquartiles.\nNow, notice that we have a right-skewed distribution.\nWhat could cause this? - Region? - Bmi? - Sex? - Children? - Age?\nKeep in mind, there is a chance that we may not be able to determine the cause of those outliers.\n\n\n7378.07\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#analyzing-the-insurance-cost-of-nonsmokers",
    "href": "posts/US_HealthIns_Costs/index.html#analyzing-the-insurance-cost-of-nonsmokers",
    "title": "U.S. Medical Insurance Costs",
    "section": "Analyzing the insurance cost of nonsmokers",
    "text": "Analyzing the insurance cost of nonsmokers\nNow let’s look at the nonsmokers.\nSummary Stats: - Mean: $8,434.27 - Median: $7,345.41 - Max: $39,910.61 - Min: $1,121.87 - Standard Deviation: $5,990.96 - Variance: 35891656\n\nAnalyzing Male Nonsmokers\nGraph One: Cost Distribution - Right-skewed distribution is again present. - Knowing this, we should go with the median as the most appropriate statistic instead of the mean.\nGraph Two: Barplot of Average Cost per Child - There seems to be a trend that having more children will lead to higher average insurance costs. - Given the information from the smokers, when an individual has 5 children or more, their insurance costs drops signifcantly. - However, the lowest average comes from having no children at all.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is no correlation between cost and BMI of nonsmokers. - This is an interesting finding. - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - Strong linear relationship between insurance cost and age. - A secondary trend is shown with the number of children. - The less children, the insurance cost is on the lower end of the linear relationship. - The more children, the insurance cost is on the higher end of the linear relationship - The outliers do not follow the trend line.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn. - Some regions are simply cheaper than others.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nAnalyzing Female Nonsmokers\nGraph One: Cost Distribution - Right-skewed distribution is again present. - Knowing this, we should go with the median as the most appropriate statistic instead of the mean.\nGraph Two: Barplot of Average Cost per Child - Similar to the male nonsmokers, there seems to be a trend that having more children will result in higher average insurance costs. - Given the information from the smokers, when an individual has 5 children or more, their insurance costs drops on average. - However, the lowest average comes from having no children at all.\nGraph Three: Scatterplot of Cost vs BMI with Region - Again, there is no correlation between cost and BMI of nonsmokers. - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - Strong linear relationship between insurance cost and age. - A secondary trend is shown with the number of children. - The less children, the insurance cost is on the lower end of the linear relationship. - The more children, the insurance cost is on the higher end of the linear relationship - The outliers do not follow the trend line.\nGraph Five: Barplot of Average Cost per Region - There is a slight trend of region importance. - The northeast is more expensive on average than the southwest.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#purpose",
    "href": "posts/PredictPinotWine_ML/index.html#purpose",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of this project was to develop a predictive model for identifying the province of origin for wines based on descriptions provided by critics. To achieve this goal, a random forest model was built and evaluated for its performance, achieving a kappa score of 0.82. This project aimed to provide a useful tool for wine connoisseurs and industry professionals in identifying the origin of wines based on their sensory characteristics."
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#setup",
    "href": "posts/PredictPinotWine_ML/index.html#setup",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)\nwine = read_rds(\"pinot.rds\")"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#model-performance",
    "href": "posts/PredictPinotWine_ML/index.html#model-performance",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Model Performance",
    "text": "Model Performance\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               216          5                 0           0        1\n  California              13        758                14          23       19\n  Casablanca_Valley        0          0                10           0        0\n  Marlborough              0          0                 0          12        0\n  New_York                 0          0                 0           0        0\n  Oregon                   9         28                 2          10        6\n                   Reference\nPrediction          Oregon\n  Burgundy              10\n  California            62\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               475\n\nOverall Statistics\n                                          \n               Accuracy : 0.8793          \n                 95% CI : (0.8627, 0.8945)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8069          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9076            0.9583                 0.384615\nSpecificity                   0.9889            0.8515                 1.000000\nPos Pred Value                0.9310            0.8526                 1.000000\nNeg Pred Value                0.9847            0.9579                 0.990379\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1291            0.4531                 0.005977\nDetection Prevalence          0.1387            0.5314                 0.005977\nBalanced Accuracy             0.9482            0.9049                 0.692308\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.266667         0.00000        0.8684\nSpecificity                    1.000000         1.00000        0.9512\nPos Pred Value                 1.000000             NaN        0.8962\nNeg Pred Value                 0.980132         0.98446        0.9370\nPrevalence                     0.026898         0.01554        0.3270\nDetection Rate                 0.007173         0.00000        0.2839\nDetection Prevalence           0.007173         0.00000        0.3168\nBalanced Accuracy              0.633333         0.50000        0.9098"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#conclusion",
    "href": "posts/PredictPinotWine_ML/index.html#conclusion",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Conclusion",
    "text": "Conclusion\nA kappa value of 0.82 indicates a very good level of agreement between the predictions of the random forest model and the actual outcomes. Kappa is a statistical measure of inter-rater agreement, which is commonly used to evaluate the performance of classification models.\nIn the context of a random forest model, the kappa value measures how well the model predicts the correct class labels for a given set of data. A kappa value of 0.82 indicates that the model’s predictions are in very good agreement with the true class labels, with a high degree of precision and accuracy.\nOverall, a kappa value of 0.82 suggests that the random forest model is performing very well, and can be considered a reliable predictor of the target variable in the dataset."
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#purpose",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#purpose",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Purpose",
    "text": "Purpose\nThe objective of this study was to investigate the impact of temperature on the toxicity of pesticides, driven by concerns that the global temperature rise may cause alterations in disease transmission rates. Additionally, the study aimed to address the issue that current pesticide testing may not account for the potential effects of varying temperatures on pesticide toxicity. The central research question was to determine whether temperature has a significant influence on the toxicity of pesticides, and to address this question, survival analysis was employed to analyze the impact of temperature on the time-to-death of the exposed subjects. By utilizing survival analysis, the study aimed to determine whether temperature significantly affects the survival rate of the exposed organisms and to evaluate the magnitude of this effect on the toxicity of pesticides."
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#investigating-the-impact-of-temperature-on-pesticide-toxicity-a-survival-analysis-approach",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#investigating-the-impact-of-temperature-on-pesticide-toxicity-a-survival-analysis-approach",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "text": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach"
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#presentation",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#presentation",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html",
    "href": "posts/AppleInc_IncomeStatement/index.html",
    "title": "Apple’s Stock Journey:",
    "section": "",
    "text": "The primary objective of this undertaking was to delve into the application of R’s plotly and create dynamic and interactive visualizations of Time Series Graphs. In this regard, I opted to scrutinize the Apple stock prices spanning the period from 1981 to the present, with a keen focus on identifying any notable trends that have emerged over time. Upon a closer inspection of the data, I observed a remarkable spike in the company’s stock prices in the aftermath of 2010, which I attribute to the resounding success of Apple’s iPhone and the accompanying products.\nThe shift in Apple’s market value was not just a result of its breakthrough innovation but was also influenced by its impeccable marketing strategy that has ensured that their products always remain relevant to the ever-changing consumer needs. It is noteworthy that Apple’s transformative technology has revolutionized the tech industry and has made significant contributions to its economic growth. Overall, the utilization of R’s plotly in the visualization of Time Series Graphs proved to be an incredibly insightful undertaking, helping to identify and explain the trends in the stock market in a more meaningful way."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#purpose",
    "href": "posts/Pokemon_Database/index.html#purpose",
    "title": "Pokédex Database",
    "section": "Purpose",
    "text": "Purpose\nThe main objective of this project was to construct a fully operational Postgresql database in a time frame of fewer than two weeks by employing the Extract, Transform, Load (ETL) methodology. The purpose of this approach was to extract data from various sources, transform it into a format that could be easily integrated into the database, and finally load the transformed data into the database.\nThe process involved several intricate steps, including identifying the relevant data sources, cleansing the extracted data to remove inconsistencies, standardizing the data to a uniform format, and applying data validation and verification techniques to ensure accuracy and completeness. Furthermore, it required careful consideration of the database schema, including the design of tables, relationships between tables, and the use of appropriate data types.\nThe successful implementation of this project was dependent on the utilization of cutting-edge technologies and tools, such as data integration software, data profiling tools, and scripting languages. The result was a functional database that can efficiently store and manage data, making it readily available for analysis, decision-making, and reporting purposes."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#summary",
    "href": "posts/Pokemon_Database/index.html#summary",
    "title": "Pokédex Database",
    "section": "Summary",
    "text": "Summary\nThe inquiry of identifying the optimal base stat Pokemon type sparked my interest, prompting me to delve into the realm of data engineering. In order to craft a well-informed response to this question, I began by utilizing the expansive and multifaceted “Pokémon of Kanto, Johto, and Hoenn Region” dataset to establish a structured and organized database."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#database-with-postgresql",
    "href": "posts/Pokemon_Database/index.html#database-with-postgresql",
    "title": "Pokédex Database",
    "section": "Database with PostgreSQL",
    "text": "Database with PostgreSQL\n\nPart 1\nDue to time constraints there are some missing tables that would have added more flexiblity to my analysis.\nCREATE TABLE IF NOT EXISTS pokemon (\n    id SMALLINT PRIMARY KEY,\n    identifier TEXT,\n    species_id SMALLINT,\n    height SMALLINT,\n    weight SMALLINT,\n    base_experience SMALLINT,\n    \"order\" SMALLINT,\n    is_default BOOLEAN\n);\n\n\nCREATE TABLE IF NOT EXISTS pokemon_types (\n    pokemon_id SMALLINT,\n    type_id SMALLINT PRIMARY KEY,\n    slot SMALLINT,\n    CONSTRAINT ref_pokemon\n        FOREIGN KEY(pokemon_id)\n            REFERENCES pokemon(id)\n);\n\nCREATE TABLE IF NOT EXISTS pokemon_abilities (\n    pokemon_id SMALLINT,\n    ability_id SMALLINT PRIMARY KEY,\n    is_hidden BOOLEAN,\n    slot SMALLINT,\n    CONSTRAINT ref_pokemon\n        FOREIGN KEY(pokemon_id)\n            REFERENCES pokemon(id)\n);\n\n\nCREATE TABLE IF NOT EXISTS generations (\n    id SMALLINT,\n    main_region_id SMALLINT PRIMARY KEY,\n    identifier CHAR(15) \n);\n\nCREATE TABLE IF NOT EXISTS types (\n    id SMALLINT PRIMARY KEY,\n    identifier CHAR(8),\n    generation_id SMALLINT,\n    damage_class_id SMALLINT,\n    CONSTRAINT ref_pokemon_types\n        FOREIGN KEY(id)\n            REFERENCES pokemon_types(type_id),\n    CONSTRAINT ref_generations \n        FOREIGN KEY(generation_id)\n            REFERENCES generations(main_region_id)\n);\n\nCREATE TABLE IF NOT EXISTS abilities (\n    id SMALLINT,\n    identifier TEXT,\n    generation_id SMALLINT,\n    is_main_series BOOLEAN,\n    CONSTRAINT ref_pokemon_abilities\n        FOREIGN KEY(id)\n            REFERENCES pokemon_abilities(ability_id),\n    CONSTRAINT ref_generations \n        FOREIGN KEY(generation_id)\n            REFERENCES generations(main_region_id)\n);\n\nCREATE TABLE IF NOT EXISTS moves (\n  id SMALLINT,\n  identifier TEXT,\n  generation_id SMALLINT,\n  type_id SMALLINT,\n  power SMALLINT,\n  pp SMALLINT,\n  accuracy SMALLINT,\n  priority SMALLINT,\n  target_id SMALLINT,\n  damage_class_id SMALLINT,\n  effect_id SMALLINT,\n  effect_chance SMALLINT,\n  contest_type_id SMALLINT,\n  contest_effect_id SMALLINT,\n  super_contest_effect_id SMALLINT,\n  CONSTRAINT ref_types\n        UNIQUE(damage_class_id, type_id),\n  CONSTRAINT ref_generations \n        FOREIGN KEY(generation_id)\n            REFERENCES generations(main_region_id),\n  CONSTRAINT ref_types_2\n        FOREIGN KEY(type_id)\n            REFERENCES types(id)\n);\n\n\nPart 2\nThis portion of the sql file is for transforming and preparing a csv file for analysis.\nSELECT \n    identifier AS pokemon_name, \n    pokemon_types.type_id,\n    pokemon_abilities.ability_id\nINTO temp1\nFROM pokemon\nLEFT JOIN pokemon_types\nON pokemon.id = pokemon_types.pokemon_id\nLEFT JOIN pokemon_abilities\nON pokemon.id  = pokemon_abilities.pokemon_id;\n\n\nSELECT \n    pokemon_name,\n    types.identifier AS pokemon_type,\n    abilities.identifier AS pokemon_ability,\n    types.generation_id AS gen_id,\n    types.id AS type_id\nINTO temp2\nFROM temp1\nLEFT JOIN types\nON temp1.type_id = types.id\nLEFT JOIN abilities\nON temp1.ability_id = abilities.id;\n\n\nDROP TABLE temp3;\nSELECT \n    pokemon_name,\n    pokemon_type,\n    pokemon_ability,\n    generations.identifier AS pokemon_generation,\n    moves.identifier AS pokemon_move,\n    moves.power AS pokemon_power,\n    moves.accuracy AS pokemon_accuracy,\n    moves.pp AS pokemon_pp\nINTO temp3\nFROM temp2\nLEFT JOIN generations\nON temp2.gen_id = generations.main_region_id\nLEFT JOIN moves\nON temp2.type_id = moves.type_id;\n\nSELECT *\nFROM temp3\nWHERE pokemon_power IS NOT NULL \n    AND pokemon_accuracy IS NOT NULL\nORDER BY pokemon_accuracy, pokemon_power;\n\n\nCOPY temp3\nTO '/Users/Shared/Data_503/Datasets/scuffed_pokedex.csv'\nWITH (FORMAT CSV, HEADER);\n\n\nPart 3\nHere is my R analysis! Again, feel free to use this as you wish!\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\npokemon <- read_csv(\"scuffed_pokedex.csv\")\n\nnames(pokemon)\n\nnb.cols <- 18\nmycolors <- colorRampPalette(brewer.pal(8, \"YlOrRd\"))(nb.cols)\n\npokemon %>% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %>%\n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_power, y = reorder(pokemon_type, avg_power), fill = reorder(pokemon_type, avg_power)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average power\",\n       y = \"Pokemon type\",\n       title = \"FIRE! The Best Pokemon Type For Damage Output Is...?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\nnb.cols <- 18\nmycolors <- colorRampPalette(brewer.pal(8, \"Blues\"))(nb.cols)\n\npokemon %>% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %>%\n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_accuracy, y = reorder(pokemon_type, avg_accuracy), fill = reorder(pokemon_type, avg_accuracy)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average accuracy\",\n       y = \"Pokemon type\",\n       title = \"Ouch! Who wins the bullseye competition?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\npokemon %>% \n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_power, y = avg_accuarcy, color = pokemon_type)) +\n  geom_point()\n\n\nstats <- pokemon %>% \n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE))\n            \nmodel <- lm(data = stats, avg_accuracy ~ avg_power)\nplot(model)"
  },
  {
    "objectID": "posts/PredictPremiumDefault_ML/index.html",
    "href": "posts/PredictPremiumDefault_ML/index.html",
    "title": "Predicting Premium Default (Insurance)",
    "section": "",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)\ntest <- read_csv(\"test.csv\")\ntrain <- read_csv(\"train.csv\")\n\n\n\nWrangling\n\n#head(train)\n#head(test)\n\nds <- train %>%\n  mutate(age_in_years = round(age_in_days/365, 1),\n         income_class = factor(case_when(Income > 24000 & Income <= 32000 ~ \"24K-32K\",\n                                  Income > 32000 & Income <= 45000 ~ \"32K-45K\",\n                                  Income > 45000 & Income <= 60000 ~ \"45K-60K\",\n                                  Income > 60000 & Income <= 85000 ~ \"60K-85K\",\n                                  Income > 85000 & Income <= 110000 ~ \"85K-110K\",\n                                  Income > 110000 & Income <= 200000 ~ \"110K-200K\",\n                                  Income > 200000 & Income <= 400000 ~ \"200K-400K\",\n                                  Income > 400000 ~ \">400K\"), \n                               levels = c(\"24K-32K\",\n                                          \"32K-45K\",\n                                          \"45K-60K\",\n                                          \"60K-85K\",\n                                          \"85K-110K\",\n                                          \"110K-200K\",\n                                          \"200K-400K\",\n                                          \">400K\"))) %>%\n  select(-application_underwriting_score, -sourcing_channel, - age_in_days)"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html",
    "href": "posts/Customer Return ML/Predict Return Probability.html",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "",
    "text": "To ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I then ran the logistic model and examined the coefficients, noting the highest ratios for ProductDepartment, with a significant influence from men’s and women’s products. Noting that for future exploration, I concluded my model and wrote the submssion file.\nNote: I had explored the idea of using rpart, rf and gbm, however I was not accustomed to using those models. There was much more refinement to be done, but the 3 hour constraint kept me focused on building a draft of the model. Would this go into production? No, but it would be a step in the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#load-the-required-packages",
    "href": "posts/Customer Return ML/Predict Return Probability.html#load-the-required-packages",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Load the required packages",
    "text": "Load the required packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#load-the-training-and-test-data",
    "href": "posts/Customer Return ML/Predict Return Probability.html#load-the-training-and-test-data",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Load the training and test data",
    "text": "Load the training and test data\n\ntrain <- read_csv(\"train.csv\") \ntest <- read_csv(\"test.csv\") \n\nglimpse(train)\n\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#data-exploration",
    "href": "posts/Customer Return ML/Predict Return Probability.html#data-exploration",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n\n\n\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n\n\n\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#feature-engineering",
    "href": "posts/Customer Return ML/Predict Return Probability.html#feature-engineering",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 56, 56, 56, 56, 44, 44, 44, 56, 56, 56, 58, 58, 39, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n\nsummary(train)\n\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :26.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:36.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :47.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :48.26  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:60.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :77.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n\ntable(train$Returned)\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#fit-a-logistical-regression-model",
    "href": "posts/Customer Return ML/Predict Return Probability.html#fit-a-logistical-regression-model",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Fit a Logistical Regression Model",
    "text": "Fit a Logistical Regression Model\n\nset.seed(345)\n\n#Model using Logistical Regression\nlogModel <- glm(Returned ~ .,\n                data = train,\n                family = \"binomial\")\nsummary(logModel)\n\n\nCall:\nglm(formula = Returned ~ ., family = \"binomial\", data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2617  -0.9725  -0.8697   1.3508   2.2074  \n\nCoefficients: (1 not defined because of singularities)\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                 -1.5204170  0.1444030 -10.529  < 2e-16 ***\nCustomerStateAlaska         -0.3745168  0.2706629  -1.384  0.16645    \nCustomerStateArizona        -0.1559535  0.0865001  -1.803  0.07140 .  \nCustomerStateArkansas       -0.1680269  0.1086315  -1.547  0.12192    \nCustomerStateCalifornia     -0.1757649  0.0719039  -2.444  0.01451 *  \nCustomerStateColorado       -0.1682868  0.0931554  -1.807  0.07084 .  \nCustomerStateConnecticut    -0.1087649  0.1043767  -1.042  0.29739    \nCustomerStateDC             -0.1803593  0.2203419  -0.819  0.41305    \nCustomerStateDelaware        0.1134396  0.1748311   0.649  0.51643    \nCustomerStateFlorida        -0.1420662  0.0757237  -1.876  0.06064 .  \nCustomerStateGeorgia        -0.0615201  0.0818030  -0.752  0.45202    \nCustomerStateHawaii         -0.1545264  0.1520539  -1.016  0.30951    \nCustomerStateIdaho          -0.1194081  0.1236664  -0.966  0.33426    \nCustomerStateIllinois       -0.1515665  0.0790256  -1.918  0.05512 .  \nCustomerStateIndiana        -0.0053376  0.0872975  -0.061  0.95125    \nCustomerStateIowa           -0.1039855  0.1103701  -0.942  0.34611    \nCustomerStateKansas         -0.2496458  0.1112347  -2.244  0.02481 *  \nCustomerStateKentucky       -0.3531264  0.1073438  -3.290  0.00100 ** \nCustomerStateLouisiana      -0.2381560  0.0915155  -2.602  0.00926 ** \nCustomerStateMaine          -0.2986534  0.1477344  -2.022  0.04322 *  \nCustomerStateMaryland       -0.1224028  0.0910360  -1.345  0.17877    \nCustomerStateMassachusetts  -0.1545115  0.0849803  -1.818  0.06903 .  \nCustomerStateMichigan       -0.2591744  0.0849511  -3.051  0.00228 ** \nCustomerStateMinnesota      -0.1422575  0.0912242  -1.559  0.11890    \nCustomerStateMississippi    -0.0312455  0.1186160  -0.263  0.79223    \nCustomerStateMissouri       -0.2411922  0.0950728  -2.537  0.01118 *  \nCustomerStateMontana        -0.1814644  0.1471433  -1.233  0.21748    \nCustomerStateNebraska       -0.2027473  0.1442761  -1.405  0.15994    \nCustomerStateNevada         -0.0743860  0.1120681  -0.664  0.50685    \nCustomerStateNew Hampshire  -0.0996571  0.1588999  -0.627  0.53055    \nCustomerStateNew Jersey     -0.1238822  0.0855308  -1.448  0.14751    \nCustomerStateNew Mexico     -0.3891323  0.1352394  -2.877  0.00401 ** \nCustomerStateNew York       -0.1653762  0.0755573  -2.189  0.02862 *  \nCustomerStateNorth Carolina -0.0281690  0.0820686  -0.343  0.73142    \nCustomerStateNorth Dakota   -0.1843492  0.1735505  -1.062  0.28813    \nCustomerStateOhio           -0.1949632  0.0798819  -2.441  0.01466 *  \nCustomerStateOklahoma       -0.2468512  0.1055238  -2.339  0.01932 *  \nCustomerStateOregon         -0.0733648  0.0871922  -0.841  0.40012    \nCustomerStatePennsylvania   -0.1311007  0.0795873  -1.647  0.09951 .  \nCustomerStateRhode Island   -0.1558779  0.1663353  -0.937  0.34869    \nCustomerStateSouth Carolina -0.2159403  0.0967242  -2.233  0.02558 *  \nCustomerStateSouth Dakota   -0.1269469  0.1633225  -0.777  0.43699    \nCustomerStateTennessee      -0.0368559  0.0889942  -0.414  0.67877    \nCustomerStateTexas          -0.0777116  0.0733482  -1.059  0.28938    \nCustomerStateUtah           -0.1711151  0.1082903  -1.580  0.11407    \nCustomerStateVermont        -0.0439923  0.2076074  -0.212  0.83218    \nCustomerStateVirginia       -0.1540969  0.0840292  -1.834  0.06668 .  \nCustomerStateWashington     -0.1241350  0.0861621  -1.441  0.14967    \nCustomerStateWest Virginia  -0.1091140  0.1396389  -0.781  0.43457    \nCustomerStateWisconsin      -0.0821141  0.0921258  -0.891  0.37275    \nCustomerStateWyoming        -0.3274862  0.2007843  -1.631  0.10288    \nProductDepartmentMens        1.4347920  0.0631408  22.724  < 2e-16 ***\nProductDepartmentWomens      1.4951526  0.0638800  23.406  < 2e-16 ***\nProductDepartmentYouth       0.9448759  0.0657464  14.372  < 2e-16 ***\nProductSizeL                -0.0385464  0.0353477  -1.090  0.27550    \nProductSizeM                -0.0780394  0.0354060  -2.204  0.02752 *  \nProductSizeS                -0.0406230  0.0378432  -1.073  0.28307    \nProductSizeXL               -0.0377867  0.0381043  -0.992  0.32136    \nProductSizeXS               -0.0753978  0.0426810  -1.767  0.07730 .  \nProductSizeXXL                      NA         NA      NA       NA    \nProductCost                 -0.0041875  0.0012801  -3.271  0.00107 ** \nDiscountPct                 -0.3452542  0.2723834  -1.268  0.20497    \nPurchasePrice               -0.0036330  0.0030623  -1.186  0.23549    \nSeasonSpring                -0.2631769  0.0236683 -11.119  < 2e-16 ***\nSeasonSummer                -0.1764066  0.0215047  -8.203 2.34e-16 ***\nSeasonWinter                -0.1910545  0.0235449  -8.114 4.88e-16 ***\nCustomerAge                  0.0068431  0.0005897  11.604  < 2e-16 ***\nMSRP                         0.0016353  0.0026851   0.609  0.54251    \nPriceRange$13-$30           -0.4153545  0.0984400  -4.219 2.45e-05 ***\nPriceRange$31-$60           -0.2517772  0.0628835  -4.004 6.23e-05 ***\nPriceRange$61-$100          -0.2086855  0.0375943  -5.551 2.84e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84248  on 64911  degrees of freedom\nResidual deviance: 82399  on 64842  degrees of freedom\nAIC: 82539\n\nNumber of Fisher Scoring iterations: 4\n\nodds_ratio <- exp(logModel$coefficients)\ndata.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n  arrange(desc(odds_ratio)) %>% \n  head()\n\n                                           name odds_ratio\nProductDepartmentWomens ProductDepartmentWomens   4.460017\nProductDepartmentMens     ProductDepartmentMens   4.198772\nProductDepartmentYouth   ProductDepartmentYouth   2.572494\nCustomerStateDelaware     CustomerStateDelaware   1.120124\nCustomerAge                         CustomerAge   1.006867\nMSRP                                       MSRP   1.001637"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#make-prediction-on-the-test-data",
    "href": "posts/Customer Return ML/Predict Return Probability.html#make-prediction-on-the-test-data",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Make prediction on the test data",
    "text": "Make prediction on the test data\n\ntestPredictions <- predict(logModel, newdata = test, type = \"response\")"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#writing-the-submission-file",
    "href": "posts/Customer Return ML/Predict Return Probability.html#writing-the-submission-file",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Writing the Submission File",
    "text": "Writing the Submission File\n\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\nwrite.csv(submission, \"submission.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#leftout-features-that-were-considered",
    "href": "posts/Customer Return ML/Predict Return Probability.html#leftout-features-that-were-considered",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Leftout Features that were considered",
    "text": "Leftout Features that were considered\n\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html",
    "href": "posts/Customer Return ML/index.html",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "",
    "text": "To ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I then ran the logistic model and examined the coefficients, noting the highest ratios for ProductDepartment, with a significant influence from men’s and women’s products. Noting that for future exploration, I concluded my model and wrote the submssion file.\nNote: I had explored the idea of using rpart, rf and gbm, however I was not accustomed to using those models. There was much more refinement to be done, but the 3 hour constraint kept me focused on building a draft of the model. Would this go into production? No, but it would be a step in the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/index.html#load-the-required-packages",
    "href": "posts/Customer Return ML/index.html#load-the-required-packages",
    "title": "Predicting Customer Returns",
    "section": "Load the required packages",
    "text": "Load the required packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#load-the-training-and-test-data",
    "href": "posts/Customer Return ML/index.html#load-the-training-and-test-data",
    "title": "Predicting Customer Returns",
    "section": "Load the training and test data",
    "text": "Load the training and test data\n\ntrain <- read_csv(\"train.csv\") \ntest <- read_csv(\"test.csv\") \n\nglimpse(train)\n\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#data-exploration",
    "href": "posts/Customer Return ML/index.html#data-exploration",
    "title": "Predicting Customer Returns",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n\n\n\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n\n\n\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#feature-engineering",
    "href": "posts/Customer Return ML/index.html#feature-engineering",
    "title": "Predicting Customer Returns",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 56, 56, 56, 56, 44, 44, 44, 56, 56, 56, 58, 58, 39, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n\nsummary(train)\n\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :26.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:36.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :47.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :48.29  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:60.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :77.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n\ntable(train$Returned)\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#fit-a-logistical-regression-model",
    "href": "posts/Customer Return ML/index.html#fit-a-logistical-regression-model",
    "title": "Predicting Customer Returns",
    "section": "Fit a Logistical Regression Model",
    "text": "Fit a Logistical Regression Model\n\nset.seed(345)\n\n#Model using Logistical Regression\nctrl <- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)\nfit <- train(Returned ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 50,\n             tuneLength = 3,\n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n\nRandom Forest \n\n64912 samples\n   10 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 43274, 43275, 43275 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5550386  1.0000000  0.0000000\n  36    0.6250091  0.8337812  0.3601433\n  70    0.6245318  0.8249076  0.3712024\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 36."
  },
  {
    "objectID": "posts/Customer Return ML/index.html#make-prediction-on-the-test-data",
    "href": "posts/Customer Return ML/index.html#make-prediction-on-the-test-data",
    "title": "Predicting Customer Returns",
    "section": "Make prediction on the test data",
    "text": "Make prediction on the test data\n\ntestPredictions <- predict(fit, newdata = test, type = \"prob\")[,2]"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#writing-the-submission-file",
    "href": "posts/Customer Return ML/index.html#writing-the-submission-file",
    "title": "Predicting Customer Returns",
    "section": "Writing the Submission File",
    "text": "Writing the Submission File\n\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\nwrite.csv(submission, \"submission.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#leftout-features-that-were-considered",
    "href": "posts/Customer Return ML/index.html#leftout-features-that-were-considered",
    "title": "Predicting Customer Returns",
    "section": "Leftout Features that were considered",
    "text": "Leftout Features that were considered\n\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#approach-and-methodology",
    "href": "posts/Customer Return ML/index.html#approach-and-methodology",
    "title": "Predicting Customer Returns",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\nTo ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I ran a Random Forest model and achieved an AUC of 0.625. This indicates moderate predictive power for the model, and suggests that further feature engineering or model tuning may be necessary for better performance.\nInitially, I utilized a logistic regression model, however, it required further refinement and tuning. Due to a three-hour time constraint, I focused on building a draft model. While not suitable for production, it is a step towards the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#approach-and-methodology",
    "href": "posts/Customer Return ML/bettermodel.html#approach-and-methodology",
    "title": "Improving Customer Return Model",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\nTo ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I then ran the logistic model and examined the coefficients, noting the highest ratios for ProductDepartment, with a significant influence from men’s and women’s products. Noting that for future exploration, I concluded my model and wrote the submssion file.\nNote: I had explored the idea of using rpart, rf and gbm, however I was not accustomed to using those models. There was much more refinement to be done, but the 3 hour constraint kept me focused on building a draft of the model. Would this go into production? No, but it would be a step in the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#load-the-required-packages",
    "href": "posts/Customer Return ML/bettermodel.html#load-the-required-packages",
    "title": "Improving Customer Return Model",
    "section": "Load the required packages",
    "text": "Load the required packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#load-the-training-and-test-data",
    "href": "posts/Customer Return ML/bettermodel.html#load-the-training-and-test-data",
    "title": "Improving Customer Return Model",
    "section": "Load the training and test data",
    "text": "Load the training and test data\n\ntrain <- read_csv(\"train.csv\") \ntest <- read_csv(\"test.csv\") \n\nglimpse(train)\n\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#data-exploration",
    "href": "posts/Customer Return ML/bettermodel.html#data-exploration",
    "title": "Improving Customer Return Model",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n\n\n\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n\n\n\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#feature-engineering",
    "href": "posts/Customer Return ML/bettermodel.html#feature-engineering",
    "title": "Improving Customer Return Model",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 56, 56, 56, 56, 44, 44, 44, 56, 56, 56, 58, 58, 39, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n\nsummary(train)\n\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :26.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:36.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :47.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :48.28  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:60.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :77.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n\ntable(train$Returned)\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#fit-a-logistical-regression-model",
    "href": "posts/Customer Return ML/bettermodel.html#fit-a-logistical-regression-model",
    "title": "Improving Customer Return Model",
    "section": "Fit a Logistical Regression Model",
    "text": "Fit a Logistical Regression Model\n\nset.seed(345)\n\n#Model using Logistical Regression\nlogModel <- glm(Returned ~ .,\n                data = train,\n                family = \"binomial\")\nsummary(logModel)\n\n\nCall:\nglm(formula = Returned ~ ., family = \"binomial\", data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2616  -0.9725  -0.8697   1.3508   2.2074  \n\nCoefficients: (1 not defined because of singularities)\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                 -1.5207537  0.1444108 -10.531  < 2e-16 ***\nCustomerStateAlaska         -0.3741477  0.2706633  -1.382  0.16687    \nCustomerStateArizona        -0.1556297  0.0865003  -1.799  0.07199 .  \nCustomerStateArkansas       -0.1676433  0.1086317  -1.543  0.12278    \nCustomerStateCalifornia     -0.1754825  0.0719041  -2.441  0.01467 *  \nCustomerStateColorado       -0.1679037  0.0931558  -1.802  0.07148 .  \nCustomerStateConnecticut    -0.1083832  0.1043765  -1.038  0.29909    \nCustomerStateDC             -0.1799928  0.2203424  -0.817  0.41400    \nCustomerStateDelaware        0.1135606  0.1748332   0.650  0.51599    \nCustomerStateFlorida        -0.1418228  0.0757237  -1.873  0.06108 .  \nCustomerStateGeorgia        -0.0612147  0.0818032  -0.748  0.45427    \nCustomerStateHawaii         -0.1541336  0.1520537  -1.014  0.31074    \nCustomerStateIdaho          -0.1195617  0.1236672  -0.967  0.33364    \nCustomerStateIllinois       -0.1512965  0.0790256  -1.915  0.05555 .  \nCustomerStateIndiana        -0.0050442  0.0872975  -0.058  0.95392    \nCustomerStateIowa           -0.1037765  0.1103698  -0.940  0.34708    \nCustomerStateKansas         -0.2493995  0.1112344  -2.242  0.02495 *  \nCustomerStateKentucky       -0.3528027  0.1073443  -3.287  0.00101 ** \nCustomerStateLouisiana      -0.2377653  0.0915155  -2.598  0.00937 ** \nCustomerStateMaine          -0.2983985  0.1477348  -2.020  0.04340 *  \nCustomerStateMaryland       -0.1223623  0.0910361  -1.344  0.17891    \nCustomerStateMassachusetts  -0.1541913  0.0849800  -1.814  0.06961 .  \nCustomerStateMichigan       -0.2587789  0.0849507  -3.046  0.00232 ** \nCustomerStateMinnesota      -0.1419437  0.0912240  -1.556  0.11971    \nCustomerStateMississippi    -0.0308985  0.1186159  -0.260  0.79448    \nCustomerStateMissouri       -0.2408855  0.0950727  -2.534  0.01129 *  \nCustomerStateMontana        -0.1813837  0.1471433  -1.233  0.21769    \nCustomerStateNebraska       -0.2024296  0.1442762  -1.403  0.16060    \nCustomerStateNevada         -0.0742491  0.1120680  -0.663  0.50763    \nCustomerStateNew Hampshire  -0.0996278  0.1589004  -0.627  0.53067    \nCustomerStateNew Jersey     -0.1235066  0.0855316  -1.444  0.14874    \nCustomerStateNew Mexico     -0.3888035  0.1352393  -2.875  0.00404 ** \nCustomerStateNew York       -0.1650836  0.0755573  -2.185  0.02890 *  \nCustomerStateNorth Carolina -0.0278880  0.0820683  -0.340  0.73400    \nCustomerStateNorth Dakota   -0.1839630  0.1735504  -1.060  0.28915    \nCustomerStateOhio           -0.1947161  0.0798819  -2.438  0.01479 *  \nCustomerStateOklahoma       -0.2466360  0.1055241  -2.337  0.01943 *  \nCustomerStateOregon         -0.0730939  0.0871920  -0.838  0.40186    \nCustomerStatePennsylvania   -0.1309621  0.0795873  -1.646  0.09986 .  \nCustomerStateRhode Island   -0.1554846  0.1663351  -0.935  0.34991    \nCustomerStateSouth Carolina -0.2157011  0.0967242  -2.230  0.02574 *  \nCustomerStateSouth Dakota   -0.1268369  0.1633226  -0.777  0.43739    \nCustomerStateTennessee      -0.0367034  0.0889942  -0.412  0.68003    \nCustomerStateTexas          -0.0774882  0.0733484  -1.056  0.29077    \nCustomerStateUtah           -0.1708986  0.1082913  -1.578  0.11453    \nCustomerStateVermont        -0.0436083  0.2076073  -0.210  0.83363    \nCustomerStateVirginia       -0.1537185  0.0840291  -1.829  0.06735 .  \nCustomerStateWashington     -0.1238717  0.0861623  -1.438  0.15053    \nCustomerStateWest Virginia  -0.1087151  0.1396384  -0.779  0.43625    \nCustomerStateWisconsin      -0.0820722  0.0921260  -0.891  0.37300    \nCustomerStateWyoming        -0.3279518  0.2007806  -1.633  0.10239    \nProductDepartmentMens        1.4348495  0.0631407  22.725  < 2e-16 ***\nProductDepartmentWomens      1.4952204  0.0638800  23.407  < 2e-16 ***\nProductDepartmentYouth       0.9449300  0.0657463  14.372  < 2e-16 ***\nProductSizeL                -0.0385858  0.0353476  -1.092  0.27500    \nProductSizeM                -0.0780820  0.0354060  -2.205  0.02743 *  \nProductSizeS                -0.0406725  0.0378430  -1.075  0.28248    \nProductSizeXL               -0.0378496  0.0381042  -0.993  0.32056    \nProductSizeXS               -0.0754693  0.0426809  -1.768  0.07702 .  \nProductSizeXXL                      NA         NA      NA       NA    \nProductCost                 -0.0041879  0.0012801  -3.271  0.00107 ** \nDiscountPct                 -0.3452504  0.2723828  -1.268  0.20497    \nPurchasePrice               -0.0036323  0.0030623  -1.186  0.23558    \nSeasonSpring                -0.2631683  0.0236683 -11.119  < 2e-16 ***\nSeasonSummer                -0.1763956  0.0215047  -8.203 2.35e-16 ***\nSeasonWinter                -0.1910599  0.0235449  -8.115 4.87e-16 ***\nCustomerAge                  0.0068400  0.0005896  11.600  < 2e-16 ***\nMSRP                         0.0016355  0.0026851   0.609  0.54245    \nPriceRange$13-$30           -0.4153104  0.0984400  -4.219 2.45e-05 ***\nPriceRange$31-$60           -0.2517379  0.0628836  -4.003 6.25e-05 ***\nPriceRange$61-$100          -0.2086568  0.0375943  -5.550 2.85e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84248  on 64911  degrees of freedom\nResidual deviance: 82399  on 64842  degrees of freedom\nAIC: 82539\n\nNumber of Fisher Scoring iterations: 4\n\nodds_ratio <- exp(logModel$coefficients)\ndata.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n  arrange(desc(odds_ratio)) %>% \n  head()\n\n                                           name odds_ratio\nProductDepartmentWomens ProductDepartmentWomens   4.460319\nProductDepartmentMens     ProductDepartmentMens   4.199013\nProductDepartmentYouth   ProductDepartmentYouth   2.572633\nCustomerStateDelaware     CustomerStateDelaware   1.120260\nCustomerAge                         CustomerAge   1.006863\nMSRP                                       MSRP   1.001637"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#make-prediction-on-the-test-data",
    "href": "posts/Customer Return ML/bettermodel.html#make-prediction-on-the-test-data",
    "title": "Improving Customer Return Model",
    "section": "Make prediction on the test data",
    "text": "Make prediction on the test data\n\ntestPredictions <- predict(logModel, newdata = test, type = \"response\")"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#writing-the-submission-file",
    "href": "posts/Customer Return ML/bettermodel.html#writing-the-submission-file",
    "title": "Improving Customer Return Model",
    "section": "Writing the Submission File",
    "text": "Writing the Submission File\n\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\nwrite.csv(submission, \"submission.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#leftout-features-that-were-considered",
    "href": "posts/Customer Return ML/bettermodel.html#leftout-features-that-were-considered",
    "title": "Improving Customer Return Model",
    "section": "Leftout Features that were considered",
    "text": "Leftout Features that were considered\n\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#fit-a-random-forest-model",
    "href": "posts/Customer Return ML/index.html#fit-a-random-forest-model",
    "title": "Predicting Customer Returns",
    "section": "Fit a Random Forest Model",
    "text": "Fit a Random Forest Model\n\nset.seed(345)\n\n#Model using Random Forest \nctrl <- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)\nfit <- train(Returned ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 50,\n             tuneLength = 3,\n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n\nRandom Forest \n\n64912 samples\n   10 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 43274, 43275, 43275 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5560054  1.0000000  0.0000000\n  36    0.6232798  0.8319257  0.3590509\n  70    0.6249459  0.8264303  0.3719893\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 70."
  },
  {
    "objectID": "posts/PredictPremiumDefault_ML/index.html#setup",
    "href": "posts/PredictPremiumDefault_ML/index.html#setup",
    "title": "Predicting Premium Default (Insurance)",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)\ntest <- read_csv(\"test.csv\")\ntrain <- read_csv(\"train.csv\")"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Presentations",
    "section": "",
    "text": "Link: https://bcervantesalvarez.github.io/Presentations/AboutMe/#/title-slide"
  },
  {
    "objectID": "presentation.html#parasites-slides-work-in-progress",
    "href": "presentation.html#parasites-slides-work-in-progress",
    "title": "Presentations",
    "section": "Parasites Slides [Work in progress]",
    "text": "Parasites Slides [Work in progress]\nLink: https://bcervantesalvarez.github.io/Presentations/Parasites/#/title-slide"
  },
  {
    "objectID": "presentation.html#parasites-slides",
    "href": "presentation.html#parasites-slides",
    "title": "Presentations",
    "section": "Parasites Slides",
    "text": "Parasites Slides\nLink: https://bcervantesalvarez.github.io/Presentations/Parasites/#/title-slide"
  },
  {
    "objectID": "presentation.html#test",
    "href": "presentation.html#test",
    "title": "Presentations",
    "section": "{TEST}",
    "text": "{TEST}"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#apply-custom-theme",
    "href": "posts/AppleInc_IncomeStatement/index.html#apply-custom-theme",
    "title": "Apple’s Stock Journey:",
    "section": "Apply Custom Theme",
    "text": "Apply Custom Theme\n\nmyTheme <- function(){ \n    font <- \"SF Mono\"   #assign font family up front\n    \n    theme_minimal() %+replace%    #replace elements we want to change\n    \n    theme(\n      \n      #grid elements\n      panel.grid.major.x = element_blank(),    #strip major gridlines\n      panel.grid.minor = element_blank(),    #strip minor gridlines\n      axis.ticks = element_blank(),          #strip axis ticks\n      \n      #since theme_minimal() already strips axis lines, \n      #we don't need to do that again\n      \n      #text elements\n      plot.title = element_text(             #title\n                   family = font,            #set font family\n                   size = 16,                #set font size\n                   face = 'bold',            #bold typeface\n                   hjust = 0,                #left align\n                   vjust = 2),               #raise slightly\n      \n      plot.subtitle = element_text(          #subtitle\n                   family = font,            #font family\n                   size = 12),               #font size\n      \n      plot.caption = element_text(           #caption\n                   family = font,            #font family\n                   size = 9,                 #font size\n                   hjust = 1),               #right align\n      \n      axis.title = element_text(             #axis titles\n                   family = font,            #font family\n                   size = 10),               #font size\n      \n      axis.text = element_text(              #axis text\n                   family = font,            #axis famuly\n                   size = 9),                #font size\n      \n      axis.text.x = element_text(            #margin for axis text\n                    margin=margin(5, b = 10))\n      \n      #since the legend often requires manual tweaking \n      #based on plot content, don't define it here\n    )\n}"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#data-wrangling",
    "href": "posts/AppleInc_IncomeStatement/index.html#data-wrangling",
    "title": "Apple’s Stock Journey:",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nyearlyDs <- ds %>% \n  drop_na() %>%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %>%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %>%\n  summarize(Open = mean(Open),\n            High = mean(High),\n            Low = mean(Low),\n            Close = mean(Close),\n            AdjClose = mean(`Adj Close`),\n            Volume = mean(Volume))\n\n\nlog_yearlyDs <- ds %>% \n  drop_na() %>%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %>%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %>%\n  summarize(Open = log(mean(Open)),\n            High = log(mean(High)),\n            Low = log(mean(Low)),\n            Close = log(mean(Close)),\n            AdjClose = log(mean(`Adj Close`)),\n            Volume = log(mean(Volume)))"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#section",
    "href": "posts/AppleInc_IncomeStatement/index.html#section",
    "title": "Finance: Apple’s Journey",
    "section": "",
    "text": "p <- yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price Since 1981\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  myTheme()\n\nggplotly(p) %>%\n  layout(hovermode = \"x unified\") %>% \n  style(hovertext = paste0(\" High: $\", round(yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: $\", round(yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: $\", round(yearlyDs$AdjClose,2)),\n        traces = 3)\n\n\n\n\n\n\np2 <- log_yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price (Log-Normalized) Since 1981\") +\n  myTheme()\n\nggplotly(p2, tooltip = \"text\") %>%\n  layout(hovermode = \"x unified\", \n         hovertext = paste0(\" Year: \", log_yearlyDs$Year)) %>% \n  style(hovertext = paste0(\" High: \", round(log_yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: \", round(log_yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: \", round(log_yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "href": "posts/AppleInc_IncomeStatement/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "title": "Apple’s Stock Journey:",
    "section": "Times Series Plots of Apple Inc. Stock Prices",
    "text": "Times Series Plots of Apple Inc. Stock Prices\n\np <- yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price Since 1981\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  myTheme()\n\nggplotly(p) %>%\n  layout(hovermode = \"x unified\") %>% \n  style(hovertext = paste0(\" High: $\", round(yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: $\", round(yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: $\", round(yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#applying-log-norm",
    "href": "posts/AppleInc_IncomeStatement/index.html#applying-log-norm",
    "title": "Apple’s Stock Journey:",
    "section": "Applying Log-norm",
    "text": "Applying Log-norm\n\np2 <- log_yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price (Log-Normalized)\") +\n  myTheme()\n\nggplotly(p2, tooltip = \"text\") %>%\n  layout(hovermode = \"x unified\", \n         hovertext = paste0(\" Year: \", log_yearlyDs$Year)) %>% \n  style(hovertext = paste0(\" High: \", round(log_yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: \", round(log_yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: \", round(log_yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#setup",
    "href": "posts/AppleInc_IncomeStatement/index.html#setup",
    "title": "Apple’s Stock Journey:",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\n\n\nds <- read_csv(\"AppleInc_Stocks.csv\")\n\n#head(ds)"
  }
]