[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Building A Quick Dashboard For Amazon Products (EN.)\n\n\n\n\n\n\n\nR\n\n\nRShiny\n\n\nCSS\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Individuals Who Earn More Than 50K\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nGradient Boosting Machines\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nBrian Cervantes Alvarez, Willa Van Liew\n\n\n\n\n\n\n  \n\n\n\n\nGot Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress\n\n\n\n\n\n\n\nR\n\n\nRPlotly\n\n\nReports\n\n\nTime Series\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nApple’s Journey In The Stock Market\n\n\n\n\n\n\n\nR\n\n\nTime Series\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Customer Returns\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nRandom Forest\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nInsurance Premium Default\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nPredicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nRandom Forest\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nInvestigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach\n\n\n\n\n\n\n\nR\n\n\nReveal.JS\n\n\nSurvival Analysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nBrian Cervantes Alvarez, Willa Van Liew, Hans Lehndorff\n\n\n\n\n\n\n  \n\n\n\n\nDo You Like Stretching? I Would Reconsider!\n\n\n\n\n\n\n\nR\n\n\nRPlotly\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nRating Pinot Wines: Is More Expensive Better?\n\n\n\n\n\n\n\nR\n\n\nDatatables\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nHealthcare Spending Is Only Getting Worse\n\n\n\n\n\n\n\nR\n\n\nLinear Regression\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2022\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nPokédex Database\n\n\n\n\n\n\n\nR\n\n\nPostgreSQL\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\n  \n\n\n\n\nResources for Prospective College Students\n\n\n\n\n\n\n\nR\n\n\nRShiny\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\nBrian Cervantes Alvarez, Corey Cassell\n\n\n\n\n\n\n  \n\n\n\n\nU.S. Medical Insurance Costs\n\n\n\n\n\n\n\nPython\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2022\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brian",
    "section": "",
    "text": "Who am I?\nAs a next-generation data scientist, I have acquired skills in machine learning and survival analysis, as well as in effectively communicating data insights. I am proficient in programming languages such as Python, R, and SQL and have experience working with various data visualization tools and statistical analysis techniques. My dedication to continuous learning allows me to stay up-to-date with the latest developments in the data science field. I am excited to leverage my skills to tackle real-world problems and learn from experienced professionals in the industry.\n\n\nWhere do I come from?\nI was born in Pasco, Washington, a small city in the southeastern part of the state. Pasco is part of the Tri-Cities area, along with Richland and Kennewick, and is known for its agricultural industry, particularly the cultivation of potatoes, apples, and wine grapes. I enjoyed the rural atmosphere of Pasco, with its open fields, orchards, and vineyards.\n\n\n\n\n\n\n\n\n\nPasco, Washington\n\n\nBefore finishing 1st grade, I moved to Salem, Oregon, a mid-sized city in the heart of the Willamette Valley. Salem is known for its farms and vineyards that produce some of the best wine and food in the state. Growing up in Salem, I was immersed in the outdoors and enjoyed activities such as hiking, camping, and running. Salem also has a rich history, with landmarks like the Oregon State Capitol and Willamette University, which is the oldest university in the west. However, as I got older, I also became aware of some of the social and economic challenges faced by many families in the area, such as poverty, limited job opportunities, and inadequate access to healthcare. My family and I suffered poverty and food instability before I graduated high school. Despite these challenges, I feel a strong connection to Salem and its people, and I am grateful for the experiences and values that I gained from growing up in this community.\n\n\n\n\n\n\n\n\n\nSalem, Oregon"
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#abstract",
    "href": "posts/US_HealthCare_Spending/index.html#abstract",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "ABSTRACT",
    "text": "ABSTRACT\nU.S. Healthcare costs are increasing in every category since 1980. There are an unfiltered number of reasons for the rise of these costs. From population increase to doctor’s wages, they are but a few examples of the unforgiving healthcare spending increases that are observed in a yearly basis. This report examines the expenditure trends from 1980 to 2005 and up to 2014. The results show that each sector in the United States is experiencing an unprecedented increase in health care costs. Hence, the findings revealed in this report serve to provide context behind the United States’ status as one the most expensive health care nations in the world."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#introduction",
    "href": "posts/US_HealthCare_Spending/index.html#introduction",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "INTRODUCTION",
    "text": "INTRODUCTION\nHealthcare is a vital component to our lives. Without it, we cannot get the necessary help to live happy lives. Or worse, we could not live long enough to enjoy it. However, Healthcare spending continues to skyrocket in a year to year basis. This report reveals the frightening truth about the healthcare spending and visualizes each component. From the overall spending nationally, to showing each category and their individual trends, the spending remains increasingly constant."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#background",
    "href": "posts/US_HealthCare_Spending/index.html#background",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "BACKGROUND",
    "text": "BACKGROUND\nNow, the data set that was used for this report is called US Healthcare Spending Per Capita (source: Kaggle). The format of this data set was quite tricky. Instead of being in the long format, with few columns and lengthy rows, it was in the wide format, lots of columns and very few rows. Specifically, the years were in the format, “Y####” which made it very difficult to do any sort of analysis at first. Though, with some pivoting and string manipulation, the updated data set was ready to do some quirky analysis. The full step process is coming up next."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#methodology",
    "href": "posts/US_HealthCare_Spending/index.html#methodology",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "METHODOLOGY",
    "text": "METHODOLOGY\nBefore jumping into the data, first start with seeing whether it is “wide” or “long”. Check out the number of rows and columns to allow some room to do some data wrangling.\n\n\n# A tibble: 5 × 42\n   Code Item     Group Region_Number Region_Name State_Name  Y1980  Y1981  Y1982\n  <dbl> <chr>    <chr>         <dbl> <chr>       <chr>       <dbl>  <dbl>  <dbl>\n1     1 Persona… Unit…             0 United Sta… <NA>       216977 251789 283073\n2     1 Persona… Regi…             1 New England <NA>        12960  14845  16759\n3     1 Persona… Regi…             2 Mideast     <NA>        43479  49604  55406\n4     1 Persona… Regi…             3 Great Lakes <NA>        40658  46668  51440\n5     1 Persona… Regi…             4 Plains      <NA>        16980  19682  21919\n# ℹ 33 more variables: Y1983 <dbl>, Y1984 <dbl>, Y1985 <dbl>, Y1986 <dbl>,\n#   Y1987 <dbl>, Y1988 <dbl>, Y1989 <dbl>, Y1990 <dbl>, Y1991 <dbl>,\n#   Y1992 <dbl>, Y1993 <dbl>, Y1994 <dbl>, Y1995 <dbl>, Y1996 <dbl>,\n#   Y1997 <dbl>, Y1998 <dbl>, Y1999 <dbl>, Y2000 <dbl>, Y2001 <dbl>,\n#   Y2002 <dbl>, Y2003 <dbl>, Y2004 <dbl>, Y2005 <dbl>, Y2006 <dbl>,\n#   Y2007 <dbl>, Y2008 <dbl>, Y2009 <dbl>, Y2010 <dbl>, Y2011 <dbl>,\n#   Y2012 <dbl>, Y2013 <dbl>, Y2014 <dbl>, …\n\n\n [1] \"Code\"                          \"Item\"                         \n [3] \"Group\"                         \"Region_Number\"                \n [5] \"Region_Name\"                   \"State_Name\"                   \n [7] \"Y1980\"                         \"Y1981\"                        \n [9] \"Y1982\"                         \"Y1983\"                        \n[11] \"Y1984\"                         \"Y1985\"                        \n[13] \"Y1986\"                         \"Y1987\"                        \n[15] \"Y1988\"                         \"Y1989\"                        \n[17] \"Y1990\"                         \"Y1991\"                        \n[19] \"Y1992\"                         \"Y1993\"                        \n[21] \"Y1994\"                         \"Y1995\"                        \n[23] \"Y1996\"                         \"Y1997\"                        \n[25] \"Y1998\"                         \"Y1999\"                        \n[27] \"Y2000\"                         \"Y2001\"                        \n[29] \"Y2002\"                         \"Y2003\"                        \n[31] \"Y2004\"                         \"Y2005\"                        \n[33] \"Y2006\"                         \"Y2007\"                        \n[35] \"Y2008\"                         \"Y2009\"                        \n[37] \"Y2010\"                         \"Y2011\"                        \n[39] \"Y2012\"                         \"Y2013\"                        \n[41] \"Y2014\"                         \"Average_Annual_Percent_Growth\"\n\n\nAs explained earlier, the data set was evidently in the wide format. So, using the “pivot_longer” function with the “tidyverse” package, the years columns were placed into one column called “Year” with their values in the another column called “Cost”. I used the “gsub” function to fix up my Year column and factored a few columns for later use.\n\n\n# A tibble: 6 × 5\n  Item                 Region_Name   State_Name  Year   Cost\n  <fct>                <fct>         <fct>      <dbl>  <dbl>\n1 Personal Health Care United States <NA>        1980 216977\n2 Personal Health Care United States <NA>        1981 251789\n3 Personal Health Care United States <NA>        1982 283073\n4 Personal Health Care United States <NA>        1983 311677\n5 Personal Health Care United States <NA>        1984 341645\n6 Personal Health Care United States <NA>        1985 376376\n\n\n\nHealth Care Costs Have Increased\nDiving straight into the first visual. It appears very clearly that Healthcare spending is in an upward trend and does not appear to slow down. The graph was filter from the years 1980 to 2005 to highlight this era of healthcare costs.\n\n\n\n\n\n\n\nPersonal, Hospital and Physician & Clinical Care Are The Dominate Spending Categories\nYikes. Personal health care went from about $10K to nearly $80K in that short time span. Hospital and Clinical Care are heavy hitters for spending. Though this graph shows the same trend for each category. Not very pleasant information.\n\n\n\n\n\n\n\nEach Region Is Trending The Same\nTerrible news, each region has not stopped increasing spending. Their trend lines all appear to be similar and proportionally alike to one and another. The highlights are that the Mideast is the most expensive and the Rocky Mountains is the cheapest. Note, this was looking at the spending up to 2005. Maybe things will change by 2014.\n\n\n\n\n\n\n\nThe Trends Remain. Healthcare Spending Continues To Rise In Every Region In The U.S.\nIn the bar chart, you can distinguish more clearly each region and their overall spending. What stands out the most is that the Plains, New England and the Rocky Mountains are one of the lowest in terms of medical funding. In some cases, they are 1/3 the cost of the most expensive regions.\n\n\n\n\n\n\n\nThe Far West Ranks 3rd In Healthcare Spending\nFor 2014, that is quite the jump. It can be assumed that all those states are heavy hitters in terms of spending, but that is proven quite wrong in the next graphic.\n\n\n\n\n\n[1] \"$21.81M\"\n[1] \"$21.81M\"\n\n\n\n\nOregon Ranks 3rd, But Don’t Be Fooled!\nLook, California takes the #1 spot in spending. You cannot deny the underlying reasons behind that, such as their ridicously population size. While this report did not look into it, you would probably push the Far West region closer to the Plains or New England in terms of spending.\n\n\n\n\n\n[1] \"$1.41M\"  \"$53.64M\" \"$1.9M\"   \"$3.41M\"  \"$5.81M\"  \"$10.16M\"\n[1] \"$1.41M\"  \"$53.64M\" \"$1.9M\"   \"$3.41M\"  \"$5.81M\"  \"$10.16M\"\n[1] \"$5.81M\"\n[1] \"$5.81M\"\n\n\n\n\nA Linear Fit Is Not The Right Model For This Data\nRight off the bat, the model appears to have an adjusted R-Squared value of 0.8572. That looks wondeful, right? No! This model is not accurate at all and is highly discouraged. It becomes clear that their is no correlation between Cost and Region_Name per Year. While not shown, this was true for the filtered data set that went up from 1980 to 2014. The residual plots speak for themselves. The Residuals vs Fitted was showing clear signs of being a quadratic fit instead of a linear one. The Q-Q was short of linear but had a number of curves along the fitted line. The scale location evidently showed that this model was simple not the correct fit for this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      1 \n11072.4 \n\n\n\nCall:\nlm(formula = Cost ~ Region_Name + Year, data = regionHealthCareSince2005)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2683.8  -782.9  -279.8   597.7  4139.3 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -680650.41   24479.29 -27.805  < 2e-16 ***\nRegion_NameGreat Lakes        1438.65     368.55   3.904 0.000130 ***\nRegion_NameMideast            1657.50     368.55   4.497 1.17e-05 ***\nRegion_NameNew England       -3909.68     368.55 -10.608  < 2e-16 ***\nRegion_NamePlains            -3830.75     368.55 -10.394  < 2e-16 ***\nRegion_NameRocky Mountains   -5106.26     368.55 -13.855  < 2e-16 ***\nRegion_NameSoutheast         -1231.18     368.55  -3.341 0.000998 ***\nRegion_NameSouthwest          -849.93     368.55  -2.306 0.022133 *  \nYear                           344.83      12.29  28.069  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1329 on 199 degrees of freedom\nMultiple R-squared:   0.88, Adjusted R-squared:  0.8752 \nF-statistic: 182.4 on 8 and 199 DF,  p-value: < 2.2e-16\n\n\n             Df    Sum Sq   Mean Sq F value Pr(>F)    \nRegion_Name   7 1.185e+09 1.693e+08    95.9 <2e-16 ***\nYear          1 1.391e+09 1.391e+09   787.9 <2e-16 ***\nResiduals   199 3.514e+08 1.766e+06                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nWas there A Significant Difference In The Means Of Each Region?\nI wished to explore if it was statistically significant on where each region’s mean over the years mattered. I actually started off using a LeveneTest to see if the variance (the spread of the spending for each region) was important. Given both tests yielded an extremely small p-value, it is clear that there are 3 regions that have a completely different variance than the other regions. I was not done there, I now wanted to confirm this with the TukeyHsd test to see if their means differed as a result. Yes, their means differed as expected from the LeveneTest. To make clear, New England, Plains and Rocky Mountains spent much lower on average. Despite that fact, they are following the trend of growth with an 18% since 2005.\nI wished to explore if it was statistically significant on where each region’s mean over the years mattered. I actually started off using a LeveneTest to see if the variance (the spread of the spending for each region) was important. Given both tests yielded an extremely small p-value, it is clear that there are 3 regions that have a completely different variance than the other regions. I was not done there, I now wanted to confirm this with the TukeyHsd test to see if their means differed as a result. Yes, their means differed as expected from the LeveneTest. To make clear, New England, Plains and Rocky Mountains spent much lower on average. Despite that fact, they are following the trend of growth with an 18% since 2005.\n\n\n[1] \"The Average Spending In The Expensive Regions since 2005 = $5333.29\"\n\n\n[1] \"The Average Spending In The Expensive Regions since 2014= $7724.45\"\n\n\n[1] \"Difference: +$2391.16 | Percentage Increase: +18.31%\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2005 = $2173.22\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2014= $3142.21\"\n\n\n[1] \"Difference: +$968.99 | Percentage Increase: 18.23%\"\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(>F)    \ngroup   7   12.03 8.727e-13 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   7  11.462 3.292e-12 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(>F)    \ngroup   7  19.546 < 2.2e-16 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   7  12.828 3.075e-14 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n             Df    Sum Sq   Mean Sq F value Pr(>F)    \nRegion_Name   7 1.185e+09 169337440   19.43 <2e-16 ***\nResiduals   200 1.743e+09   8712933                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Cost ~ Region_Name, data = regionHealthCareSince2005)\n\n$Region_Name\n                                   diff         lwr           upr     p adj\nGreat Lakes-Far West         1438.64777 -1069.19906  3946.4945990 0.6495584\nMideast-Far West             1657.50100  -850.34583  4165.3478291 0.4679930\nNew England-Far West        -3909.68132 -6417.52815 -1401.8344885 0.0000930\nPlains-Far West             -3830.75287 -6338.59970 -1322.9060420 0.0001417\nRocky Mountains-Far West    -5106.25783 -7614.10466 -2598.4109954 0.0000001\nSoutheast-Far West          -1231.18113 -3739.02796  1276.6657036 0.8046614\nSouthwest-Far West           -849.92577 -3357.77260  1657.9210559 0.9679983\nMideast-Great Lakes           218.85323 -2288.99360  2726.7000602 0.9999950\nNew England-Great Lakes     -5348.32909 -7856.17592 -2840.4822574 0.0000000\nPlains-Great Lakes          -5269.40064 -7777.24747 -2761.5538109 0.0000000\nRocky Mountains-Great Lakes -6544.90559 -9052.75242 -4037.0587643 0.0000000\nSoutheast-Great Lakes       -2669.82890 -5177.67573  -161.9820653 0.0279871\nSouthwest-Great Lakes       -2288.57354 -4796.42037   219.2732870 0.1019616\nNew England-Mideast         -5567.18232 -8075.02915 -3059.3354875 0.0000000\nPlains-Mideast              -5488.25387 -7996.10070 -2980.4070410 0.0000000\nRocky Mountains-Mideast     -6763.75882 -9271.60565 -4255.9119944 0.0000000\nSoutheast-Mideast           -2888.68213 -5396.52896  -380.8352954 0.0119419\nSouthwest-Mideast           -2507.42677 -5015.27360     0.4200569 0.0500724\nPlains-New England             78.92845 -2428.91838  2586.7752767 1.0000000\nRocky Mountains-New England -1196.57651 -3704.42334  1311.2703233 0.8267302\nSoutheast-New England        2678.50019   170.65336  5186.3470223 0.0270977\nSouthwest-New England        3059.75554   551.90871  5567.6023746 0.0058329\nRocky Mountains-Plains      -1275.50495 -3783.35178  1232.3418768 0.7745369\nSoutheast-Plains             2599.57175    91.72492  5107.4185757 0.0361922\nSouthwest-Plains             2980.82710   472.98027  5488.6739280 0.0081616\nSoutheast-Rocky Mountains    3875.07670  1367.22987  6382.9235291 0.0001119\nSouthwest-Rocky Mountains    4256.33205  1748.48522  6764.1788814 0.0000135\nSouthwest-Southeast           381.25535 -2126.59148  2889.1021825 0.9997829"
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#results",
    "href": "posts/US_HealthCare_Spending/index.html#results",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "RESULTS",
    "text": "RESULTS\nU.S. Healthcare costs have more than quintupled from 1980 to 2014. There are trends throughout each region that show no clear indication of healthcare spending going down. While some regions are not as expensive as others, they are still growing at the same pace nationally. Following the trend, personal health care spending averaged approximately $10,000 in 1980, but has substantially increased to nearly $80,000 in 2014. The top 3 most expensive regions as of 2014 are the Mideast, Great Lakes, and Far West. The top 3 least cheapest regions as of 2014 are the Rocky Mountains, New England, and Plains. In the Far West region, Oregon ranks 3rd in being the most expensive State in the region."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#conclusion",
    "href": "posts/US_HealthCare_Spending/index.html#conclusion",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nThe United States continues to spend more in their healthcare system. Though, in some categories such as Personal Health Care, you could argue that it is becoming increasingly unaffordable. A $70K difference in 35 years is quite off from inflation expectations. Speaking of inflation, this dataset would have had great potential if inflation adjusted values were given. In that sense, a deeper analysis could be made and more insight would have been gained. Now, one could explore this with even more in depth see if there is any statistical significance between each individual state and their spending habits. That is left and open for anyone to undertake in the future."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#purpose",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#purpose",
    "title": "Resources for Prospective College Students",
    "section": "Purpose:",
    "text": "Purpose:\nHow can we provide a prospective college student with information regarding their potential income and student debt upon embarking on a career path? To address this issue, we have created an interactive tool. Although it is uncomplicated and subject to enhancement, it has enabled us to delve into interactive visualizations through the utilization of R Shiny."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#summary",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#summary",
    "title": "Resources for Prospective College Students",
    "section": "Summary:",
    "text": "Summary:\nThe interactive visualization is comprised of four distinct parts: salary estimator, tuition estimator, debt estimator, and debt calculator. The salary estimator allows prospective students to explore the potential salaries associated with different majors, thereby enabling them to make informed decisions about their future careers. The tuition estimator provides an estimate of the generalized tuition costs associated with pursuing a major in a particular state. The debt estimator calculates the potential four-year degree debt based on the student’s family income, utilizing an average of all students attending each university, and offers insight into the likely debt accumulation over the four years of study. The final tool analyzes the expected length of time a student will remain in debt, based on their chosen major category, and provides a visualization of the projected student debts. Overall, this interactive tool is an invaluable resource for high school students who are considering higher education, offering a comprehensive set of tools to help them make informed decisions about their future."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#shiny-interactive-tool",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#shiny-interactive-tool",
    "title": "Resources for Prospective College Students",
    "section": "Shiny Interactive Tool",
    "text": "Shiny Interactive Tool"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#setup",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#setup",
    "title": "Resources for Prospective College Students",
    "section": "Setup",
    "text": "Setup\n\n\nSetup + Wrangling\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(rsconnect)\nlibrary(colorspace)\nlibrary(plotly)\nlibrary(shinyWidgets)\nlibrary(scales)\nlibrary(ggplot2)\n\n#read the files in from github\nallAgesDf <- read_csv(\"all-ages.csv\")\ntuition_cost <- read_csv(\"tuition_income.csv\")\ntuition <- read_csv(\"tuition_cost.csv\")\nds4<-read_csv(\"salary_and_stats.csv\")\n\n#Wrangling Salary Potential\nsalary <- allAgesDf %>% \n  dplyr::select(Major, P25th, Median, P75th) %>% \n  pivot_longer(c(P25th, Median, P75th),\n               names_to = \"Percentile_Range\", values_to = \"Salary\") %>%\n  arrange(Major) %>%\n  mutate(Percentile_Range = as.factor(Percentile_Range),\n         Major = as.factor(Major))\n\n#Wrangling Potential Tuition Burden\n\n\ntuition_cost <- tuition_cost %>% \n  filter(year == 2018 & net_cost > 0) %>%\n  arrange(name) %>%\n  mutate(income_lvl = as.factor(income_lvl),\n         name = as.factor(name))\n  \n\ntuition_cost$income_lvl <- recode(tuition_cost$income_lvl, \n                                  \"0 to 30,000\" = \"$0 to $30,000\",\n                                  \"30,001 to 48,000\" = \"$30,001 to $48,000\",\n                                  \"48_001 to 75,000\" = \"$48,001 to $75,000\",\n                                  \"75,001 to 110,000\" = \"$75,001 to $110,000\",\n                                  \"Over 110,000\" = \"Over $110,000\")\nsalary$Percentile_Range <- factor(salary$Percentile_Range, levels = c(\"P25th\", \"Median\", \"P75th\"))\nsalary$Percentile_Range <- recode(salary$Percentile_Range, \n                                  \"P25th\" = \"Early Career\",\n                                  \"Median\" = \"Middle Career\",\n                                  \"P75th\" = \"Late Career\")\nsalary$Major <- str_to_title(salary$Major)\nsalary$Major <- gsub(\"And\", \"and\", salary$Major)\n\n\ndf <- tuition %>% \n  group_by(state, degree_length, type) %>% filter(!is.na(state) & degree_length != \"Other\") %>%\n  summarise(room_expenses = mean(room_and_board, na.rm = TRUE),\n            inStateTotal = mean(in_state_total, na.rm = TRUE),\n            outOfStateTotal = mean(out_of_state_total, na.rm = TRUE))\n\ndf$degree_length <- as.factor(df$degree_length)\ndf$type <- as.factor(df$type)\n\ndf <- df %>% rename(\"Room and Board\" = room_expenses,\n              \"In State Tuition\" = inStateTotal,\n              \"Out of State Tuition\" = outOfStateTotal)\n\n\n\n\nColor Theme\n#vars  \n  title = 25\n  subtitle = 20\n  facet_title = 25\n  axis_title = 18\n  tick_numbers = 13\n  title_color = \"black\"\n  background = \"gainsboro\"\n  plot_background = \"gainsboro\"\n  facet_header_background = \"gainsboro\"\n  line_type = \"solid\"\n\nCoreyPlotTheme <- theme(\n    text = element_text(family = \"Futura\"),\n    #background color of page\n    plot.background = element_rect(fill = background),\n    \n    #graph background and grid\n    panel.background = element_blank(),\n    panel.grid.major = element_line(size = .1, linetype = line_type, colour = \"gainsboro\"), \n    panel.grid.minor = element_line(size = .1, linetype = line_type, colour = \"black\"),\n    \n    #title/font/labels\n    plot.title = element_text(color = title_color, size = title,family = \"Futura\",hjust = 0.5),\n    plot.subtitle = element_text(color = title_color, size = subtitle,family = \"Futura\", hjust = 0.5),\n    #plot.caption = element_textbox_simple(halign = 0, size = tick_numbers, maxwidth = 30,family = \"Futura\"),\n    plot.caption = element_text(color = title_color, face = \"bold\", size = tick_numbers, family = \"Futura\", hjust=0),\n    strip.text = element_text(color = title_color,size = facet_title, family = \"Futura\"),\n    strip.background = element_rect(fill = facet_header_background),\n    \n    #tick marks\n    axis.text = element_text(color = title_color, size = tick_numbers, family = \"Futura\"),\n    axis.title = element_text(color = title_color, size = axis_title, family = \"Futura\"),\n    axis.ticks.x = element_blank(),\n    \n    #legend\n    legend.title = element_text(color = title_color,size =subtitle, family = \"Futura\"),\n    legend.background = element_rect(fill = plot_background),\n    legend.text = element_text(size = tick_numbers, family =\"Futura\" )\n  )"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#plot-sidebar-inputs",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#plot-sidebar-inputs",
    "title": "Resources for Prospective College Students",
    "section": "Plot Sidebar Inputs",
    "text": "Plot Sidebar Inputs\n\n\nSalary Estimator Selectors\n#INPUT FOR PLOT 1\n\ninput1 <- inputPanel(\n  selectInput(\"selectInput1\", label = \"Choose your major:\", \n              choices = unique(salary$Major),\n              selected = \"ART HISTORY AND CRITICISM\"),\n  checkboxGroupInput(\"percentile_choice\", label = \"Pick your career level:\", \n                     choices = list(\"Early Career \" = \"Early Career\",\n                                    \"Middle Career \" = \"Middle Career\",\n                                    \"Late Career \" = \"Late Career\"),\n                     selected = c(\"Early Career\", \"Middle Career\", \"Late Career\")),\n)\n\n\n\n\nTuition Estimator Options\n#INPUT FOR PLOT 2\n\ninput2 <- inputPanel(\n  selectInput(\"money\", label = \"Select the type of expense:\",\n              choices = c(\"Room and Board\" = \"Room and Board\",\n                          \"In State Tuition\" = \"In State Tuition\",\n                          \"Out of State Tuition\" = \"Out of State Tuition\"),\n              selected = \"In State Tuition\"),\n  selectInput(\"state\", label = \"Pick your State:\", \n              choices = unique(df$state),\n              selected = \"Oregon\"),\n)\n\n\n\n\nDebt Estimator Levels\n#INPUT FOR PLOT 3\n\ninput3 <- inputPanel(\n  selectInput(\"selectInput2\", \n              label = \"Select your university:\",\n              choices = unique(tuition_cost$name), \n              selected = \"Willamette University\"),\n  checkboxGroupInput(\"checkGroup\", \n                     label = \"Select your household income bracket:\", \n                     choices = list(\"$0 to $30,000\" = \"$0 to $30,000\",\n                                    \"$30,001 to $48,000\" = \"$30,001 to $48,000\",\n                                    \"$48,001 to $75,000\" = \"$48,001 to $75,000\",\n                                    \"$75,001 to $110,000\" = \"$75,001 to $110,000\",\n                                    \"Over $110,000\" = \"Over $110,000\"),\n                               selected = c(\"$0 to $30,000\",\n                                            \"$30,001 to $48,000\",\n                                            \"$48,001 to $75,000\",\n                                            \"$75,001 to $110,000\",\n                                            \"Over $110,000\")),\n)\n\n\n\n\nDebt Calculator Choices\n#INPUT FOR PLOT 4\n\ninput4 <- inputPanel(\n  selectInput(\"major_category\", \n              label = \"Pick a major category:\", \n              choices = unique(ds4$major_category),\n              selected = \"Computers & Mathematics\"),\n)"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#plots",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#plots",
    "title": "Resources for Prospective College Students",
    "section": "Plots",
    "text": "Plots\n\n\nSalary Estimator\n#PLOT1\nplot1 <- renderPlot({\n  salary %>% \n    filter((Major %in% input$selectInput1) & (Percentile_Range %in% input$percentile_choice)) %>% \n    ggplot(aes(x = Percentile_Range, y = Salary, fill = Percentile_Range)) +\n      geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n      geom_label(aes(y = Salary,\n                     label = print(paste0(\"$\", round(Salary/1000, 2), \"K\"))),\n                 show.legend = FALSE,\n                 size = 7,\n                 family = \"Futura\",\n                 fill = \"white\") +\n      scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n      labs(x = NULL,\n           y = NULL,\n           title = paste0(\"Estimated Salary for \", input$selectInput1),\n           caption = \"Source: TuitionTracker.org @ 2018\") + \n      CoreyPlotTheme +\n      scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nTuition Estimator\n#PLOT2\nplot2 <- renderPlot({\n  df %>% filter(state == input$state) %>%\n      ggplot(aes(x = degree_length, y = .data[[input$money]], fill = degree_length)) +\n      geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n      facet_wrap(~type) + \n      geom_label(aes(y = .data[[input$money]],\n                     label = print(paste0(\"$\", round(.data[[input$money]]/1000, 2), \"K\"))),\n                 family = \"Oswald\",\n                 size = 7,\n                 show.legend = FALSE,\n                 fill = \"white\") +\n      scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3),\n                         limits = c(0,55000)) +\n      labs(x = NULL,\n           y = NULL,\n           title = paste0(\"Average \", input$money, \" for \", input$state, \" Universities\"),\n           subtitle = \"For Undergraduate Degrees\",\n           caption = \"Source: TuitionTracker.org @ 2018\") + \n      CoreyPlotTheme +\n      scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nDebt Estimator\n#PLOT3\nplot3 <- renderPlot({\n  tuition_cost %>% \n      filter((income_lvl %in% input$checkGroup) & (name %in% input$selectInput2)) %>%\n      ggplot(aes(x = income_lvl, y = net_cost, fill = income_lvl)) +\n      geom_col(color = \"black\", width = 0.4, position = \"dodge\", show.legend = FALSE) +\n      geom_label(aes(y = net_cost,\n                     label = print(paste0(\"$\", round(net_cost/1000, 2), \"K\"))),\n                 family = \"Oswald\",\n                 size = 7,\n                 show.legend = FALSE,\n                 fill = \"white\") +\n      scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n      labs(x = NULL,\n           y = NULL,\n           title = paste0(\"Median Student Loan Debt for \", input$selectInput2),\n           subtitle = \"After Completing Their Undergraduate Degree\",\n           caption = \"Source: TuitionTracker.org @ 2018\") +\n      CoreyPlotTheme + \n      scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nDebt Calculator\n#PlOT4\nplot4 <- renderPlot({\n  ds4 %>% \n      filter(major_category == input$major_category) %>% \n      ggplot(aes(perfect_payback_period,reorder(major, perfect_payback_period), fill = perfect_payback_period))+\n      geom_col(show.legend = FALSE) +\n      geom_label(aes(label=paste(round(perfect_payback_period,2),\" yrs.\")), \n                 show.legend = FALSE, \n                 fill = \"white\", \n                 hjust = 1.1) +\n      theme(axis.title.y = element_blank(),\n            axis.text.x = element_blank()) +\n      labs(title = 'How Long Will You Be In Debt?',\n           subtitle = \"Based on Your Major\",\n           x = 'Time to pay off loans')+\n      CoreyPlotTheme +\n      theme(plot.title = element_text(hjust = 0.5)) +\n      scale_fill_continuous_sequential(\"PuBuGn\")\n})"
  },
  {
    "objectID": "posts/FancyTables/index.html#purpose",
    "href": "posts/FancyTables/index.html#purpose",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Purpose",
    "text": "Purpose\nThis project centers on the exploration of various techniques to design visually appealing and comprehensible data tables. It is a well-known fact that the conventional format of Excel spreadsheets can be tedious and challenging to read. As such, the project aims to investigate alternative ways of presenting data in tables that are both aesthetically pleasing and easy to understand.\nBy utilizing various design principles such as color theory, typography, and layout, we seek to create data tables that are visually striking and convey information effectively. Additionally, the project involves the evaluation of different software tools and platforms that offer innovative and user-friendly options for creating data tables.\nIt is crucial to recognize that the presentation of data plays a significant role in its interpretation and understanding. The traditional Excel format may not provide sufficient visual cues to highlight key data points or insights. Therefore, this project seeks to address this limitation by exploring new and innovative methods of presenting data tables that are both functional and visually appealing."
  },
  {
    "objectID": "posts/FancyTables/index.html#advanced-data-tables",
    "href": "posts/FancyTables/index.html#advanced-data-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Advanced Data Tables",
    "text": "Advanced Data Tables\n\n\n\n\n\nDT Table Code\nds_starter <- ds %>% \n  mutate(province = as.factor(province),\n         price = price,\n         thetaPointMean = mean(points),\n         thetaPriceMean = mean(price))\n\nds_starter %>% \n    arrange(province, year) %>%\n  select(Province = province, \n         Year = year, \n         Price = price, \n         Points = points, \n         Description = description) %>%\n  datatable(., \n            filter = \"bottom\", \n            extensions = 'Buttons', \n            options = list(dom = 'Bfrtip',\n                           buttons = c('copy', 'csv', 'excel'), \n                           initComplete = JS(\"function(settings, json) {\",\n                                             \"$(this.api().table().header()).css({'background-color': '#131F4F', 'color': '#fff'});\",\n                                             \"}\")))"
  },
  {
    "objectID": "posts/FancyTables/index.html#adding-statistical-visuals-to-summary-tables",
    "href": "posts/FancyTables/index.html#adding-statistical-visuals-to-summary-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Adding Statistical Visuals To Summary Tables",
    "text": "Adding Statistical Visuals To Summary Tables\n\n\nGT Table Code\nfancyTbl <- ds_summary %>%\n  gt() %>%\n# format the numeric output to 3 digit rounding  \n  fmt_number(columns = c(pointsMean, pointsSD, priceMean, priceSD),\n             decimals = 3) %>%\n# create nice labels for a few ugly variable names\n  cols_label(province = \"Province\",\n             pointsMean = \"Avg. Points\",\n             pointsSD = \"Std. Dev. Points\",\n             priceMean = \"Avg. Price\",\n             priceSD = \"Std. Dev. Price\",\n             points = \"Points Trend\",\n             price = \"Price Trend\",) %>%\n# Plot the sparklines from the list column\n  gt_plt_sparkline(points, \n                   type=\"ref_median\", \n                   same_limit = TRUE\n                   ) %>%\n  gt_plt_sparkline(price, \n                   type=\"ref_median\", \n                   same_limit = TRUE\n                   ) %>%\n# use the guardian's table theme\n  gt_theme_guardian() %>% \n# give hulk coloring to the Mean Human Rights Score\n  gt_hulk_col_numeric(pointsMean) %>%\n  gt_hulk_col_numeric(priceMean) %>%\n# create a header and subheader\n  tab_header(title=\"Province Pinot Wine Summary\", subtitle = \"Source: Dr. Hendrick\") %>%\n# attach excel file\n  tab_source_note(excel_file_attachment)\n# save the original as an image\n#gtsave(fancyTbl, \"table.png\")\n# show the table themed in accordance with the page\nfancyTbl\n\n\n\n\n\n\n  \n    \n      Province Pinot Wine Summary\n    \n    \n      Source: Dr. Hendrick\n    \n    \n      Province\n      Avg. Points\n      Std. Dev. Points\n      Avg. Price\n      Std. Dev. Price\n      Points Trend\n      Price Trend\n    \n  \n  \n    Burgundy\n90.438\n2.989\n98.035\n132.856\n          89.0\n          83.0\n    California\n90.517\n2.831\n47.465\n18.553\n          91.0\n          34.0\n    Casablanca_Valley\n86.282\n2.428\n21.107\n11.953\n          87.0\n          30.0\n    Marlborough\n87.550\n2.245\n27.668\n13.833\n          85.0\n          25.0\n    New_York\n87.748\n2.268\n25.679\n9.565\n          88.0\n          35.0\n    Oregon\n89.489\n2.663\n44.856\n20.209\n          90.0\n          22.0\n  \n  \n    \n      \n   Download Excel"
  },
  {
    "objectID": "posts/FancyTables/index.html#conclusion",
    "href": "posts/FancyTables/index.html#conclusion",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Conclusion",
    "text": "Conclusion\nThe objective of this endeavor is to create a thorough and all-encompassing compilation of principles and instructions for crafting data tables that are simple to interpret and comprehend. By employing these guidelines, our aim is to enhance the usability and accessibility of data tables across a broad spectrum of applications and sectors."
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html",
    "href": "posts/PredictPinotWine_ML/index.html",
    "title": "Predicting Pinot Wines From 6 Provinces Through Their Individual Descriptions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)\nwine = read_rds(\"pinot.rds\")"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#feature-engineering",
    "href": "posts/PredictPinotWine_ML/index.html#feature-engineering",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nwine = read_rds(\"pinot.rds\") \n\nwine_words <- function(df, j, stem = T){ \n  data(stop_words)\n  words <- df %>%\n    unnest_tokens(word, description) %>%\n    anti_join(stop_words) %>%\n    filter(str_detect(string = word, pattern = \"[a-z+]\")) %>% # get rid weird non alphas \n    filter(str_length(word) >= 3) %>% # get rid of strings shorter than 3 characters \n    filter(!(word %in% c(\"wine\",\"pinot\", \"vineyard\"))) %>%\n    group_by(word) %>%\n    mutate(total=n()) %>%\n    ungroup()\n  \n  if(stem){\n    words <- words %>% \n      mutate(word = wordStem(word))\n  }\n  \n  words <- words %>% \n    count(id, word) %>% \n    group_by(id) %>% \n    mutate(exists = (n>0)) %>% \n    ungroup %>% \n    group_by(word) %>% \n    mutate(total = sum(n)) %>% \n    filter(total > j) %>% \n    pivot_wider(id_cols = id,\n                names_from = word,\n                values_from = exists,\n                values_fill = list(exists=0)) %>% \n    right_join(select(df,id,province)) %>% \n    select(-id) %>% \n    mutate(across(-province, ~replace_na(.x, F)))\n}\n\nwino <- wine_words(wine, j = 190, stem = T)\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(id)`"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#specification",
    "href": "posts/PredictPinotWine_ML/index.html#specification",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Specification",
    "text": "Specification\n\nset.seed(504) \n\nctrl <- trainControl(method = \"cv\", number = 3)\n\n\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\ntest <- wino[-wine_index, ]\n\nfit <- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 100,\n             tuneLength = 15,\n             nodesize = 10,\n             verbose = TRUE,\n             trControl = ctrl,\n             metric = \"Kappa\")"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#re-fit-and-evaluation",
    "href": "posts/PredictPinotWine_ML/index.html#re-fit-and-evaluation",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Re-fit and evaluation",
    "text": "Re-fit and evaluation\n\nset.seed(1504)\n\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\ntest <- wino[-wine_index, ]\n\n# example spec for knn\nfit_final <- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             tuneGrid = fit$bestTune) \n# The last line means we will fit a model using the best tune parameters your CV found above."
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#final-model-performance",
    "href": "posts/PredictPinotWine_ML/index.html#final-model-performance",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Final Model Performance",
    "text": "Final Model Performance\n\nconfusionMatrix(predict(fit_final, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               219          7                 0           0        0\n  California               7        752                12          13       20\n  Casablanca_Valley        0          0                12           0        0\n  Marlborough              0          1                 0          22        0\n  New_York                 0          0                 0           0        1\n  Oregon                  12         31                 2          10        5\n                   Reference\nPrediction          Oregon\n  Burgundy               7\n  California            55\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               485\n\nOverall Statistics\n                                          \n               Accuracy : 0.8912          \n                 95% CI : (0.8753, 0.9057)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8274          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9202            0.9507                 0.461538\nSpecificity                   0.9902            0.8787                 1.000000\nPos Pred Value                0.9399            0.8754                 1.000000\nNeg Pred Value                0.9868            0.9521                 0.991571\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1309            0.4495                 0.007173\nDetection Prevalence          0.1393            0.5134                 0.007173\nBalanced Accuracy             0.9552            0.9147                 0.730769\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                     0.48889       0.0384615        0.8867\nSpecificity                     0.99939       1.0000000        0.9467\nPos Pred Value                  0.95652       1.0000000        0.8899\nNeg Pred Value                  0.98606       0.9850478        0.9450\nPrevalence                      0.02690       0.0155409        0.3270\nDetection Rate                  0.01315       0.0005977        0.2899\nDetection Prevalence            0.01375       0.0005977        0.3258\nBalanced Accuracy               0.74414       0.5192308        0.9167"
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#background",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#background",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Background",
    "text": "Background\nEchinostoma trivolvis is a species of trematode parasite that commonly infects birds, mammals, and some reptiles. It is part of the family Echinostomatidae, which includes several other species of trematodes that infect animals and humans. Echinostoma trivolvis has a complex life cycle that involves multiple hosts, including snails, birds, and mammals. The adult worms inhabit the small intestine of their hosts, where they feed on blood and nutrients. Infection with Echinostoma trivolvis can cause a range of symptoms, including abdominal pain, diarrhea, and malnutrition. The parasite is found throughout North America, particularly in wetlands and other aquatic environments where its intermediate hosts, freshwater snails, are abundant."
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#echinostoma-trivolvis-vs.-pesticides",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#echinostoma-trivolvis-vs.-pesticides",
    "title": "Does Temperature Kill Parasites Faster? Yes!",
    "section": "Echinostoma trivolvis Vs. Pesticides",
    "text": "Echinostoma trivolvis Vs. Pesticides"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html",
    "href": "posts/GymRat_Plotly/index.html",
    "title": "Do You Like Stretching? I would Reconsider!",
    "section": "",
    "text": "Stretching is not a good method of building muscle for several reasons. First, stretching is primarily focused on increasing flexibility and range of motion, rather than building muscle mass or strength. While stretching can help prepare the muscles for exercise and prevent injury, it is not a sufficient method of building muscle. Second, stretching alone does not provide enough resistance or tension on the muscles to stimulate muscle growth. To build muscle, the body needs to be challenged with weight or resistance training. Finally, stretching may actually reduce muscle strength by decreasing muscle activation and power output. While stretching can be a useful addition to a muscle-building routine, it should not be relied upon as the primary method for building muscle mass and strength."
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#general-setup",
    "href": "posts/GymRat_Plotly/index.html#general-setup",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "General Setup",
    "text": "General Setup\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(extrafont)\nlibrary(plotly)\nlibrary(htmlwidgets)\n#font_import()\nloadfonts()"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#data-wrangling-for-plot-1",
    "href": "posts/GymRat_Plotly/index.html#data-wrangling-for-plot-1",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Data Wrangling For Plot 1",
    "text": "Data Wrangling For Plot 1\n\ngymDs <- read_csv(\"megaGymDataset.csv\")\n\n#head(gymDs, 5)\n#names(gymDs)\n\nds <- gymDs %>%\n  mutate(ID = ...1,\n         Level = factor(Level, levels = c(\"Beginner\", \"Intermediate\", \"Expert\")),\n         Type = as.factor(Type)) %>%\n  select(-...1) %>% \n  drop_na()"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#using-plotly-for-plot-1",
    "href": "posts/GymRat_Plotly/index.html#using-plotly-for-plot-1",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Using Plotly for Plot 1",
    "text": "Using Plotly for Plot 1\n\nmyTheme <- theme(text = element_text(family = \"Futura Medium\"),\n                 plot.margin = margin(0.5, 0.5, 0.5, 0.5, unit = \"cm\"),\n                 plot.title = element_text(size = 15, family = \"Futura Condensed ExtraBold\"),\n                 plot.subtitle = element_text(size = 10, family = \"Futura Medium\"),\n                 strip.text.y = element_text(angle = 0, size = 10, family = \"Futura Condensed ExtraBold\"),\n                 strip.placement = \"inside\",\n                 axis.title.x = element_text(margin = margin(t = 0.5, b = 0.5, unit = \"cm\")),\n                 axis.title.y = element_blank(),\n                 axis.text = element_text(size = 9),\n                 legend.position = \"none\",\n                 panel.grid.major.y = element_blank())\n\n\nplotDs <- ds %>% \n  group_by(Type, Level) %>%\n  summarize(meanRating = mean(Rating)) %>%\n  arrange(Type, meanRating) %>%\n  ungroup()\n\np <- plotDs %>%\n  highlight_key(., ~reorder_within(Type, meanRating, Level)) %>%\n  ggplot(aes(x = meanRating, \n             y = reorder_within(Type, meanRating, Level),\n             fill = fct_reorder(Type, meanRating),\n             text = paste0(\"Rating: \", round(meanRating,2),\n                           \"<br>Type: \", Type))) +\n  geom_col(color = \"black\") + \n  facet_grid(rows = vars(Level), \n             scales = \"free_y\", \n             switch = \"y\", \n             space = \"free_y\") +\n  scale_y_reordered() +\n  scale_fill_brewer(palette = \"PuBuGn\") +\n  labs(title = \"Ranking Exercise Type According to Experience Level of Individuals\",\n       x = \"Average Rating of Each Exercise Type\",\n       fill = \"Workout Types\") + \n  theme_minimal() +\n  myTheme \n\n\nggplotly(p, tooltip = \"text\") %>%\n  config(displayModeBar = FALSE) %>%\n  highlight(on = \"plotly_hover\", off = \"plotly_doubleclick\") %>%\n  layout(\n    uniformtext=list(minsize=8, mode='hide'),\n    margin = list(b = 70, l = 140, r = 140)\n  )"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#background",
    "href": "posts/GymRat_Plotly/index.html#background",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Background",
    "text": "Background\nStretching is not a good method of building muscle for several reasons. First, stretching is primarily focused on increasing flexibility and range of motion, rather than building muscle mass or strength. While stretching can help prepare the muscles for exercise and prevent injury, it is not a sufficient method of building muscle. Second, stretching alone does not provide enough resistance or tension on the muscles to stimulate muscle growth. To build muscle, the body needs to be challenged with weight or resistance training. Finally, stretching may actually reduce muscle strength by decreasing muscle activation and power output. While stretching can be a useful addition to a muscle-building routine, it should not be relied upon as the primary method for building muscle mass and strength."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Background\nI have a background in retail, with experience in customer service, sales, and inventory management. However, I am passionate about data and its potential to drive growth and inform business decisions. Through coursework in data science and proficiency in Python, R, and SQL, I have completed projects in data cleaning, exploratory data analysis, and predictive modeling. My retail experience taught me the importance of understanding customer behavior and market trends, which can be enhanced by data-driven approaches. I am eager to apply my skills to a career in data science and contribute to innovative organizations that value data-driven decision-making.\n\n\nGoal\nAs a Data Scientist, I aim to contribute to an innovative organization through my skills in statistical analysis, machine learning, and data visualization. I seek a challenging role where I can solve complex problems and drive business growth by applying my proficiency in Python, R, and SQL. With a strong academic background and practical experience, I am eager to continue learning from experienced professionals in the industry.\n\n\nEducation\nWillamette University, Salem, Oregon, United States | Master of Science in Data Science | August 2022 - August 2023\nLinfield University, McMinnville, Oregon, United States | Bachelor of Arts in Mathematics | August 2018 - May 2022\n\n\nProfessional Experience\nThe North Face, a VF Company | Retail Specialist | Oct 2019 - June 2021, Oct 2022 - present\n\nDeveloped strong skills in active listening, problem-solving, and conflict resolution\nProficient in suggestive selling, upselling, and cross-selling; experienced in closing sales effectively and processing transactions accurately and efficiently\nIn-depth knowledge of product features, benefits, and intended uses; can explain technical details in a clear and concise manner\nExperienced in receiving, organizing, and stocking products; skilled in tracking inventory levels and identifying when reorders are necessary; can arrange products in an aesthetically pleasing and organized manner\nAble to work collaboratively with others; comfortable with delegating tasks, taking direction, and providing constructive feedback; can work independently and take initiative when necessary\nExcellent verbal and written communication skills; able to communicate effectively with customers, team members, and management; comfortable with public speaking and presenting information to groups\n\n\n\nLanguages\n\nBilingual; Proficient in navigating cultural differences with sensitivity and respect in both English and Spanish-speaking communities.\nBasic proficiency in French verbal and written communication, including introducing oneself, basic conversations, and writing simple sentences and paragraphs."
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html",
    "href": "posts/US_HealthIns_Costs/index.html",
    "title": "U.S. Medical Insurance Costs",
    "section": "",
    "text": "The focal point of this project pertains to the medical insurance industry. I showcase my proficiency in creating compelling visualizations and conducting statistical analyses on a refined data set. Prior to delving into the data set, I formulated several key questions to guide my investigation. These inquiries included identifying the principal driver of heightened insurance costs, evaluating the variables that contribute to rising or falling medical insurance expenses, and identifying the optimal individual profile that minimizes medical insurance costs.\nOf particular interest in this study was the analysis of smoking habits as a variable that may have a significant impact on medical insurance expenses. Moreover, a detailed exploration of the disparities in insurance costs between male and female individuals, both smokers and non-smokers, was conducted.\nTo accomplish these objectives, advanced analytical tools and methodologies were utilized. Rigorous data cleaning and wrangling were carried out to ensure the accuracy and integrity of the data. Subsequently, advanced statistical techniques such as linear regression and hypothesis testing were utilized to extract meaningful insights from the data.\nUltimately, this project provides valuable insights into the medical insurance industry and serves as a testament to the power of data analytics in unlocking actionable insights that can drive informed decision-making.\nIf you wish to skip towards the results portion, feel free to scroll down and have a quick read! Interesting results were found.\n\n\n\n\n   age     sex     bmi  children smoker     region      charges\n0   19  female  27.900         0    yes  southwest  16884.92400\n1   18    male  33.770         1     no  southeast   1725.55230\n2   28    male  33.000         3     no  southeast   4449.46200\n3   33    male  22.705         0     no  northwest  21984.47061\n4   32    male  28.880         0     no  northwest   3866.85520\n\n\n\n\n\n\nWe want to know what influences the cost of insurance. What causes an individual to pay more for their insurance? We can infer that being a smoker will drastically increase their out of pocket medical insurance expenses.\nI take a look into how many smokers we have in this data set. Then, I take a quick look into the smokers’ distribution of charges and show a boxplot to visualize the insurance cost quartiles. Next, I dive into using some statistics such as finding the min, max, mean, median, etc. I follow up by seperating male and female smokers and seeing if there’s a drastic change in their expenses.\n\n\n\nSmoker Count: 274\nNonsmoker Count: 1064\n\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nThe interquartile range is $20,125.33. Very high spread from the first and third interquartiles.\nNow, notice that we have a bimodal distribution. This means that there is a variable in the data set that is drastically affecting the insurance cost. Since there is two “humps” we must identify the component that causes a further jump in insurance costs for those who smoke.\nWhat could cause this? - Region? - Bmi? - Sex? - Children? - Age?\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nSummary Stats: - Mean: $32,050.23 - Median: $34,456.35 - Max: $63,770.43 - Min: $12,829.46 - Standard Deviation: $11,520.47 - Variance: 132721153.14\nGiven the distribution, we can say that the mean represents the true average insurance cost for smokers.\n\n\n\nI chose to seperate male and female smokers to see if there was a noticable difference in cost. Notice in the graphs below, that we can easily identify the culprit for the bimodal distribution. The BMI drastically influences the insurance cost when it is greater than or equal to 30 (the obese rating starts at 30 forward). This means that being obese AND being a smoker can sharply increase your medical insurance cost.\nNote: As Age slowly increases, the insurance cost also slowly increases. So, there is a gradual increase in insurance cost as you age. That is common sense since as one ages, one develops more health issues as they near death.\n\n\n\nGraph One: Cost Distribution - Bimodal Distribution is again present. - Two clusters of cost are due to an underlying variable.\nGraph Two: Barplot of Average Cost per Child - No clear trend. - Only valuable note is that having 4 children seems to have an overall lower average insurance cost.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is a visable linear relationship between BMI and Cost. - There are two clusters of individuals but follows the linear relationship. - At >=30 BMI, there is a clear signifcant increase in insurance cost for male smokers. - Note: a BMI of 30 or implies that the individual is “obese” (though, BMI is not necessarily a good measurement–see results). - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - There is a linear trend, but split into two linear clusters. - As found earlier, this is most likely due to BMI differences in the indivduals. - Overtime, an individual’s insurance cost will increase as expected.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nAs mentioned above, the reason why the cost distribution had two clusters was due to the difference in BMI in each male smoker. Male smokers with a BMI >= 30 will that their expense increase by approximately 47%.\n\n\nText(0, 0.5, 'Insurance Costs (USD)')\n\n\n\n\n\n\n\n\nGraph One: Cost Distribution - Bimodal Distribution is again present. - Two clusters of cost are due to an underlying variable.\nGraph Two: Barplot of Average Cost per Child - No clear trend. - Only valuable note is that having 5 children seems to have an overall lower average insurance cost.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is a visable linear relationship between BMI and Cost - There are two clusters of individuals but follows the linear relationship. - At >=30 BMI, there is a clear signifcant increase in insurance cost for female smokers. - Note: a BMI of 30 or implies that the individual is “obese” (though, BMI is not necessarily a good measurement–see results). - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - There is a linear trend, but split into two linear clusters. - As found earlier, this is most likely due to BMI differences in the indivduals. - Overtime, an individual’s insurance cost will increase as expected.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn.\nGraph Six: Boxplot of Female Smoker Insurance Cost - Noteworthy to see the spread of female smokers’ insurance cost.\nGiven that the Insurane Cost vs BMI is essentially the same as male smokers, the stated results will apply here.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nNoteworthy mention: - Male smokers pay approximately 1.34% more than their female counterparts when their BMI less than 30. - Male smokers pay approximately 1.42% less than their female counterparts when their BMI greater than or equal to 30.\n\n\n\n\n\n<function matplotlib.pyplot.clf()>\n\n\n\n\n\nThe interquartile range is $7,378.07. Spread is minimal from the first and third interquartiles.\nNow, notice that we have a right-skewed distribution.\nWhat could cause this? - Region? - Bmi? - Sex? - Children? - Age?\nKeep in mind, there is a chance that we may not be able to determine the cause of those outliers.\n\n\n7378.07\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\nNow let’s look at the nonsmokers.\nSummary Stats: - Mean: $8,434.27 - Median: $7,345.41 - Max: $39,910.61 - Min: $1,121.87 - Standard Deviation: $5,990.96 - Variance: 35891656\n\n\nGraph One: Cost Distribution - Right-skewed distribution is again present. - Knowing this, we should go with the median as the most appropriate statistic instead of the mean.\nGraph Two: Barplot of Average Cost per Child - There seems to be a trend that having more children will lead to higher average insurance costs. - Given the information from the smokers, when an individual has 5 children or more, their insurance costs drops signifcantly. - However, the lowest average comes from having no children at all.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is no correlation between cost and BMI of nonsmokers. - This is an interesting finding. - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - Strong linear relationship between insurance cost and age. - A secondary trend is shown with the number of children. - The less children, the insurance cost is on the lower end of the linear relationship. - The more children, the insurance cost is on the higher end of the linear relationship - The outliers do not follow the trend line.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn. - Some regions are simply cheaper than others.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nGraph One: Cost Distribution - Right-skewed distribution is again present. - Knowing this, we should go with the median as the most appropriate statistic instead of the mean.\nGraph Two: Barplot of Average Cost per Child - Similar to the male nonsmokers, there seems to be a trend that having more children will result in higher average insurance costs. - Given the information from the smokers, when an individual has 5 children or more, their insurance costs drops on average. - However, the lowest average comes from having no children at all.\nGraph Three: Scatterplot of Cost vs BMI with Region - Again, there is no correlation between cost and BMI of nonsmokers. - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - Strong linear relationship between insurance cost and age. - A secondary trend is shown with the number of children. - The less children, the insurance cost is on the lower end of the linear relationship. - The more children, the insurance cost is on the higher end of the linear relationship - The outliers do not follow the trend line.\nGraph Five: Barplot of Average Cost per Region - There is a slight trend of region importance. - The northeast is more expensive on average than the southwest.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#purpose-of-this-project",
    "href": "posts/US_HealthIns_Costs/index.html#purpose-of-this-project",
    "title": "U.S. Medical Insurance Costs",
    "section": "Purpose of this Project",
    "text": "Purpose of this Project\nThe focal point of this project pertains to the medical insurance industry. I showcase my proficiency in creating compelling visualizations and conducting statistical analyses on a refined data set. Prior to delving into the data set, I formulated several key questions to guide my investigation. These inquiries included identifying the principal driver of heightened insurance costs, evaluating the variables that contribute to rising or falling medical insurance expenses, and identifying the optimal individual profile that minimizes medical insurance costs.\nOf particular interest in this study was the analysis of smoking habits as a variable that may have a significant impact on medical insurance expenses. Moreover, a detailed exploration of the disparities in insurance costs between male and female individuals, both smokers and non-smokers, was conducted.\nTo accomplish these objectives, advanced analytical tools and methodologies were utilized. Rigorous data cleaning and wrangling were carried out to ensure the accuracy and integrity of the data. Subsequently, advanced statistical techniques such as linear regression and hypothesis testing were utilized to extract meaningful insights from the data.\nUltimately, this project provides valuable insights into the medical insurance industry and serves as a testament to the power of data analytics in unlocking actionable insights that can drive informed decision-making.\nIf you wish to skip towards the results portion, feel free to scroll down and have a quick read! Interesting results were found.\n\nSet Up and Importing the Data\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n#read in the csv file\ndf = pd.read_csv('insurance.csv')\nprint(df.head())\n\n   age     sex     bmi  children smoker     region      charges\n0   19  female  27.900         0    yes  southwest  16884.92400\n1   18    male  33.770         1     no  southeast   1725.55230\n2   28    male  33.000         3     no  southeast   4449.46200\n3   33    male  22.705         0     no  northwest  21984.47061\n4   32    male  28.880         0     no  northwest   3866.85520"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#analyzing-smoker-insurance-costs",
    "href": "posts/US_HealthIns_Costs/index.html#analyzing-smoker-insurance-costs",
    "title": "U.S. Medical Insurance Costs",
    "section": "Analyzing Smoker Insurance Costs",
    "text": "Analyzing Smoker Insurance Costs\nWe want to know what influences the cost of insurance. What causes an individual to pay more for their insurance? We can infer that being a smoker will drastically increase their out of pocket medical insurance expenses.\nI take a look into how many smokers we have in this data set. Then, I take a quick look into the smokers’ distribution of charges and show a boxplot to visualize the insurance cost quartiles. Next, I dive into using some statistics such as finding the min, max, mean, median, etc. I follow up by seperating male and female smokers and seeing if there’s a drastic change in their expenses.\n\nHistogram Plot of Smokers vs Nonsmokers\n\nSmoker Count: 274\nNonsmoker Count: 1064\n\n\n#while a histogram is not necessary, it can really show in the visual count of smokers vs nonsmokers\nplt.figure(figsize = (8, 6))\nsns.set(font_scale = 1.5)\nsns.histplot(df.smoker)\nplt.title('Smoker vs Nonsmoker Count')\n\nplt.tight_layout(pad = 2)\nplt.show()\nplt.clf()\n\n#num_smokers = df.smoker.value_counts()['yes']\n#num_nonsmokers = df.smoker.value_counts()['no']\n#print(\"Exact number of smokers: {num_smokers}\".format(num_smokers = num_smokers))\n#print(\"Exact number of nonsmokers: {num_nonsmokers}\".format(num_nonsmokers = num_nonsmokers))\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nBoxplot and Histogram Distribution of Insurance Costs for Smokers\nThe interquartile range is $20,125.33. Very high spread from the first and third interquartiles.\nNow, notice that we have a bimodal distribution. This means that there is a variable in the data set that is drastically affecting the insurance cost. Since there is two “humps” we must identify the component that causes a further jump in insurance costs for those who smoke.\nWhat could cause this? - Region? - Bmi? - Sex? - Children? - Age?\n\ndf_smokers = df[df.smoker == 'yes']\n#print(df_smokers.head())\nsns.set(font_scale = 0.8)\nIQR = stats.iqr(df_smokers.charges, interpolation = 'midpoint')\n#print(round(IQR,2))\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,6))\n#boxplot\nsns.boxplot(y = df_smokers.charges, ax = ax1)\nax1.set_ylabel('Insurance cost (USD)')\nax1.set_title('Insurance Cost Boxplot for Smokers')\n\n#histogram\nsns.histplot(df_smokers.charges, bins = 40, ax = ax2)\nax2.set_xlabel('Insurance Cost (USD)')\nax2.set_title('Insurance Cost Distribution for Smokers')\n\nplt.tight_layout(pad = 3)\n\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nInsurance Cost Statistics for Smokers\nSummary Stats: - Mean: $32,050.23 - Median: $34,456.35 - Max: $63,770.43 - Min: $12,829.46 - Standard Deviation: $11,520.47 - Variance: 132721153.14\nGiven the distribution, we can say that the mean represents the true average insurance cost for smokers.\n\n#find mean of insurance cost for smokers\nsmoker_mean = round(np.mean(df_smokers.charges), 2)\n#print('The average insurance cost for being a smoker: ${mean}'.format(mean = smoker_mean))\n\n#find median of insurance cost for smokers\nsmoker_median = round(np.median(df_smokers.charges), 2)\n#print('The median insurance cost for being a smoker: ${median}'.format(median = smoker_median))\n\n#find standard deviation...\nsmoker_std = round(np.std(df_smokers.charges),2)\n#print('The standard deviation is: ${std}'.format(std = smoker_std))\n\n#let's see the variance\nsmoker_var = round(np.var(df_smokers.charges),2)\n#print('The variance is: {var}'.format(var = smoker_var))\n\n#maximum cost\nsmoker_max = round(np.max(df_smokers.charges), 2)\n#print('The maximum insurance cost for being a smoker is: ${max}'.format(max = smoker_max))\n\n#minimum cost\nsmoker_min = round(np.min(df_smokers.charges), 2)\n#print('The minimum insurance cost for being a smoker is: ${min}'.format(min = smoker_min))\n\n\n\nSeperating Male and Female Smokers\nI chose to seperate male and female smokers to see if there was a noticable difference in cost. Notice in the graphs below, that we can easily identify the culprit for the bimodal distribution. The BMI drastically influences the insurance cost when it is greater than or equal to 30 (the obese rating starts at 30 forward). This means that being obese AND being a smoker can sharply increase your medical insurance cost.\nNote: As Age slowly increases, the insurance cost also slowly increases. So, there is a gradual increase in insurance cost as you age. That is common sense since as one ages, one develops more health issues as they near death.\n\n\nAnalyzing Male Smokers\nGraph One: Cost Distribution - Bimodal Distribution is again present. - Two clusters of cost are due to an underlying variable.\nGraph Two: Barplot of Average Cost per Child - No clear trend. - Only valuable note is that having 4 children seems to have an overall lower average insurance cost.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is a visable linear relationship between BMI and Cost. - There are two clusters of individuals but follows the linear relationship. - At >=30 BMI, there is a clear signifcant increase in insurance cost for male smokers. - Note: a BMI of 30 or implies that the individual is “obese” (though, BMI is not necessarily a good measurement–see results). - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - There is a linear trend, but split into two linear clusters. - As found earlier, this is most likely due to BMI differences in the indivduals. - Overtime, an individual’s insurance cost will increase as expected.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n#seperate all male smokers from the main data set\ndf_male_smoker = df_smokers[df_smokers.sex == 'male']\ndf_nonsmokers = df[df.smoker == 'no']\n#print(df_nonsmokers.head())\n#print(df_male_smoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male smoker\nsns.histplot(df_male_smoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Male Smoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of male smokers per child\ndf_children = df_male_smoker.groupby('children').mean().reset_index()\n#print(df_children.head())\n\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Male Smoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of male smokers by BMI\nsns.scatterplot(x = df_male_smoker.bmi, y = df_male_smoker.charges,\n                hue = df_male_smoker.region,\n                size = df_male_smoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Male Smoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of male smokers by age\nsns.scatterplot(x = df_male_smoker.age, y = df_male_smoker.charges,\n                hue = df_male_smoker.children,\n                size = df_male_smoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Male Smoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of male smokers per region\ndf_region = df_male_smoker.groupby('region').mean().reset_index()\n#print(df_region.head())\n\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Male Smoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of male smoker boxplot\nsns.boxplot(x = df_male_smoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Smoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\n\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nBMI Graph Highlighted\nAs mentioned above, the reason why the cost distribution had two clusters was due to the difference in BMI in each male smoker. Male smokers with a BMI >= 30 will that their expense increase by approximately 47%.\n\n#highlight BMI graph\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = df_male_smoker.bmi, y = df_male_smoker.charges,\n                hue = df_male_smoker.region,\n                size = df_male_smoker.region,\n                sizes = (7.5, 25))\nplt.axvline(x = 30, color = 'gray', label = 'Obese BMI')\nplt.title('Male Smoker Insurance Cost vs BMI')\nplt.xlabel('BMI')\nplt.ylabel('Insurance Costs (USD)')\n\nText(0, 0.5, 'Insurance Costs (USD)')\n\n\n\n\n\n\n#analyze BMI < 30\n\nmale_df_bmi = df_male_smoker[df_male_smoker.bmi < 30]\nmale_avg_cost = round(np.mean(male_df_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for male smokers with a BMI of less than 30 equates to: ${charge}\".format(charge = male_avg_cost))\n\n#analyze BMI >= 30\n\nmale_df2_bmi = df_male_smoker[df_male_smoker.bmi >= 30]\nmale_avg_cost_2 = round(np.mean(male_df2_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for male smokers with a BMI of greater than or equal to 30 equates to: ${charge}\".format(charge = male_avg_cost_2))\n\n#Difference of Insurance Cost\nmale_diff_cost = male_avg_cost_2 - male_avg_cost\n#print(\"The difference in insurance cost from being at or over 30 BMI compared to being under 30 BMI for males is: ${diff}\".format(diff = male_diff_cost))\n\nmale_diff_cost_percent = 100*round(male_diff_cost / male_avg_cost_2, 2)\n#print('A male smoker will suffer an increase of {percent}% in their insurance cost by being obese (BMI >= 30)'.format(percent = male_diff_cost_percent))\n\n\n\nAnalyzing Female Smokers\nGraph One: Cost Distribution - Bimodal Distribution is again present. - Two clusters of cost are due to an underlying variable.\nGraph Two: Barplot of Average Cost per Child - No clear trend. - Only valuable note is that having 5 children seems to have an overall lower average insurance cost.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is a visable linear relationship between BMI and Cost - There are two clusters of individuals but follows the linear relationship. - At >=30 BMI, there is a clear signifcant increase in insurance cost for female smokers. - Note: a BMI of 30 or implies that the individual is “obese” (though, BMI is not necessarily a good measurement–see results). - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - There is a linear trend, but split into two linear clusters. - As found earlier, this is most likely due to BMI differences in the indivduals. - Overtime, an individual’s insurance cost will increase as expected.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn.\nGraph Six: Boxplot of Female Smoker Insurance Cost - Noteworthy to see the spread of female smokers’ insurance cost.\nGiven that the Insurane Cost vs BMI is essentially the same as male smokers, the stated results will apply here.\n\n#seperate all female smokers from the main data set\ndf_female_smoker = df_smokers[df_smokers.sex == 'female']\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male smoker\nsns.histplot(df_female_smoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Female Smoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of female smokers per child\ndf_children = df_female_smoker.groupby('children').mean().reset_index()\n#print(df_children.head())\n\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Female Smoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of female smokers by BMI\nsns.scatterplot(x = df_female_smoker.bmi, y = df_female_smoker.charges,\n                hue = df_female_smoker.region,\n                size = df_female_smoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Female Smoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of female smokers by age\nsns.scatterplot(x = df_female_smoker.age, y = df_female_smoker.charges,\n                hue = df_female_smoker.children,\n                size = df_female_smoker.children,\n                sizes = (12.5, 25), ax = ax[1,0])\nax[1,0].set_title('Female Smoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of female smokers per region\ndf_region = df_female_smoker.groupby('region').mean().reset_index()\n#print(df_region.head())\n\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average. Female Smoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of female smoker age vs bmi\nsns.boxplot(x = df_female_smoker.charges, ax = ax[1,2])\nax[1,2].set_title('Female Smoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n#analyze BMI < 30\n\nfemale_df_bmi = df_female_smoker[df_female_smoker.bmi < 30]\nfemale_avg_cost = round(np.mean(female_df_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for female smokers with a BMI of less than 30 equates to: ${charge}\".format(charge = female_avg_cost))\n\n#analyze BMI >= 30\n\nfemale_df2_bmi = df_female_smoker[df_female_smoker.bmi >= 30]\nfemale_avg_cost_2 = round(np.mean(female_df2_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for female smokers with a BMI of greater than or equal to 30 equates to: ${charge}\".format(charge = female_avg_cost_2))\n\n#Difference of Insurance Cost\nfemale_diff_cost = female_avg_cost_2 - female_avg_cost\n#print(\"The difference in cost from being at or over 30 BMI compared to being under 30 BMI for females is: ${diff}\".format(diff = female_diff_cost))\n\n\n\nAverage Insurance Cost Comparision of Male and Female Smokers\nNoteworthy mention: - Male smokers pay approximately 1.34% more than their female counterparts when their BMI less than 30. - Male smokers pay approximately 1.42% less than their female counterparts when their BMI greater than or equal to 30.\n\n#Plot average insurance cost for male and female \ndf_bmi_less_than_30 = df_smokers[df_smokers.bmi < 30]\ndf_one = df_bmi_less_than_30.groupby('sex').mean().reset_index()\n\n#Get value\nmale_bmi_less_than_30 = df_one[df_one.sex == 'male'].charges.sum()\nfemale_bmi_less_than_30 = df_one[df_one.sex == 'female'].charges.sum()\n#print value\n\nmale_female_sum = male_bmi_less_than_30 + female_bmi_less_than_30\npor_male_less_30 = male_bmi_less_than_30 / male_female_sum\npor_female_less_30 = female_bmi_less_than_30 / male_female_sum\n\n\ndiff_por = round((por_male_less_30 - por_female_less_30), 4) * 100\n#print(\"Male smokers are expected to pay approximately {diff_por}% more than their female counterparts when their BMI < 30 \".format(diff_por = diff_por))\n\ndf_bmi_greater_than_or_equal_30 = df_smokers[df_smokers.bmi >= 30]\ndf_two = df_bmi_greater_than_or_equal_30.groupby('sex').mean().reset_index()\n\nmale_bmi_greq_30 = df_two[df_two.sex == 'male'].charges.sum()\nfemale_bmi_greq_30 = df_two[df_two.sex == 'female'].charges.sum()\n\n\nmale_female_sum = male_bmi_greq_30 + female_bmi_greq_30\npor_male_greq_30 = male_bmi_greq_30 / male_female_sum\npor_female_greq_30 = female_bmi_greq_30 / male_female_sum\n\ndiff_por_greq = round((por_male_greq_30 - por_female_greq_30), 4) * 100\ndiff_por_greq = abs(round(diff_por_greq, 4))\n#print(\"Male smokers are expected to pay approximately {diff_por_greq}% less than their female counterparts when their BMI >= 30\".format(diff_por_greq = diff_por_greq))\n\n#print(df_one)\n#print(df_two)\n#df3 = pd.concat([df_one, df_two])\n#print(df3)\n\nfig, ax = plt.subplots(1,2,figsize=(8,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.8)\n\n#BMI is less than 30\nsns.set_palette('bright')\nsns.barplot(x = df_one.sex, y = df_one.charges, ax = ax[0])\nax[0].set_xlabel('Sex')\nax[0].set_ylabel('Insurance Cost (USD)')\nax[0].set_title('Average Smoker Insurance Cost with BMI < 30')\n\n\n#BMI is greater than or equal to 30\nsns.set_palette('dark')\nsns.barplot(x = df_two.sex, y = df_two.charges, ax = ax[1])\nax[1].set_xlabel('Sex')\nax[1].set_ylabel('Insurance Cost (USD)')\nax[1].set_title('Average Smoker Insurance Cost with BMI >= 30')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf\n\n\n\n\n<function matplotlib.pyplot.clf()>\n\n\n\n\nBoxplot and Histogram Distribution of Insurance Costs for Nonsmokers\nThe interquartile range is $7,378.07. Spread is minimal from the first and third interquartiles.\nNow, notice that we have a right-skewed distribution.\nWhat could cause this? - Region? - Bmi? - Sex? - Children? - Age?\nKeep in mind, there is a chance that we may not be able to determine the cause of those outliers.\n\nIQR = stats.iqr(df_nonsmokers.charges, interpolation = 'midpoint')\nIQR = round(IQR, 2)\nprint(IQR)\n\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,6))\n#boxplot\nsns.boxplot(y = df_nonsmokers.charges, ax = ax1)\nax1.set_ylabel('Insurance cost (USD)')\nax1.set_title('Insurance Cost Boxplot for Nonsmokers')\n\n#histogram\nsns.histplot(df_nonsmokers.charges, bins = 40, ax = ax2)\nax2.set_xlabel('Insurance Cost (USD)')\nax2.set_title('Insurance Cost Distribution for Nonsmokers')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n#right skewed distribution...explain potential causes:  Male and Female? BMI?\n\n7378.07\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#analyzing-the-insurance-cost-of-nonsmokers",
    "href": "posts/US_HealthIns_Costs/index.html#analyzing-the-insurance-cost-of-nonsmokers",
    "title": "U.S. Medical Insurance Costs",
    "section": "Analyzing the insurance cost of nonsmokers",
    "text": "Analyzing the insurance cost of nonsmokers\nNow let’s look at the nonsmokers.\nSummary Stats: - Mean: $8,434.27 - Median: $7,345.41 - Max: $39,910.61 - Min: $1,121.87 - Standard Deviation: $5,990.96 - Variance: 35891656\n\n#find mean of insurance cost for nonsmokers\nnonsmoker_mean = round(np.mean(df_nonsmokers.charges), 2)\n#print('The nonsmoker average insurance cost for being a smoker: ${mean}'.format(mean = nonsmoker_mean))\n\n#find median of insurance cost for nonsmokers\nnonsmoker_median = round(np.median(df_nonsmokers.charges), 2)\n#print('The nonsmoker median insurance cost for being a smoker: ${median}'.format(median = nonsmoker_median))\n\n#find standard deviation...\nnonsmoker_std = round(np.std(df_nonsmokers.charges),2)\n#print('The nonsmoker standard deviation is: ${std}'.format(std = nonsmoker_std))\n\n#let's see the variance\nnonsmoker_var = round(np.var(df_nonsmokers.charges),2)\n#print('The nonsmoker variance is: {var}'.format(var = nonsmoker_var))\n\n#maximum cost\nnonsmoker_max = round(np.max(df_nonsmokers.charges), 2)\n#print('The nonsmoker maximum insurance cost is: ${max}'.format(max = nonsmoker_max))\n\n#minimum cost\nnonsmoker_min = round(np.min(df_nonsmokers.charges), 2)\n#print('The nonsmoker minimum insurance cost is: ${min}'.format(min = nonsmoker_min))\n\n#For explaination a high standard deviation tells us that here are other reasons the cost is so high since it varies signifcantly\n\n\nAnalyzing Male Nonsmokers\nGraph One: Cost Distribution - Right-skewed distribution is again present. - Knowing this, we should go with the median as the most appropriate statistic instead of the mean.\nGraph Two: Barplot of Average Cost per Child - There seems to be a trend that having more children will lead to higher average insurance costs. - Given the information from the smokers, when an individual has 5 children or more, their insurance costs drops signifcantly. - However, the lowest average comes from having no children at all.\nGraph Three: Scatterplot of Cost vs BMI with Region - There is no correlation between cost and BMI of nonsmokers. - This is an interesting finding. - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - Strong linear relationship between insurance cost and age. - A secondary trend is shown with the number of children. - The less children, the insurance cost is on the lower end of the linear relationship. - The more children, the insurance cost is on the higher end of the linear relationship - The outliers do not follow the trend line.\nGraph Five: Barplot of Average Cost per Region - No valuable insights can be drawn. - Some regions are simply cheaper than others.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n#seperate all male nonsmokers from the main data set\ndf_male_nonsmoker = df_nonsmokers[df_nonsmokers.sex == 'male']\n\n#print(male_nonsmoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male nonsmoker\nsns.histplot(df_male_nonsmoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Male Nonsmoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of male nonsmokers per child\ndf_children = df_male_nonsmoker.groupby('children').mean().reset_index()\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Male Nonsmoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of male nonsmokers by BMI\nsns.scatterplot(x = df_male_nonsmoker.bmi, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.region,\n                size = df_male_nonsmoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Male Nonsmoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of male nonsmokers by age\nsns.scatterplot(x = df_male_nonsmoker.age, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.children,\n                size = df_male_nonsmoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Male Nonsmoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of male nonsmokers per region\ndf_region = df_male_nonsmoker.groupby('region').mean().reset_index()\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Male Nonsmoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of male nonsmoker boxplot\nsns.boxplot(x = df_male_nonsmoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Nonsmoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n#highlight scatter plot for linear trend\n\nsns.set(font_scale = 0.8)\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = df_male_nonsmoker.age, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.children,\n                size = df_male_nonsmoker.children,\n                sizes = (7.5, 20))\nplt.title('Male Nonsmoker Insurance Cost vs Age')\nplt.xlabel('Age')\nplt.ylabel('Insurance Costs (USD)')\nplt.legend(bbox_to_anchor = (1.0, 1))\n\nplt.show()\nplt.clf()\n#highlight this\n#solid evidence that as age increases, your insurance cost increases as well.\n#people with more children see a slightly higher cost overall \n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nAnalyzing Female Nonsmokers\nGraph One: Cost Distribution - Right-skewed distribution is again present. - Knowing this, we should go with the median as the most appropriate statistic instead of the mean.\nGraph Two: Barplot of Average Cost per Child - Similar to the male nonsmokers, there seems to be a trend that having more children will result in higher average insurance costs. - Given the information from the smokers, when an individual has 5 children or more, their insurance costs drops on average. - However, the lowest average comes from having no children at all.\nGraph Three: Scatterplot of Cost vs BMI with Region - Again, there is no correlation between cost and BMI of nonsmokers. - No trends with the region.\nGraph Four: Scatterplot of Cost vs Age with Children - Strong linear relationship between insurance cost and age. - A secondary trend is shown with the number of children. - The less children, the insurance cost is on the lower end of the linear relationship. - The more children, the insurance cost is on the higher end of the linear relationship - The outliers do not follow the trend line.\nGraph Five: Barplot of Average Cost per Region - There is a slight trend of region importance. - The northeast is more expensive on average than the southwest.\nGraph Six: Boxplot of Male Smoker Insurance Cost - Noteworthy to see the spread of male smokers’ insurance cost.\n\n#seperate all female nonsmokers from the main data set\ndf_female_nonsmoker = df_nonsmokers[df_nonsmokers.sex == 'female']\n\n#print(df_female_nonsmoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram Female nonsmoker\nsns.histplot(df_female_nonsmoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Female Nonsmoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of Female nonsmokers per child\ndf_children = df_female_nonsmoker.groupby('children').mean().reset_index()\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Female Nonsmoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of Female nonsmokers by BMI\nsns.scatterplot(x = df_female_nonsmoker.bmi, y = df_female_nonsmoker.charges,\n                hue = df_female_nonsmoker.region,\n                size = df_female_nonsmoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Female Nonsmoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\nax[0,2].legend(bbox_to_anchor = (1.0, 1))\n\n#scatterplot for insurance cost of Female nonsmokers by age\nsns.scatterplot(x = df_female_nonsmoker.age, y = df_female_nonsmoker.charges,\n                hue = df_female_nonsmoker.children,\n                size = df_female_nonsmoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Female Nonsmoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of Female nonsmokers per region\ndf_region = df_female_nonsmoker.groupby('region').mean().reset_index()\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Female Nonsmoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of Female nonsmoker boxplot\nsns.boxplot(x = df_male_nonsmoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Nonsmoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#purpose",
    "href": "posts/PredictPinotWine_ML/index.html#purpose",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of this project was to develop a predictive model for identifying the province of origin for wines based on descriptions provided by critics. To achieve this goal, a random forest model was built and evaluated for its performance, achieving a kappa score of 0.82. This project aimed to provide a useful tool for wine connoisseurs and industry professionals in identifying the origin of wines based on their sensory characteristics."
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#setup",
    "href": "posts/PredictPinotWine_ML/index.html#setup",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#model-performance",
    "href": "posts/PredictPinotWine_ML/index.html#model-performance",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Model Performance",
    "text": "Model Performance\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               216          5                 0           0        1\n  California              13        758                14          23       19\n  Casablanca_Valley        0          0                10           0        0\n  Marlborough              0          0                 0          12        0\n  New_York                 0          0                 0           0        0\n  Oregon                   9         28                 2          10        6\n                   Reference\nPrediction          Oregon\n  Burgundy              10\n  California            62\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               475\n\nOverall Statistics\n                                          \n               Accuracy : 0.8793          \n                 95% CI : (0.8627, 0.8945)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8069          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9076            0.9583                 0.384615\nSpecificity                   0.9889            0.8515                 1.000000\nPos Pred Value                0.9310            0.8526                 1.000000\nNeg Pred Value                0.9847            0.9579                 0.990379\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1291            0.4531                 0.005977\nDetection Prevalence          0.1387            0.5314                 0.005977\nBalanced Accuracy             0.9482            0.9049                 0.692308\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.266667         0.00000        0.8684\nSpecificity                    1.000000         1.00000        0.9512\nPos Pred Value                 1.000000             NaN        0.8962\nNeg Pred Value                 0.980132         0.98446        0.9370\nPrevalence                     0.026898         0.01554        0.3270\nDetection Rate                 0.007173         0.00000        0.2839\nDetection Prevalence           0.007173         0.00000        0.3168\nBalanced Accuracy              0.633333         0.50000        0.9098"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#conclusion",
    "href": "posts/PredictPinotWine_ML/index.html#conclusion",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Conclusion",
    "text": "Conclusion\nA kappa value of 0.82 indicates a very good level of agreement between the predictions of the random forest model and the actual outcomes. Kappa is a statistical measure of inter-rater agreement, which is commonly used to evaluate the performance of classification models.\nIn the context of a random forest model, the kappa value measures how well the model predicts the correct class labels for a given set of data. A kappa value of 0.82 indicates that the model’s predictions are in very good agreement with the true class labels, with a high degree of precision and accuracy.\nOverall, a kappa value of 0.82 suggests that the random forest model is performing very well, and can be considered a reliable predictor of the target variable in the dataset."
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#purpose",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#purpose",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Purpose",
    "text": "Purpose\nThe objective of this study was to investigate the impact of temperature on the toxicity of pesticides, driven by concerns that the global temperature rise may cause alterations in disease transmission rates. Additionally, the study aimed to address the issue that current pesticide testing may not account for the potential effects of varying temperatures on pesticide toxicity. The central research question was to determine whether temperature has a significant influence on the toxicity of pesticides, and to address this question, survival analysis was employed to analyze the impact of temperature on the time-to-death of the exposed subjects. By utilizing survival analysis, the study aimed to determine whether temperature significantly affects the survival rate of the exposed organisms and to evaluate the magnitude of this effect on the toxicity of pesticides."
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#investigating-the-impact-of-temperature-on-pesticide-toxicity-a-survival-analysis-approach",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#investigating-the-impact-of-temperature-on-pesticide-toxicity-a-survival-analysis-approach",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "text": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach"
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#presentation",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#presentation",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html",
    "href": "posts/AppleInc_IncomeStatement/index.html",
    "title": "Apple’s Journey In The Stock Market",
    "section": "",
    "text": "The primary objective of this undertaking was to delve into the application of R’s plotly and create dynamic and interactive visualizations of Time Series Graphs. In this regard, I opted to scrutinize the Apple stock prices spanning the period from 1981 to the present, with a keen focus on identifying any notable trends that have emerged over time. Upon a closer inspection of the data, I observed a remarkable spike in the company’s stock prices in the aftermath of 2010, which I attribute to the resounding success of Apple’s iPhone and the accompanying products.\nThe shift in Apple’s market value was not just a result of its breakthrough innovation but was also influenced by its impeccable marketing strategy that has ensured that their products always remain relevant to the ever-changing consumer needs. It is noteworthy that Apple’s transformative technology has revolutionized the tech industry and has made significant contributions to its economic growth. Overall, the utilization of R’s plotly in the visualization of Time Series Graphs proved to be an incredibly insightful undertaking, helping to identify and explain the trends in the stock market in a more meaningful way."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#purpose",
    "href": "posts/Pokemon_Database/index.html#purpose",
    "title": "Pokédex Database",
    "section": "Purpose",
    "text": "Purpose\nThe main objective of this project was to construct a fully operational Postgresql database in a time frame of fewer than two weeks by employing the Extract, Transform, Load (ETL) methodology. The purpose of this approach was to extract data from various sources, transform it into a format that could be easily integrated into the database, and finally load the transformed data into the database.\nThe process involved several intricate steps, including identifying the relevant data sources, cleansing the extracted data to remove inconsistencies, standardizing the data to a uniform format, and applying data validation and verification techniques to ensure accuracy and completeness. Furthermore, it required careful consideration of the database schema, including the design of tables, relationships between tables, and the use of appropriate data types.\nThe successful implementation of this project was dependent on the utilization of cutting-edge technologies and tools, such as data integration software, data profiling tools, and scripting languages. The result was a functional database that can efficiently store and manage data, making it readily available for analysis, decision-making, and reporting purposes."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#summary",
    "href": "posts/Pokemon_Database/index.html#summary",
    "title": "Pokédex Database",
    "section": "Summary",
    "text": "Summary\nThe inquiry of identifying the optimal base stat Pokemon type sparked my interest, prompting me to delve into the realm of data engineering. In order to craft a well-informed response to this question, I began by utilizing the expansive and multifaceted “Pokémon of Kanto, Johto, and Hoenn Region” dataset to establish a structured and organized database."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#database-with-postgresql",
    "href": "posts/Pokemon_Database/index.html#database-with-postgresql",
    "title": "Pokédex Database",
    "section": "Database with PostgreSQL",
    "text": "Database with PostgreSQL\n\nPart 1\nDue to time constraints there are some missing tables that would have added more flexiblity to my analysis.\nCREATE TABLE IF NOT EXISTS pokemon (\n    id SMALLINT PRIMARY KEY,\n    identifier TEXT,\n    species_id SMALLINT,\n    height SMALLINT,\n    weight SMALLINT,\n    base_experience SMALLINT,\n    \"order\" SMALLINT,\n    is_default BOOLEAN\n);\n\n\nCREATE TABLE IF NOT EXISTS pokemon_types (\n    pokemon_id SMALLINT,\n    type_id SMALLINT PRIMARY KEY,\n    slot SMALLINT,\n    CONSTRAINT ref_pokemon\n        FOREIGN KEY(pokemon_id)\n            REFERENCES pokemon(id)\n);\n\nCREATE TABLE IF NOT EXISTS pokemon_abilities (\n    pokemon_id SMALLINT,\n    ability_id SMALLINT PRIMARY KEY,\n    is_hidden BOOLEAN,\n    slot SMALLINT,\n    CONSTRAINT ref_pokemon\n        FOREIGN KEY(pokemon_id)\n            REFERENCES pokemon(id)\n);\n\n\nCREATE TABLE IF NOT EXISTS generations (\n    id SMALLINT,\n    main_region_id SMALLINT PRIMARY KEY,\n    identifier CHAR(15) \n);\n\nCREATE TABLE IF NOT EXISTS types (\n    id SMALLINT PRIMARY KEY,\n    identifier CHAR(8),\n    generation_id SMALLINT,\n    damage_class_id SMALLINT,\n    CONSTRAINT ref_pokemon_types\n        FOREIGN KEY(id)\n            REFERENCES pokemon_types(type_id),\n    CONSTRAINT ref_generations \n        FOREIGN KEY(generation_id)\n            REFERENCES generations(main_region_id)\n);\n\nCREATE TABLE IF NOT EXISTS abilities (\n    id SMALLINT,\n    identifier TEXT,\n    generation_id SMALLINT,\n    is_main_series BOOLEAN,\n    CONSTRAINT ref_pokemon_abilities\n        FOREIGN KEY(id)\n            REFERENCES pokemon_abilities(ability_id),\n    CONSTRAINT ref_generations \n        FOREIGN KEY(generation_id)\n            REFERENCES generations(main_region_id)\n);\n\nCREATE TABLE IF NOT EXISTS moves (\n  id SMALLINT,\n  identifier TEXT,\n  generation_id SMALLINT,\n  type_id SMALLINT,\n  power SMALLINT,\n  pp SMALLINT,\n  accuracy SMALLINT,\n  priority SMALLINT,\n  target_id SMALLINT,\n  damage_class_id SMALLINT,\n  effect_id SMALLINT,\n  effect_chance SMALLINT,\n  contest_type_id SMALLINT,\n  contest_effect_id SMALLINT,\n  super_contest_effect_id SMALLINT,\n  CONSTRAINT ref_types\n        UNIQUE(damage_class_id, type_id),\n  CONSTRAINT ref_generations \n        FOREIGN KEY(generation_id)\n            REFERENCES generations(main_region_id),\n  CONSTRAINT ref_types_2\n        FOREIGN KEY(type_id)\n            REFERENCES types(id)\n);\n\n\nPart 2\nThis portion of the sql file is for transforming and preparing a csv file for analysis.\nSELECT \n    identifier AS pokemon_name, \n    pokemon_types.type_id,\n    pokemon_abilities.ability_id\nINTO temp1\nFROM pokemon\nLEFT JOIN pokemon_types\nON pokemon.id = pokemon_types.pokemon_id\nLEFT JOIN pokemon_abilities\nON pokemon.id  = pokemon_abilities.pokemon_id;\n\n\nSELECT \n    pokemon_name,\n    types.identifier AS pokemon_type,\n    abilities.identifier AS pokemon_ability,\n    types.generation_id AS gen_id,\n    types.id AS type_id\nINTO temp2\nFROM temp1\nLEFT JOIN types\nON temp1.type_id = types.id\nLEFT JOIN abilities\nON temp1.ability_id = abilities.id;\n\n\nDROP TABLE temp3;\nSELECT \n    pokemon_name,\n    pokemon_type,\n    pokemon_ability,\n    generations.identifier AS pokemon_generation,\n    moves.identifier AS pokemon_move,\n    moves.power AS pokemon_power,\n    moves.accuracy AS pokemon_accuracy,\n    moves.pp AS pokemon_pp\nINTO temp3\nFROM temp2\nLEFT JOIN generations\nON temp2.gen_id = generations.main_region_id\nLEFT JOIN moves\nON temp2.type_id = moves.type_id;\n\nSELECT *\nFROM temp3\nWHERE pokemon_power IS NOT NULL \n    AND pokemon_accuracy IS NOT NULL\nORDER BY pokemon_accuracy, pokemon_power;\n\n\nCOPY temp3\nTO '/Users/Shared/Data_503/Datasets/scuffed_pokedex.csv'\nWITH (FORMAT CSV, HEADER);\n\n\nPart 3\nHere is my R analysis! Again, feel free to use this as you wish!\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\npokemon <- read_csv(\"scuffed_pokedex.csv\")\n\nnames(pokemon)\n\nnb.cols <- 18\nmycolors <- colorRampPalette(brewer.pal(8, \"YlOrRd\"))(nb.cols)\n\npokemon %>% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %>%\n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_power, y = reorder(pokemon_type, avg_power), fill = reorder(pokemon_type, avg_power)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average power\",\n       y = \"Pokemon type\",\n       title = \"FIRE! The Best Pokemon Type For Damage Output Is...?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\nnb.cols <- 18\nmycolors <- colorRampPalette(brewer.pal(8, \"Blues\"))(nb.cols)\n\npokemon %>% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %>%\n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_accuracy, y = reorder(pokemon_type, avg_accuracy), fill = reorder(pokemon_type, avg_accuracy)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average accuracy\",\n       y = \"Pokemon type\",\n       title = \"Ouch! Who wins the bullseye competition?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\npokemon %>% \n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_power, y = avg_accuarcy, color = pokemon_type)) +\n  geom_point()\n\n\nstats <- pokemon %>% \n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE))\n            \nmodel <- lm(data = stats, avg_accuracy ~ avg_power)\nplot(model)"
  },
  {
    "objectID": "posts/PredictPremiumDefault_ML/index.html",
    "href": "posts/PredictPremiumDefault_ML/index.html",
    "title": "Insurance Premium Default",
    "section": "",
    "text": "library(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)\ntest <- read_csv(\"test.csv\")\ntrain <- read_csv(\"train.csv\")\n\n\n\n\n\n#head(train)\n#head(test)\n\nds <- train %>%\n  mutate(age_in_years = round(age_in_days/365, 1),\n         income_class = factor(case_when(Income > 24000 & Income <= 32000 ~ \"24K-32K\",\n                                  Income > 32000 & Income <= 45000 ~ \"32K-45K\",\n                                  Income > 45000 & Income <= 60000 ~ \"45K-60K\",\n                                  Income > 60000 & Income <= 85000 ~ \"60K-85K\",\n                                  Income > 85000 & Income <= 110000 ~ \"85K-110K\",\n                                  Income > 110000 & Income <= 200000 ~ \"110K-200K\",\n                                  Income > 200000 & Income <= 400000 ~ \"200K-400K\",\n                                  Income > 400000 ~ \">400K\"), \n                               levels = c(\"24K-32K\",\n                                          \"32K-45K\",\n                                          \"45K-60K\",\n                                          \"60K-85K\",\n                                          \"85K-110K\",\n                                          \"110K-200K\",\n                                          \"200K-400K\",\n                                          \">400K\"))) %>%\n  select(-application_underwriting_score, -sourcing_channel, - age_in_days)"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html",
    "href": "posts/Customer Return ML/Predict Return Probability.html",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "",
    "text": "To ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I then ran the logistic model and examined the coefficients, noting the highest ratios for ProductDepartment, with a significant influence from men’s and women’s products. Noting that for future exploration, I concluded my model and wrote the submssion file.\nNote: I had explored the idea of using rpart, rf and gbm, however I was not accustomed to using those models. There was much more refinement to be done, but the 3 hour constraint kept me focused on building a draft of the model. Would this go into production? No, but it would be a step in the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#load-the-required-packages",
    "href": "posts/Customer Return ML/Predict Return Probability.html#load-the-required-packages",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Load the required packages",
    "text": "Load the required packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#load-the-training-and-test-data",
    "href": "posts/Customer Return ML/Predict Return Probability.html#load-the-training-and-test-data",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Load the training and test data",
    "text": "Load the training and test data\n\ntrain <- read_csv(\"train.csv\") \ntest <- read_csv(\"test.csv\") \n\nglimpse(train)\n\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#data-exploration",
    "href": "posts/Customer Return ML/Predict Return Probability.html#data-exploration",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n\n\n\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n\n\n\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#feature-engineering",
    "href": "posts/Customer Return ML/Predict Return Probability.html#feature-engineering",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 56, 56, 56, 56, 44, 44, 44, 56, 56, 56, 58, 58, 39, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n\nsummary(train)\n\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :26.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:36.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :47.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :48.26  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:60.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :77.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n\ntable(train$Returned)\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#fit-a-logistical-regression-model",
    "href": "posts/Customer Return ML/Predict Return Probability.html#fit-a-logistical-regression-model",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Fit a Logistical Regression Model",
    "text": "Fit a Logistical Regression Model\n\nset.seed(345)\n\n#Model using Logistical Regression\nlogModel <- glm(Returned ~ .,\n                data = train,\n                family = \"binomial\")\nsummary(logModel)\n\n\nCall:\nglm(formula = Returned ~ ., family = \"binomial\", data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2617  -0.9725  -0.8697   1.3508   2.2074  \n\nCoefficients: (1 not defined because of singularities)\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                 -1.5204170  0.1444030 -10.529  < 2e-16 ***\nCustomerStateAlaska         -0.3745168  0.2706629  -1.384  0.16645    \nCustomerStateArizona        -0.1559535  0.0865001  -1.803  0.07140 .  \nCustomerStateArkansas       -0.1680269  0.1086315  -1.547  0.12192    \nCustomerStateCalifornia     -0.1757649  0.0719039  -2.444  0.01451 *  \nCustomerStateColorado       -0.1682868  0.0931554  -1.807  0.07084 .  \nCustomerStateConnecticut    -0.1087649  0.1043767  -1.042  0.29739    \nCustomerStateDC             -0.1803593  0.2203419  -0.819  0.41305    \nCustomerStateDelaware        0.1134396  0.1748311   0.649  0.51643    \nCustomerStateFlorida        -0.1420662  0.0757237  -1.876  0.06064 .  \nCustomerStateGeorgia        -0.0615201  0.0818030  -0.752  0.45202    \nCustomerStateHawaii         -0.1545264  0.1520539  -1.016  0.30951    \nCustomerStateIdaho          -0.1194081  0.1236664  -0.966  0.33426    \nCustomerStateIllinois       -0.1515665  0.0790256  -1.918  0.05512 .  \nCustomerStateIndiana        -0.0053376  0.0872975  -0.061  0.95125    \nCustomerStateIowa           -0.1039855  0.1103701  -0.942  0.34611    \nCustomerStateKansas         -0.2496458  0.1112347  -2.244  0.02481 *  \nCustomerStateKentucky       -0.3531264  0.1073438  -3.290  0.00100 ** \nCustomerStateLouisiana      -0.2381560  0.0915155  -2.602  0.00926 ** \nCustomerStateMaine          -0.2986534  0.1477344  -2.022  0.04322 *  \nCustomerStateMaryland       -0.1224028  0.0910360  -1.345  0.17877    \nCustomerStateMassachusetts  -0.1545115  0.0849803  -1.818  0.06903 .  \nCustomerStateMichigan       -0.2591744  0.0849511  -3.051  0.00228 ** \nCustomerStateMinnesota      -0.1422575  0.0912242  -1.559  0.11890    \nCustomerStateMississippi    -0.0312455  0.1186160  -0.263  0.79223    \nCustomerStateMissouri       -0.2411922  0.0950728  -2.537  0.01118 *  \nCustomerStateMontana        -0.1814644  0.1471433  -1.233  0.21748    \nCustomerStateNebraska       -0.2027473  0.1442761  -1.405  0.15994    \nCustomerStateNevada         -0.0743860  0.1120681  -0.664  0.50685    \nCustomerStateNew Hampshire  -0.0996571  0.1588999  -0.627  0.53055    \nCustomerStateNew Jersey     -0.1238822  0.0855308  -1.448  0.14751    \nCustomerStateNew Mexico     -0.3891323  0.1352394  -2.877  0.00401 ** \nCustomerStateNew York       -0.1653762  0.0755573  -2.189  0.02862 *  \nCustomerStateNorth Carolina -0.0281690  0.0820686  -0.343  0.73142    \nCustomerStateNorth Dakota   -0.1843492  0.1735505  -1.062  0.28813    \nCustomerStateOhio           -0.1949632  0.0798819  -2.441  0.01466 *  \nCustomerStateOklahoma       -0.2468512  0.1055238  -2.339  0.01932 *  \nCustomerStateOregon         -0.0733648  0.0871922  -0.841  0.40012    \nCustomerStatePennsylvania   -0.1311007  0.0795873  -1.647  0.09951 .  \nCustomerStateRhode Island   -0.1558779  0.1663353  -0.937  0.34869    \nCustomerStateSouth Carolina -0.2159403  0.0967242  -2.233  0.02558 *  \nCustomerStateSouth Dakota   -0.1269469  0.1633225  -0.777  0.43699    \nCustomerStateTennessee      -0.0368559  0.0889942  -0.414  0.67877    \nCustomerStateTexas          -0.0777116  0.0733482  -1.059  0.28938    \nCustomerStateUtah           -0.1711151  0.1082903  -1.580  0.11407    \nCustomerStateVermont        -0.0439923  0.2076074  -0.212  0.83218    \nCustomerStateVirginia       -0.1540969  0.0840292  -1.834  0.06668 .  \nCustomerStateWashington     -0.1241350  0.0861621  -1.441  0.14967    \nCustomerStateWest Virginia  -0.1091140  0.1396389  -0.781  0.43457    \nCustomerStateWisconsin      -0.0821141  0.0921258  -0.891  0.37275    \nCustomerStateWyoming        -0.3274862  0.2007843  -1.631  0.10288    \nProductDepartmentMens        1.4347920  0.0631408  22.724  < 2e-16 ***\nProductDepartmentWomens      1.4951526  0.0638800  23.406  < 2e-16 ***\nProductDepartmentYouth       0.9448759  0.0657464  14.372  < 2e-16 ***\nProductSizeL                -0.0385464  0.0353477  -1.090  0.27550    \nProductSizeM                -0.0780394  0.0354060  -2.204  0.02752 *  \nProductSizeS                -0.0406230  0.0378432  -1.073  0.28307    \nProductSizeXL               -0.0377867  0.0381043  -0.992  0.32136    \nProductSizeXS               -0.0753978  0.0426810  -1.767  0.07730 .  \nProductSizeXXL                      NA         NA      NA       NA    \nProductCost                 -0.0041875  0.0012801  -3.271  0.00107 ** \nDiscountPct                 -0.3452542  0.2723834  -1.268  0.20497    \nPurchasePrice               -0.0036330  0.0030623  -1.186  0.23549    \nSeasonSpring                -0.2631769  0.0236683 -11.119  < 2e-16 ***\nSeasonSummer                -0.1764066  0.0215047  -8.203 2.34e-16 ***\nSeasonWinter                -0.1910545  0.0235449  -8.114 4.88e-16 ***\nCustomerAge                  0.0068431  0.0005897  11.604  < 2e-16 ***\nMSRP                         0.0016353  0.0026851   0.609  0.54251    \nPriceRange$13-$30           -0.4153545  0.0984400  -4.219 2.45e-05 ***\nPriceRange$31-$60           -0.2517772  0.0628835  -4.004 6.23e-05 ***\nPriceRange$61-$100          -0.2086855  0.0375943  -5.551 2.84e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84248  on 64911  degrees of freedom\nResidual deviance: 82399  on 64842  degrees of freedom\nAIC: 82539\n\nNumber of Fisher Scoring iterations: 4\n\nodds_ratio <- exp(logModel$coefficients)\ndata.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n  arrange(desc(odds_ratio)) %>% \n  head()\n\n                                           name odds_ratio\nProductDepartmentWomens ProductDepartmentWomens   4.460017\nProductDepartmentMens     ProductDepartmentMens   4.198772\nProductDepartmentYouth   ProductDepartmentYouth   2.572494\nCustomerStateDelaware     CustomerStateDelaware   1.120124\nCustomerAge                         CustomerAge   1.006867\nMSRP                                       MSRP   1.001637"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#make-prediction-on-the-test-data",
    "href": "posts/Customer Return ML/Predict Return Probability.html#make-prediction-on-the-test-data",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Make prediction on the test data",
    "text": "Make prediction on the test data\n\ntestPredictions <- predict(logModel, newdata = test, type = \"response\")"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#writing-the-submission-file",
    "href": "posts/Customer Return ML/Predict Return Probability.html#writing-the-submission-file",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Writing the Submission File",
    "text": "Writing the Submission File\n\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\nwrite.csv(submission, \"submission.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Customer Return ML/Predict Return Probability.html#leftout-features-that-were-considered",
    "href": "posts/Customer Return ML/Predict Return Probability.html#leftout-features-that-were-considered",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "Leftout Features that were considered",
    "text": "Leftout Features that were considered\n\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html",
    "href": "posts/Customer Return ML/index.html",
    "title": "Columbia Sportswear Data Science Assessment: Predict Customer Returns",
    "section": "",
    "text": "To ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I then ran the logistic model and examined the coefficients, noting the highest ratios for ProductDepartment, with a significant influence from men’s and women’s products. Noting that for future exploration, I concluded my model and wrote the submssion file.\nNote: I had explored the idea of using rpart, rf and gbm, however I was not accustomed to using those models. There was much more refinement to be done, but the 3 hour constraint kept me focused on building a draft of the model. Would this go into production? No, but it would be a step in the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/index.html#load-the-required-packages",
    "href": "posts/Customer Return ML/index.html#load-the-required-packages",
    "title": "Predicting Customer Returns",
    "section": "Load the required packages",
    "text": "Load the required packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#load-the-training-and-test-data",
    "href": "posts/Customer Return ML/index.html#load-the-training-and-test-data",
    "title": "Predicting Customer Returns",
    "section": "Load the training and test data",
    "text": "Load the training and test data\n\ntrain <- read_csv(\"train.csv\") \ntest <- read_csv(\"test.csv\") \n\nglimpse(train)\n\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#data-exploration",
    "href": "posts/Customer Return ML/index.html#data-exploration",
    "title": "Predicting Customer Returns",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n\n\n\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n\n\n\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#feature-engineering",
    "href": "posts/Customer Return ML/index.html#feature-engineering",
    "title": "Predicting Customer Returns",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 56, 56, 56, 56, 44, 44, 44, 56, 56, 56, 58, 58, 39, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n\nsummary(train)\n\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :26.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:36.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :47.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :48.39  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:60.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :77.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n\ntable(train$Returned)\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#fit-a-logistical-regression-model",
    "href": "posts/Customer Return ML/index.html#fit-a-logistical-regression-model",
    "title": "Predicting Customer Returns",
    "section": "Fit a Logistical Regression Model",
    "text": "Fit a Logistical Regression Model\n\nset.seed(345)\n\n#Model using Logistical Regression\nctrl <- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)\nfit <- train(Returned ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 50,\n             tuneLength = 3,\n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n\nRandom Forest \n\n64912 samples\n   10 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 43274, 43275, 43275 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5550386  1.0000000  0.0000000\n  36    0.6250091  0.8337812  0.3601433\n  70    0.6245318  0.8249076  0.3712024\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 36."
  },
  {
    "objectID": "posts/Customer Return ML/index.html#make-prediction-on-the-test-data",
    "href": "posts/Customer Return ML/index.html#make-prediction-on-the-test-data",
    "title": "Predicting Customer Returns",
    "section": "Make prediction on the test data",
    "text": "Make prediction on the test data\n\ntestPredictions <- predict(fit, newdata = test, type = \"prob\")[,2]"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#writing-the-submission-file",
    "href": "posts/Customer Return ML/index.html#writing-the-submission-file",
    "title": "Predicting Customer Returns",
    "section": "Writing the Submission File",
    "text": "Writing the Submission File\n\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\nwrite.csv(submission, \"submission.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#leftout-features-that-were-considered",
    "href": "posts/Customer Return ML/index.html#leftout-features-that-were-considered",
    "title": "Predicting Customer Returns",
    "section": "Leftout Features that were considered",
    "text": "Leftout Features that were considered\n\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#approach-and-methodology",
    "href": "posts/Customer Return ML/index.html#approach-and-methodology",
    "title": "Predicting Customer Returns",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\nTo ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I ran a Random Forest model and achieved an AUC of 0.625. This indicates moderate predictive power for the model, and suggests that further feature engineering or model tuning may be necessary for better performance.\nInitially, I utilized a logistic regression model, however, it required further refinement and tuning. Due to a three-hour time constraint, I focused on building a draft model. While not suitable for production, it is a step towards the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#approach-and-methodology",
    "href": "posts/Customer Return ML/bettermodel.html#approach-and-methodology",
    "title": "Improving Customer Return Model",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\nTo ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I then ran the logistic model and examined the coefficients, noting the highest ratios for ProductDepartment, with a significant influence from men’s and women’s products. Noting that for future exploration, I concluded my model and wrote the submssion file.\nNote: I had explored the idea of using rpart, rf and gbm, however I was not accustomed to using those models. There was much more refinement to be done, but the 3 hour constraint kept me focused on building a draft of the model. Would this go into production? No, but it would be a step in the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#load-the-required-packages",
    "href": "posts/Customer Return ML/bettermodel.html#load-the-required-packages",
    "title": "Improving Customer Return Model",
    "section": "Load the required packages",
    "text": "Load the required packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#load-the-training-and-test-data",
    "href": "posts/Customer Return ML/bettermodel.html#load-the-training-and-test-data",
    "title": "Improving Customer Return Model",
    "section": "Load the training and test data",
    "text": "Load the training and test data\n\ntrain <- read_csv(\"train.csv\") \ntest <- read_csv(\"test.csv\") \n\nglimpse(train)\n\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#data-exploration",
    "href": "posts/Customer Return ML/bettermodel.html#data-exploration",
    "title": "Improving Customer Return Model",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n\n\n\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n\n\n\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#feature-engineering",
    "href": "posts/Customer Return ML/bettermodel.html#feature-engineering",
    "title": "Improving Customer Return Model",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 56, 56, 56, 56, 44, 44, 44, 56, 56, 56, 58, 58, 39, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n\nsummary(train)\n\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :26.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:36.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :47.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :48.28  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:60.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :77.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n\ntable(train$Returned)\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#fit-a-logistical-regression-model",
    "href": "posts/Customer Return ML/bettermodel.html#fit-a-logistical-regression-model",
    "title": "Improving Customer Return Model",
    "section": "Fit a Logistical Regression Model",
    "text": "Fit a Logistical Regression Model\n\nset.seed(345)\n\n#Model using Logistical Regression\nlogModel <- glm(Returned ~ .,\n                data = train,\n                family = \"binomial\")\nsummary(logModel)\n\n\nCall:\nglm(formula = Returned ~ ., family = \"binomial\", data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2616  -0.9725  -0.8697   1.3508   2.2074  \n\nCoefficients: (1 not defined because of singularities)\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                 -1.5207537  0.1444108 -10.531  < 2e-16 ***\nCustomerStateAlaska         -0.3741477  0.2706633  -1.382  0.16687    \nCustomerStateArizona        -0.1556297  0.0865003  -1.799  0.07199 .  \nCustomerStateArkansas       -0.1676433  0.1086317  -1.543  0.12278    \nCustomerStateCalifornia     -0.1754825  0.0719041  -2.441  0.01467 *  \nCustomerStateColorado       -0.1679037  0.0931558  -1.802  0.07148 .  \nCustomerStateConnecticut    -0.1083832  0.1043765  -1.038  0.29909    \nCustomerStateDC             -0.1799928  0.2203424  -0.817  0.41400    \nCustomerStateDelaware        0.1135606  0.1748332   0.650  0.51599    \nCustomerStateFlorida        -0.1418228  0.0757237  -1.873  0.06108 .  \nCustomerStateGeorgia        -0.0612147  0.0818032  -0.748  0.45427    \nCustomerStateHawaii         -0.1541336  0.1520537  -1.014  0.31074    \nCustomerStateIdaho          -0.1195617  0.1236672  -0.967  0.33364    \nCustomerStateIllinois       -0.1512965  0.0790256  -1.915  0.05555 .  \nCustomerStateIndiana        -0.0050442  0.0872975  -0.058  0.95392    \nCustomerStateIowa           -0.1037765  0.1103698  -0.940  0.34708    \nCustomerStateKansas         -0.2493995  0.1112344  -2.242  0.02495 *  \nCustomerStateKentucky       -0.3528027  0.1073443  -3.287  0.00101 ** \nCustomerStateLouisiana      -0.2377653  0.0915155  -2.598  0.00937 ** \nCustomerStateMaine          -0.2983985  0.1477348  -2.020  0.04340 *  \nCustomerStateMaryland       -0.1223623  0.0910361  -1.344  0.17891    \nCustomerStateMassachusetts  -0.1541913  0.0849800  -1.814  0.06961 .  \nCustomerStateMichigan       -0.2587789  0.0849507  -3.046  0.00232 ** \nCustomerStateMinnesota      -0.1419437  0.0912240  -1.556  0.11971    \nCustomerStateMississippi    -0.0308985  0.1186159  -0.260  0.79448    \nCustomerStateMissouri       -0.2408855  0.0950727  -2.534  0.01129 *  \nCustomerStateMontana        -0.1813837  0.1471433  -1.233  0.21769    \nCustomerStateNebraska       -0.2024296  0.1442762  -1.403  0.16060    \nCustomerStateNevada         -0.0742491  0.1120680  -0.663  0.50763    \nCustomerStateNew Hampshire  -0.0996278  0.1589004  -0.627  0.53067    \nCustomerStateNew Jersey     -0.1235066  0.0855316  -1.444  0.14874    \nCustomerStateNew Mexico     -0.3888035  0.1352393  -2.875  0.00404 ** \nCustomerStateNew York       -0.1650836  0.0755573  -2.185  0.02890 *  \nCustomerStateNorth Carolina -0.0278880  0.0820683  -0.340  0.73400    \nCustomerStateNorth Dakota   -0.1839630  0.1735504  -1.060  0.28915    \nCustomerStateOhio           -0.1947161  0.0798819  -2.438  0.01479 *  \nCustomerStateOklahoma       -0.2466360  0.1055241  -2.337  0.01943 *  \nCustomerStateOregon         -0.0730939  0.0871920  -0.838  0.40186    \nCustomerStatePennsylvania   -0.1309621  0.0795873  -1.646  0.09986 .  \nCustomerStateRhode Island   -0.1554846  0.1663351  -0.935  0.34991    \nCustomerStateSouth Carolina -0.2157011  0.0967242  -2.230  0.02574 *  \nCustomerStateSouth Dakota   -0.1268369  0.1633226  -0.777  0.43739    \nCustomerStateTennessee      -0.0367034  0.0889942  -0.412  0.68003    \nCustomerStateTexas          -0.0774882  0.0733484  -1.056  0.29077    \nCustomerStateUtah           -0.1708986  0.1082913  -1.578  0.11453    \nCustomerStateVermont        -0.0436083  0.2076073  -0.210  0.83363    \nCustomerStateVirginia       -0.1537185  0.0840291  -1.829  0.06735 .  \nCustomerStateWashington     -0.1238717  0.0861623  -1.438  0.15053    \nCustomerStateWest Virginia  -0.1087151  0.1396384  -0.779  0.43625    \nCustomerStateWisconsin      -0.0820722  0.0921260  -0.891  0.37300    \nCustomerStateWyoming        -0.3279518  0.2007806  -1.633  0.10239    \nProductDepartmentMens        1.4348495  0.0631407  22.725  < 2e-16 ***\nProductDepartmentWomens      1.4952204  0.0638800  23.407  < 2e-16 ***\nProductDepartmentYouth       0.9449300  0.0657463  14.372  < 2e-16 ***\nProductSizeL                -0.0385858  0.0353476  -1.092  0.27500    \nProductSizeM                -0.0780820  0.0354060  -2.205  0.02743 *  \nProductSizeS                -0.0406725  0.0378430  -1.075  0.28248    \nProductSizeXL               -0.0378496  0.0381042  -0.993  0.32056    \nProductSizeXS               -0.0754693  0.0426809  -1.768  0.07702 .  \nProductSizeXXL                      NA         NA      NA       NA    \nProductCost                 -0.0041879  0.0012801  -3.271  0.00107 ** \nDiscountPct                 -0.3452504  0.2723828  -1.268  0.20497    \nPurchasePrice               -0.0036323  0.0030623  -1.186  0.23558    \nSeasonSpring                -0.2631683  0.0236683 -11.119  < 2e-16 ***\nSeasonSummer                -0.1763956  0.0215047  -8.203 2.35e-16 ***\nSeasonWinter                -0.1910599  0.0235449  -8.115 4.87e-16 ***\nCustomerAge                  0.0068400  0.0005896  11.600  < 2e-16 ***\nMSRP                         0.0016355  0.0026851   0.609  0.54245    \nPriceRange$13-$30           -0.4153104  0.0984400  -4.219 2.45e-05 ***\nPriceRange$31-$60           -0.2517379  0.0628836  -4.003 6.25e-05 ***\nPriceRange$61-$100          -0.2086568  0.0375943  -5.550 2.85e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84248  on 64911  degrees of freedom\nResidual deviance: 82399  on 64842  degrees of freedom\nAIC: 82539\n\nNumber of Fisher Scoring iterations: 4\n\nodds_ratio <- exp(logModel$coefficients)\ndata.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n  arrange(desc(odds_ratio)) %>% \n  head()\n\n                                           name odds_ratio\nProductDepartmentWomens ProductDepartmentWomens   4.460319\nProductDepartmentMens     ProductDepartmentMens   4.199013\nProductDepartmentYouth   ProductDepartmentYouth   2.572633\nCustomerStateDelaware     CustomerStateDelaware   1.120260\nCustomerAge                         CustomerAge   1.006863\nMSRP                                       MSRP   1.001637"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#make-prediction-on-the-test-data",
    "href": "posts/Customer Return ML/bettermodel.html#make-prediction-on-the-test-data",
    "title": "Improving Customer Return Model",
    "section": "Make prediction on the test data",
    "text": "Make prediction on the test data\n\ntestPredictions <- predict(logModel, newdata = test, type = \"response\")"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#writing-the-submission-file",
    "href": "posts/Customer Return ML/bettermodel.html#writing-the-submission-file",
    "title": "Improving Customer Return Model",
    "section": "Writing the Submission File",
    "text": "Writing the Submission File\n\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\nwrite.csv(submission, \"submission.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Customer Return ML/bettermodel.html#leftout-features-that-were-considered",
    "href": "posts/Customer Return ML/bettermodel.html#leftout-features-that-were-considered",
    "title": "Improving Customer Return Model",
    "section": "Leftout Features that were considered",
    "text": "Leftout Features that were considered\n\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#fit-a-random-forest-model",
    "href": "posts/Customer Return ML/index.html#fit-a-random-forest-model",
    "title": "Predicting Customer Returns",
    "section": "Fit a Random Forest Model",
    "text": "Fit a Random Forest Model\n\nset.seed(345)\n\n#Model using Random Forest \nctrl <- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)\nfit <- train(Returned ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 50,\n             tuneLength = 3,\n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n\nRandom Forest \n\n64912 samples\n   10 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 43274, 43275, 43275 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5545983  1.0000000  0.0000000\n  36    0.6241811  0.8308551  0.3589194\n  70    0.6238194  0.8245509  0.3729071\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 36."
  },
  {
    "objectID": "posts/PredictPremiumDefault_ML/index.html#setup",
    "href": "posts/PredictPremiumDefault_ML/index.html#setup",
    "title": "Predicting Insurance Premium Default {work in progress}",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)\ntest <- read_csv(\"test.csv\")\ntrain <- read_csv(\"train.csv\")"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Presentations",
    "section": "",
    "text": "This presentation not only serves as an introduction to my persona but also offers a glimpse into a selection of my projects that I have found immensely gratifying. Utilizing an array of programming languages ranging from R to SQL, I am thrilled to embark on my journey as a Data Scientist and become an integral part of the ever-evolving field of Data Science."
  },
  {
    "objectID": "presentation.html#parasites-slides-work-in-progress",
    "href": "presentation.html#parasites-slides-work-in-progress",
    "title": "Presentations",
    "section": "Parasites Slides [Work in progress]",
    "text": "Parasites Slides [Work in progress]\nLink: https://bcervantesalvarez.github.io/Presentations/Parasites/#/title-slide"
  },
  {
    "objectID": "presentation.html#parasites-slides",
    "href": "presentation.html#parasites-slides",
    "title": "Presentations",
    "section": "Parasites Slides",
    "text": "Parasites Slides"
  },
  {
    "objectID": "presentation.html#test",
    "href": "presentation.html#test",
    "title": "Presentations",
    "section": "{TEST}",
    "text": "{TEST}"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#apply-custom-theme",
    "href": "posts/AppleInc_IncomeStatement/index.html#apply-custom-theme",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Apply Custom Theme",
    "text": "Apply Custom Theme\n\nmyTheme <- function(){ \n    font <- \"SF Mono\"   #assign font family up front\n    \n    theme_minimal() %+replace%    #replace elements we want to change\n    \n    theme(\n      \n      #grid elements\n      panel.grid.major.x = element_blank(),    #strip major gridlines\n      panel.grid.minor = element_blank(),    #strip minor gridlines\n      axis.ticks = element_blank(),          #strip axis ticks\n      \n      #since theme_minimal() already strips axis lines, \n      #we don't need to do that again\n      \n      #text elements\n      plot.title = element_text(             #title\n                   family = font,            #set font family\n                   size = 16,                #set font size\n                   face = 'bold',            #bold typeface\n                   hjust = 0,                #left align\n                   vjust = 2),               #raise slightly\n      \n      plot.subtitle = element_text(          #subtitle\n                   family = font,            #font family\n                   size = 12),               #font size\n      \n      plot.caption = element_text(           #caption\n                   family = font,            #font family\n                   size = 9,                 #font size\n                   hjust = 1),               #right align\n      \n      axis.title = element_text(             #axis titles\n                   family = font,            #font family\n                   size = 10),               #font size\n      \n      axis.text = element_text(              #axis text\n                   family = font,            #axis famuly\n                   size = 9),                #font size\n      \n      axis.text.x = element_text(            #margin for axis text\n                    margin=margin(5, b = 10))\n      \n      #since the legend often requires manual tweaking \n      #based on plot content, don't define it here\n    )\n}"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#data-wrangling",
    "href": "posts/AppleInc_IncomeStatement/index.html#data-wrangling",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nyearlyDs <- ds %>% \n  drop_na() %>%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %>%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %>%\n  summarize(Open = mean(Open),\n            High = mean(High),\n            Low = mean(Low),\n            Close = mean(Close),\n            AdjClose = mean(`Adj Close`),\n            Volume = mean(Volume))\n\n\nlog_yearlyDs <- ds %>% \n  drop_na() %>%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %>%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %>%\n  summarize(Open = log(mean(Open)),\n            High = log(mean(High)),\n            Low = log(mean(Low)),\n            Close = log(mean(Close)),\n            AdjClose = log(mean(`Adj Close`)),\n            Volume = log(mean(Volume)))"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#section",
    "href": "posts/AppleInc_IncomeStatement/index.html#section",
    "title": "Finance: Apple’s Journey",
    "section": "",
    "text": "p <- yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price Since 1981\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  myTheme()\n\nggplotly(p) %>%\n  layout(hovermode = \"x unified\") %>% \n  style(hovertext = paste0(\" High: $\", round(yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: $\", round(yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: $\", round(yearlyDs$AdjClose,2)),\n        traces = 3)\n\n\n\n\n\n\np2 <- log_yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price (Log-Normalized) Since 1981\") +\n  myTheme()\n\nggplotly(p2, tooltip = \"text\") %>%\n  layout(hovermode = \"x unified\", \n         hovertext = paste0(\" Year: \", log_yearlyDs$Year)) %>% \n  style(hovertext = paste0(\" High: \", round(log_yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: \", round(log_yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: \", round(log_yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "href": "posts/AppleInc_IncomeStatement/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Times Series Plots of Apple Inc. Stock Prices",
    "text": "Times Series Plots of Apple Inc. Stock Prices\n\np <- yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price Since 1981\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  myTheme()\n\nggplotly(p) %>%\n  layout(hovermode = \"x unified\") %>% \n  style(hovertext = paste0(\" High: $\", round(yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: $\", round(yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: $\", round(yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#applying-log-norm",
    "href": "posts/AppleInc_IncomeStatement/index.html#applying-log-norm",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Applying Log-norm",
    "text": "Applying Log-norm\n\np2 <- log_yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price (Log-Normalized)\") +\n  myTheme()\n\nggplotly(p2, tooltip = \"text\") %>%\n  layout(hovermode = \"x unified\", \n         hovertext = paste0(\" Year: \", log_yearlyDs$Year)) %>% \n  style(hovertext = paste0(\" High: \", round(log_yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: \", round(log_yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: \", round(log_yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#setup",
    "href": "posts/AppleInc_IncomeStatement/index.html#setup",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\n\n\nds <- read_csv(\"AppleInc_Stocks.csv\")\n\n#head(ds)"
  },
  {
    "objectID": "posts/PredictPremiumDefault_ML/index.html#wrangling",
    "href": "posts/PredictPremiumDefault_ML/index.html#wrangling",
    "title": "Insurance Premium Default {updating…}",
    "section": "Wrangling",
    "text": "Wrangling\n\n#head(train)\n#head(test)\n\nds <- train %>%\n  mutate(age_in_years = round(age_in_days/365, 1),\n         income_class = factor(case_when(Income > 24000 & Income <= 32000 ~ \"24K-32K\",\n                                  Income > 32000 & Income <= 45000 ~ \"32K-45K\",\n                                  Income > 45000 & Income <= 60000 ~ \"45K-60K\",\n                                  Income > 60000 & Income <= 85000 ~ \"60K-85K\",\n                                  Income > 85000 & Income <= 110000 ~ \"85K-110K\",\n                                  Income > 110000 & Income <= 200000 ~ \"110K-200K\",\n                                  Income > 200000 & Income <= 400000 ~ \"200K-400K\",\n                                  Income > 400000 ~ \">400K\"), \n                               levels = c(\"24K-32K\",\n                                          \"32K-45K\",\n                                          \"45K-60K\",\n                                          \"60K-85K\",\n                                          \"85K-110K\",\n                                          \"110K-200K\",\n                                          \"200K-400K\",\n                                          \">400K\"))) %>%\n  select(-application_underwriting_score, -sourcing_channel, - age_in_days)"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#load-dataset",
    "href": "posts/Param_Report_Retail/index.html#load-dataset",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alliveate That Stress",
    "section": "Load Dataset",
    "text": "Load Dataset\n\nds <- read_csv(\"retail.csv\")\n\n#head(ds)\n\nds <- ds %>% \n  rename(ID = ...1) %>%\n  mutate(Month = lubridate::floor_date(Date, 'month')) %>%\n  filter(year(Month) == params$year)\n\nglimpse(ds)\n\nRows: 10,042\nColumns: 9\n$ ID         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ DocumentID <dbl> 716, 716, 716, 716, 716, 716, 716, 460, 461, 462, 463, 464,…\n$ Date       <date> 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23…\n$ SKU        <dbl> 1039, 853, 862, 868, 2313, 2355, 2529, 2361, 2723, 655, 254…\n$ Price      <dbl> 381.78, 593.22, 423.73, 201.70, 345.76, 406.78, 542.38, 139…\n$ Discount   <dbl> 67.37254, 0.00034, -0.00119, 35.58814, 61.01966, 101.69458,…\n$ Customer   <dbl> 1, 1, 1, 1, 1, 1, 1, 460, 479, 26, 580, 311, 311, 311, 311,…\n$ Quantity   <dbl> 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 4,…\n$ Month      <date> 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01…"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#setting-plot-theme",
    "href": "posts/Param_Report_Retail/index.html#setting-plot-theme",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alliveate That Stress",
    "section": "Setting Plot Theme",
    "text": "Setting Plot Theme"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#data-visualization",
    "href": "posts/Param_Report_Retail/index.html#data-visualization",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alliveate That Stress",
    "section": "Data Visualization",
    "text": "Data Visualization\n\np <- ds %>%\n  group_by(Month) %>%\n  summarize(AvgSales = round(mean(Price * Quantity),2) ) %>%\n  ggplot(aes(x = Month, \n             y = AvgSales,\n             group = 1,                 #Necessary or else line plot disappears\n             text = paste0(\"Monthly Sales: $\", (round(AvgSales/1000,2)),\"K\" ))) +\n  geom_line(size = 1) + \n  scale_y_continuous(labels = scales::dollar_format(scale = .001, suffix = \"K\")) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%B\") + \n  labs(title = paste0(\"CRM and Invoicing System Sales For FY: \", params$year),\n       caption = \"Source: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv\",\n       x = NULL,\n       y = NULL) +\n  myTheme()\n\nggplotly(p, tooltip = c(\"text\")) %>% \n  layout(hovermode = \"x unified\")"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html",
    "href": "posts/Param_Report_Retail/index.html",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alliveate That Stress",
    "section": "",
    "text": "## Purpose\nFor this particular undertaking, I focused on the utilization of parameterized Rmarkdown with the aim of enhancing the efficacy of report generation. This approach involves setting predetermined values for parameters and creating a function that automatically updates the information, thereby mitigating the need for manual modification of parameters for daily reports. The primary objective of this system is to generate reports more efficiently, which in turn helps businesses save time in the long run. Furthermore, data scientists, analysts, and engineers can benefit from this system as it enables them to produce comparable reports over time with ease. The automation of the report generation process through the use of parameterized Rmarkdown contributes to increased productivity and accuracy, which can be highly beneficial to the success of a business."
  },
  {
    "objectID": "reports/files/CRM_Retail_Sales/index.html",
    "href": "reports/files/CRM_Retail_Sales/index.html",
    "title": "Retail Reports",
    "section": "",
    "text": "library(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)"
  },
  {
    "objectID": "reports/files/CRM_Retail_Sales/index.html#load-dataset",
    "href": "reports/files/CRM_Retail_Sales/index.html#load-dataset",
    "title": "Retail Reports",
    "section": "Load Dataset",
    "text": "Load Dataset\n\nds <- read_csv(\"retail.csv\")\n\n#head(ds)\n\nds <- ds %>% \n  rename(ID = ...1) %>%\n  mutate(Month = lubridate::floor_date(Date, 'month')) %>%\n  filter(year(Month) == params$year)\n\nglimpse(ds)\n\nRows: 10,042\nColumns: 9\n$ ID         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ DocumentID <dbl> 716, 716, 716, 716, 716, 716, 716, 460, 461, 462, 463, 464,…\n$ Date       <date> 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23…\n$ SKU        <dbl> 1039, 853, 862, 868, 2313, 2355, 2529, 2361, 2723, 655, 254…\n$ Price      <dbl> 381.78, 593.22, 423.73, 201.70, 345.76, 406.78, 542.38, 139…\n$ Discount   <dbl> 67.37254, 0.00034, -0.00119, 35.58814, 61.01966, 101.69458,…\n$ Customer   <dbl> 1, 1, 1, 1, 1, 1, 1, 460, 479, 26, 580, 311, 311, 311, 311,…\n$ Quantity   <dbl> 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 4,…\n$ Month      <date> 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01…"
  },
  {
    "objectID": "reports/files/CRM_Retail_Sales/index.html#setting-plot-theme",
    "href": "reports/files/CRM_Retail_Sales/index.html#setting-plot-theme",
    "title": "Retail Reports",
    "section": "Setting Plot Theme",
    "text": "Setting Plot Theme\n\nmyTheme <- function(){ \n    font <- \"SF Mono\"   #assign font family up front\n    \n    theme_minimal() %+replace%    #replace elements we want to change\n    \n    theme(\n      \n      #grid elements\n      panel.grid.major.x = element_blank(),    #strip major gridlines\n      panel.grid.minor = element_blank(),    #strip minor gridlines\n      axis.ticks = element_blank(),          #strip axis ticks\n      \n      #since theme_minimal() already strips axis lines, \n      #we don't need to do that again\n      \n      #text elements\n      plot.title = element_text(             #title\n                   family = font,            #set font family\n                   size = 16,                #set font size\n                   face = 'bold',            #bold typeface\n                   hjust = 0,                #left align\n                   vjust = 2),               #raise slightly\n      \n      plot.margin = margin(                  #margins\n                           r = 0.5,          #right margin\n                           l = 0.5,          #left margin\n                           t = 1,            #top margin\n                           b = 0.25,            #bottom margin\n                           unit = \"cm\"),     #units      \n      \n      plot.subtitle = element_text(          #subtitle\n                   family = font,            #font family\n                   size = 12,                #font size\n                   hjust = 0,\n                   vjust = -1),               \n      \n      plot.caption = element_text(           #caption\n                   family = font,            #font family\n                   size = 9),                 #font size\n      \n      axis.title = element_text(             #axis titles\n                   family = font,            #font family\n                   size = 10),               #font size\n      \n      axis.text = element_text(              #axis text\n                   family = font,            #axis famuly\n                   size = 9),                #font size\n      \n      axis.text.x = element_text(            #margin for axis text\n                      margin = margin(t = 5, b = 20),\n                      angle = 45)\n      \n      #since the legend often requires manual tweaking \n      #based on plot content, don't define it here\n    )\n}"
  },
  {
    "objectID": "reports/files/CRM_Retail_Sales/index.html#data-visualization",
    "href": "reports/files/CRM_Retail_Sales/index.html#data-visualization",
    "title": "Retail Reports",
    "section": "Data Visualization",
    "text": "Data Visualization\n\np <- ds %>%\n  group_by(Month) %>%\n  summarize(AvgSales = round(mean(Price * Quantity),2) ) %>%\n  ggplot(aes(x = Month, \n             y = AvgSales,\n             group = 1,                 #Necessary or else line plot disappears\n             text = paste0(\"Monthly Sales: $\", (round(AvgSales/1000,2)),\"K\" ))) +\n  geom_line(size = 1) + \n  scale_y_continuous(labels = scales::dollar_format(scale = .001, suffix = \"K\")) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%B\") + \n  labs(title = paste0(\"CRM and Invoicing System Sales For FY: \", params$year),\n       caption = \"Source: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv\",\n       x = NULL,\n       y = NULL) +\n  myTheme()\n\nggplotly(p, tooltip = c(\"text\")) %>% \n  layout(hovermode = \"x unified\")"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#required-libraries",
    "href": "posts/Param_Report_Retail/index.html#required-libraries",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Required Libraries",
    "text": "Required Libraries\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)\nlibrary(rmarkdown)\nlibrary(purrr)"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#other-reports-generated",
    "href": "posts/Param_Report_Retail/index.html#other-reports-generated",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Other Reports Generated",
    "text": "Other Reports Generated\n\n2020 Report\n\n\n\n\n2021 Report\n\n\n\n\n2022 Report"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#function-to-run-parameterized-reports",
    "href": "posts/Param_Report_Retail/index.html#function-to-run-parameterized-reports",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Function To Run Parameterized Reports",
    "text": "Function To Run Parameterized Reports\n\nrenderReport <- function(year) {\n  rmarkdown::render(\n    'input.Rmd',\n    output_file = paste0(year, '.html'),\n    params = list(year = year),\n    envir = parent.frame()\n  )\n}"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#render-all-reports",
    "href": "posts/Param_Report_Retail/index.html#render-all-reports",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Render ALL Reports",
    "text": "Render ALL Reports\n\n# Renders all 4 Reports (dates range from 2019-2022)\nfor (year in 2019:2022) {\n    renderReport(year)\n}"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#load-dataset-wrangle",
    "href": "posts/Param_Report_Retail/index.html#load-dataset-wrangle",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Load Dataset & Wrangle",
    "text": "Load Dataset & Wrangle\n\nds <- read_csv(\"retail.csv\")\n\n#head(ds)\n\nds <- ds %>% \n  rename(ID = ...1) %>%\n  mutate(Month = lubridate::floor_date(Date, 'month')) %>%\n  filter(year(Month) == params$year)\n\nglimpse(ds)\n\nRows: 10,042\nColumns: 9\n$ ID         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ DocumentID <dbl> 716, 716, 716, 716, 716, 716, 716, 460, 461, 462, 463, 464,…\n$ Date       <date> 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23…\n$ SKU        <dbl> 1039, 853, 862, 868, 2313, 2355, 2529, 2361, 2723, 655, 254…\n$ Price      <dbl> 381.78, 593.22, 423.73, 201.70, 345.76, 406.78, 542.38, 139…\n$ Discount   <dbl> 67.37254, 0.00034, -0.00119, 35.58814, 61.01966, 101.69458,…\n$ Customer   <dbl> 1, 1, 1, 1, 1, 1, 1, 460, 479, 26, 580, 311, 311, 311, 311,…\n$ Quantity   <dbl> 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 4,…\n$ Month      <date> 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01…"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#visualize-the-report",
    "href": "posts/Param_Report_Retail/index.html#visualize-the-report",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Visualize The Report",
    "text": "Visualize The Report\nI utilized ggplotly, a graphical representation tool, to create an interactive visualization of monthly sales time series data for “CRM and Invoicing system,” which is a wholesale company owned by Sadi Evren. The data for this analysis was obtained from the following Kaggle dataset: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv.\nThe resulting plot provided an insightful representation of the monthly sales data, showcasing trends and patterns in the data that could potentially provide useful information for decision making in the business.\nIn addition to the initial plot, I implemented a for loop to automatically generate multiple reports based on the time series data for each year. This approach eliminated the need for manual report generation, thereby saving time and reducing the risk of errors. The loop enabled the automated generation of separate reports for each year, which provided a comprehensive view of the sales trends over time.\nOverall, the use of ggplotly for data visualization and automation of report generation using a for loop demonstrated an effective approach for efficiently analyzing and presenting data.\n\np <- ds %>%\n  group_by(Month) %>%\n  summarize(AvgSales = round(mean(Price * Quantity),2) ) %>%\n  ggplot(aes(x = Month, \n             y = AvgSales,\n             group = 1,                 #Necessary or else line plot disappears\n             text = paste0(\"Monthly Sales: $\", (round(AvgSales/1000,2)),\"K\" ))) +\n  geom_line(size = 1) + \n  scale_y_continuous(labels = scales::dollar_format(scale = .001, suffix = \"K\")) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%B\") + \n  labs(title = paste0(\"CRM and Invoicing System Sales For FY: \", params$year),\n       caption = \"Source: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv\",\n       x = NULL,\n       y = NULL) +\n  myTheme()\n\nggplotly(p, tooltip = c(\"text\")) %>% \n  layout(hovermode = \"x unified\")"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#purpose",
    "href": "posts/Param_Report_Retail/index.html#purpose",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Purpose",
    "text": "Purpose\nFor this particular undertaking, I focused on the utilization of parameterized Rmarkdown with the aim of enhancing the efficacy of report generation. This approach involves setting predetermined values for parameters and creating a function that automatically updates the information, thereby mitigating the need for manual modification of parameters for daily reports. The primary objective of this system is to generate reports more efficiently, which in turn helps businesses save time in the long run. Furthermore, data scientists, analysts, and engineers can benefit from this system as it enables them to produce comparable reports over time with ease. The automation of the report generation process through the use of parameterized Rmarkdown contributes to increased productivity and accuracy, which can be highly beneficial to the success of a business."
  },
  {
    "objectID": "presentation.html#a-survival-analysis-approach-climate-change-is-changing-host-parasite-relationships",
    "href": "presentation.html#a-survival-analysis-approach-climate-change-is-changing-host-parasite-relationships",
    "title": "Presentations",
    "section": "A Survival Analysis Approach: Climate Change Is Changing Host-Parasite Relationships",
    "text": "A Survival Analysis Approach: Climate Change Is Changing Host-Parasite Relationships\nA study was conducted to investigate whether temperature has an impact on the toxicity of pesticides. Specifically, the study aimed to determine whether the effectiveness of common pesticides used to manage parasitic infections is compromised in different temperature settings. The researchers exposed a range of parasitic organisms to varying temperatures and measured the efficacy of different pesticides under these conditions. The results of the study indicated that temperature can indeed have a significant impact on the toxicity of pesticides, with some pesticides being less effective at higher temperatures. These findings have important implications for the management of parasitic infections, particularly in the context of climate change, and underscore the need for further research to better understand the impacts of changing environmental conditions on the efficacy of management strategies."
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#purpose",
    "href": "posts/Predicting_Income_Bracket/index.html#purpose",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Purpose",
    "text": "Purpose\nIn this project, we conducted a comprehensive machine learning analysis, encompassing various stages of the data science workflow, including data preprocessing, feature engineering, and feature selection through PCA. Collaboratively, we sought to predict whether individuals had an income greater than 50K. By incorporating hyperparameter optimization and deploying a robust model pipeline, we achieved a final Kappa score of 0.9543. This project enabled us to hone our skills as aspiring Data Scientists and paves the way for future machine learning endeavors."
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#highlight-of-the-project",
    "href": "posts/Predicting_Income_Bracket/index.html#highlight-of-the-project",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Highlight Of The Project",
    "text": "Highlight Of The Project\nWilla was instrumental in laying the foundation for the models and identifying the PCA features. Without her exceptional work, we would not have been able to create such a comprehensive and outstanding model for predicting income above $50,000. Her valuable contributions provided a critical starting point for our team, enabling us to add the feature importance at the start and optimize the models to their best parameters. Willa’s dedication, skill, and hard work were truly impressive, and I feel grateful to have had her as a partner on this project.\nHere is the direct link to her website. Take a look at her data science projects!"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#predicting-income-50k",
    "href": "posts/Predicting_Income_Bracket/index.html#predicting-income-50k",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Predicting Income >50K",
    "text": "Predicting Income >50K\n\nLoad Libraries\n\n#knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(caret)\nlibrary(tidymodels)\nlibrary(fastDummies)\nlibrary(randomForest)\n\n\n\nLoad The Data\n\nraw_income = read_csv(\"openml_1590.csv\", na = c(\"?\")) %>%\n  mutate(income_above_50k = ifelse(class == \">50K\",1,0))\n\nincome = read_csv(\"openml_1590.csv\", na = c(\"?\")) %>%\n  drop_na() %>%\n  mutate(income_above_50K = ifelse(class == \">50K\",1,0)) %>%\n  select(-class) %>%\n  dummy_cols(remove_selected_columns = T)"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#run-random-forest-obtain-importance-features",
    "href": "posts/Predicting_Income_Bracket/index.html#run-random-forest-obtain-importance-features",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Run Random Forest & Obtain Importance Features",
    "text": "Run Random Forest & Obtain Importance Features\n\nset.seed(504)\nraw_index <- createDataPartition(income$income_above_50K, p = 0.8, list = FALSE)\ntrain <- income[raw_index,]\ntest  <- income[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 3)\n\nfit <- train(income_above_50K ~ .,\n            data = train, \n            method = \"rf\",\n            ntree = 50,\n            tuneLength = 3,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n\nRandom Forest \n\n36178 samples\n  104 predictor\n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 24118, 24119, 24119 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n    2   0.3476024  0.4097953  0.2772094\n   53   0.3179101  0.4613186  0.1913666\n  104   0.3202068  0.4549034  0.1912386\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 53.\n\nprint(varImp(fit), 10)\n\nrf variable importance\n\n  only 10 most important variables shown (out of 104)\n\n                                    Overall\n`marital-status_Married-civ-spouse` 100.000\nfnlwgt                               88.391\n`capital-gain`                       72.132\nage                                  65.934\n`education-num`                      65.048\n`hours-per-week`                     38.148\nrelationship_Husband                 23.865\n`capital-loss`                       22.797\n`occupation_Exec-managerial`          8.631\n`occupation_Prof-specialty`           5.868"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#pca",
    "href": "posts/Predicting_Income_Bracket/index.html#pca",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "PCA",
    "text": "PCA\n\nChose Top 8 Features\n\ninc <- income %>%\n  select(-c(fnlwgt,\n            `marital-status_Married-civ-spouse`,\n            age,\n            `capital-gain`,\n            `education-num`,\n            `hours-per-week`,\n            relationship_Husband,\n            `capital-loss`))\n\n#Remained unchanged\npr_income = prcomp(x = inc, scale=T, center = T)\nscreeplot(pr_income, type=\"lines\")\n\n\n\nrownames_to_column(as.data.frame(pr_income$rotation)) %>%\n  select(1:11) %>%\n  filter(abs(PC1) >= 0.35 | abs(PC2) >= 0.35 | abs(PC3) >= 0.35 | abs(PC4) >= 0.35 | abs(PC5) >= 0.35 | abs(PC6) >= 0.35 | abs(PC7) >= 0.35 | abs(PC8) >= 0.35 | abs(PC9) >= 0.35 | abs(PC10) >= 0.35)\n\n                       rowname         PC1         PC2          PC3         PC4\n1            workclass_Private -0.17526595 -0.13547253  0.304797799  0.02401763\n2   workclass_Self-emp-not-inc  0.14801244  0.01168218 -0.067006960  0.07757365\n3       education_Some-college -0.06530044  0.06007649  0.046053650 -0.12180499\n4   relationship_Not-in-family -0.11626278  0.09345537 -0.009681895  0.02508551\n5            relationship_Wife -0.07643452  0.10243694 -0.155839801  0.10495583\n6                   race_Black -0.20749199 -0.06347990 -0.071867670 -0.26136856\n7                   sex_Female -0.43938132  0.23499209 -0.149918556  0.12969011\n8                     sex_Male  0.43938132 -0.23499209  0.149918556 -0.12969011\n9 native-country_United-States  0.08714546  0.46368406  0.199639060 -0.22938060\n          PC5          PC6         PC7         PC8         PC9        PC10\n1 -0.20886710  0.441798469 -0.22645965 -0.03914163 -0.04681505  0.00291428\n2  0.14138682 -0.254839748  0.35058106  0.05445338 -0.03103841 -0.33625026\n3 -0.07546707 -0.063088752  0.11632314  0.19569918  0.43973405  0.15884684\n4 -0.12317719 -0.049422098 -0.03404059 -0.57428375  0.20605726 -0.14712496\n5 -0.06292606  0.110184232  0.03508095  0.39257178 -0.25791740  0.12888672\n6  0.37153500 -0.037152844 -0.32702635  0.10727161  0.06843460 -0.15897839\n7 -0.10696874  0.005407197  0.07370032  0.08521373 -0.09415713 -0.03364547\n8  0.10696874 -0.005407197 -0.07370032 -0.08521373  0.09415713  0.03364547\n9  0.15037231  0.067618017 -0.02636173 -0.01036183 -0.06064057 -0.09505529\n\n\n\n\nChose First 10 PCA Features\n\n# IMPORTANT: Since I used 8 features, I updated the prc dataframe to include\n# the features + PCA 1-10\nprc <- \n  bind_cols(select(income, \n                   c(fnlwgt, \n                    `marital-status_Married-civ-spouse`, \n                    age, \n                    `capital-gain`, \n                    age, \n                    `hours-per-week`, \n                    relationship_Husband,\n                    `capital-loss`,\n                    income_above_50K)\n                   ), \n            as.data.frame(pr_income$x)\n            ) %>%\n  select(1:18) %>%\n  ungroup() %>%\n  rename(\"NonBlack_Men\" = PC1,\n         \"US_Women\" = PC2,\n         \"PrivateSec_Men\" = PC3,\n         \"NonUS_NonBlack\" = PC4,\n         \"NonPrivateSec_Black\" = PC5,\n         \"PrivateSec\" = PC6,\n         \"NonBlack_SelfEmploy\" = PC7,\n         \"Wives\" = PC8,\n         \"NonFamily_SomeCollege\" = PC9,\n         \"NotSelfEmployes_NonBlack\" = PC10)\n\nhead(prc)\n\n# A tibble: 6 × 18\n  fnlwgt marital-status_Married-civ-spou…¹   age `capital-gain` `hours-per-week`\n   <dbl>                             <int> <dbl>          <dbl>            <dbl>\n1 226802                                 0    25              0               40\n2  89814                                 1    38              0               50\n3 336951                                 1    28              0               40\n4 160323                                 1    44           7688               40\n5 198693                                 0    34              0               30\n6 104626                                 1    63           3103               32\n# ℹ abbreviated name: ¹​`marital-status_Married-civ-spouse`\n# ℹ 13 more variables: relationship_Husband <int>, `capital-loss` <dbl>,\n#   income_above_50K <dbl>, NonBlack_Men <dbl>, US_Women <dbl>,\n#   PrivateSec_Men <dbl>, NonUS_NonBlack <dbl>, NonPrivateSec_Black <dbl>,\n#   PrivateSec <dbl>, NonBlack_SelfEmploy <dbl>, Wives <dbl>,\n#   NonFamily_SomeCollege <dbl>, NotSelfEmployes_NonBlack <dbl>"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#gradient-boosting-machine",
    "href": "posts/Predicting_Income_Bracket/index.html#gradient-boosting-machine",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Gradient Boosting Machine",
    "text": "Gradient Boosting Machine\n\n#IMPORTANT: I took a while and messed around with the hyperparameters\n# Went From 0.2 Kappa to 0.6 Kappa BEFORE updating the features.\n# After updating to the top 8 features + PCA 1-5, it jumped to \n# 0.88 Kappa. Then I added PCA 1-10 and it jumped to 0.95 for the Kappa!\nset.seed(504)\nraw_index <- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain <- prc[raw_index,]\ntest  <- prc[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 5)\nweights <- ifelse(income$income_above_50K == 1, 75, 25)\n\nhyperparameters <- expand.grid(interaction.depth = 9, \n                    n.trees = 300, \n                    shrinkage = 0.1, \n                    n.minobsinnode = 4)\nfit <- train(factor(income_above_50K) ~ .,\n            data = train, \n            method = \"gbm\",\n            verbose = FALSE,\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n\nStochastic Gradient Boosting \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9827796  0.9533806\n\nTuning parameter 'n.trees' was held constant at a value of 300\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 4\n\n\n\nConfusion Matrix For GBM\n\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6798  111\n         1   42 2093\n                                          \n               Accuracy : 0.9831          \n                 95% CI : (0.9802, 0.9856)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9536          \n                                          \n Mcnemar's Test P-Value : 3.853e-08       \n                                          \n            Sensitivity : 0.9939          \n            Specificity : 0.9496          \n         Pos Pred Value : 0.9839          \n         Neg Pred Value : 0.9803          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7517          \n   Detection Prevalence : 0.7639          \n      Balanced Accuracy : 0.9717          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#logistical-model",
    "href": "posts/Predicting_Income_Bracket/index.html#logistical-model",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Logistical Model",
    "text": "Logistical Model\n\n#I messed around with using a logistical model\n#It turns out that it's pretty good too! Not as great as the GBM\n#But a great and easy model to explain!\n\nset.seed(504)\nraw_index <- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain <- prc[raw_index,]\ntest  <- prc[-raw_index, ]\nctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 3, verboseIter = FALSE)\nhyperparameters <- expand.grid(alpha = 1, \n                               lambda = 0.001)\n\nfit <- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"glmnet\",\n            family = \"binomial\",\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\",\n            importance = TRUE)\nfit\n\nglmnet \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 32560, 32561, 32561, 32561, 32559, 32559, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.9610166  0.895033\n\nTuning parameter 'alpha' was held constant at a value of 1\nTuning\n parameter 'lambda' was held constant at a value of 0.001\n\n\n\nConfusion Matrix For Logistical Regression\n\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6679  197\n         1  161 2007\n                                          \n               Accuracy : 0.9604          \n                 95% CI : (0.9562, 0.9643)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.892           \n                                          \n Mcnemar's Test P-Value : 0.06434         \n                                          \n            Sensitivity : 0.9765          \n            Specificity : 0.9106          \n         Pos Pred Value : 0.9713          \n         Neg Pred Value : 0.9257          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7385          \n   Detection Prevalence : 0.7603          \n      Balanced Accuracy : 0.9435          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#kmeans-clustering",
    "href": "posts/Predicting_Income_Bracket/index.html#kmeans-clustering",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\n\nkclust <- kmeans(na.omit(prc), centers = 4)\nkclust$centers\n\n    fnlwgt marital-status_Married-civ-spouse      age capital-gain\n1  86931.3                         0.4798800 39.36223     1178.652\n2 307822.2                         0.4480979 37.29197      993.978\n3 486698.8                         0.4317060 35.22655     1093.921\n4 188848.4                         0.4654970 38.77250     1090.494\n  hours-per-week relationship_Husband capital-loss income_above_50K\n1       41.21239            0.4228618     85.99175        0.2453963\n2       40.99760            0.4015361     87.95284        0.2414497\n3       40.18760            0.3812397     69.68404        0.2144816\n4       40.78357            0.4129092     92.41845        0.2551951\n  NonBlack_Men    US_Women PrivateSec_Men NonUS_NonBlack NonPrivateSec_Black\n1   0.06956303  0.10628238  -0.0890912042    -0.03258891        -0.008490584\n2  -0.04531689 -0.15055893   0.0006150531    -0.02235042         0.014458490\n3  -0.20329023 -0.44646489   0.1964744251     0.04160186         0.115358306\n4  -0.01331648  0.02500494   0.0462143267     0.02882888        -0.010110363\n   PrivateSec NonBlack_SelfEmploy        Wives NonFamily_SomeCollege\n1  0.03384552          0.17173996 -0.029902238           -0.02188199\n2 -0.07527701         -0.12068976 -0.008023897            0.09248884\n3 -0.26626813         -0.36982627  0.123605508            0.13942943\n4  0.03021208         -0.04107775  0.013720687           -0.03450577\n  NotSelfEmployes_NonBlack\n1              -0.03597294\n2               0.01302787\n3              -0.02812971\n4               0.02304227\n\nkclusts <- tibble(k = 1:9) %>%\n  mutate(\n    kclust = map(k, ~kmeans(prc, .x)),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, prc)\n  )\n\nclusterings <- kclusts %>%\n  unnest(glanced, .drop = TRUE)\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line()"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#augumenting-the-gbm-model-with-kmeans-clustering",
    "href": "posts/Predicting_Income_Bracket/index.html#augumenting-the-gbm-model-with-kmeans-clustering",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Augumenting The GBM Model with KMeans Clustering",
    "text": "Augumenting The GBM Model with KMeans Clustering\n\nprc2 <- augment(kclust, prc)\n\nset.seed(504)\nraw_index <- createDataPartition(prc2$income_above_50K, p = 0.8, list = FALSE)\n\ntrain <- prc2[raw_index,]\ntest  <- prc2[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 5)\n\nhyperparameters <- expand.grid(\n  n.trees = 500,\n  interaction.depth = 5,\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n\n\nfit <- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"gbm\",\n            trControl = ctrl,\n            tuneGrid = hyperparameters,\n            verbose = FALSE)\nfit\n\nStochastic Gradient Boosting \n\n36178 samples\n   18 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9836365  0.9557243\n\nTuning parameter 'n.trees' was held constant at a value of 500\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\n\n\n\nConfusion Matrix For KMeans + GBM\n\n#We should be getting a Kappa of 0.9543!\n#Sensitivity = 0.9930, Specificity = 0.9533\n#Excellent Numbers!\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6792  103\n         1   48 2101\n                                          \n               Accuracy : 0.9833          \n                 95% CI : (0.9804, 0.9858)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9543          \n                                          \n Mcnemar's Test P-Value : 1.11e-05        \n                                          \n            Sensitivity : 0.9930          \n            Specificity : 0.9533          \n         Pos Pred Value : 0.9851          \n         Neg Pred Value : 0.9777          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7510          \n   Detection Prevalence : 0.7624          \n      Balanced Accuracy : 0.9731          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#results",
    "href": "posts/Predicting_Income_Bracket/index.html#results",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Results",
    "text": "Results\nWe used a random forest model to identify the most important variables and selected the top 8 features, then utilized principal component analysis (PCA) to further analyze the data. Our final model incorporated a gradient boosting machine (GBM) algorithm with optimized hyperparameters and achieved an accuracy rate of 0.9829731 and a Kappa score of 0.9538928. To ensure the model was not overfitting, we benchmarked it with a logistic regression model that achieved a Kappa score of 0.895033 and an accuracy rate of 0.9610166. We further improved the model’s accuracy by incorporating unsupervised machine learning with Kmeans clustering, achieving a Kappa score of 0.9543 and an accuracy rate of 0.9833. Overall, our approach of feature selection and PCA was effective and could be applied to future data analysis projects with some additional fine-tuning."
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#question",
    "href": "posts/AmazonProductsEN/index.html#question",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Question",
    "text": "Question\n\nWhat is the average product rating for each main category and subcategory?"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#load-libraries",
    "href": "posts/AmazonProductsEN/index.html#load-libraries",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\nlibrary(plotly)\nlibrary(DT)\noptions(scipen = 999)"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#part-1-data-wrangling-multiple-datasets",
    "href": "posts/AmazonProductsEN/index.html#part-1-data-wrangling-multiple-datasets",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 1: Data Wrangling Multiple Datasets",
    "text": "Part 1: Data Wrangling Multiple Datasets\n\n#ORIGINAL DATA WRANGLING \n\ndatasets <- as.data.frame(list.files(path = \"~/Documents/ShinyApps/AmazonProducts/AmazonProductApp\", pattern = \"csv\"))\ncolnames(datasets) <- \"Datasets\"\n\n#Combine all the datasets\nfor (i in length(nrow(datasets))){\n combinedDs <- read_csv(datasets[[i]])\n}\n\namazonProducts <- combinedDs %>%\n mutate(Name = name,\n       MainCategory = factor(str_to_title((sort(main_category)))),\n        SubCategory = factor(sort(sub_category)),\n        ProductImage = image,\n        ProductRating = as.numeric(ratings),\n        NumberOfRatings = as.numeric(gsub(\"\\\\,\",\"\",no_of_ratings)),\n        DiscountPrice = round(as.numeric(gsub(\"[\\\\₹,]\", \"\", discount_price)) / 81.85, 2), #convert from Rupee to USD\n        Price = round(as.numeric(gsub(\"[\\\\₹,]\", \"\", actual_price)) / 81.85, 2),           #convert from Rupee to USD\n        ProductLink = link) %>%\n select(-c(name, \n           main_category, \n           sub_category, \n           image, \n           ratings, \n           no_of_ratings, \n           discount_price, \n           actual_price,\n           link)) %>% \n drop_na() %>%\n filter(!str_detect(SubCategory, \"^All \"))\n\namazonProducts %>%\n write_csv(\"amazonProducts.csv\")\n\n\nReload Data\n\nproducts <- read_csv(\"amazonProducts.csv\")"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#part-2-data-wrangling-for-visualization",
    "href": "posts/AmazonProductsEN/index.html#part-2-data-wrangling-for-visualization",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 2: Data Wrangling For Visualization",
    "text": "Part 2: Data Wrangling For Visualization\n\nproducts <- products %>% mutate(DiscountPrice = round(DiscountPrice82.04,2), Price = round(Price82.04,2))\n\n#For Plot 1\nratings <- products %>% select(-c(ProductImage, Name, ProductLink)) %>% group_by(MainCategory,SubCategory) %>% summarise(AverageRating = mean(ProductRating)) %>% ungroup()\n\nreviews <- products %>% select(-c(ProductImage, Name, ProductLink)) %>% group_by(MainCategory,SubCategory) %>% summarise(AverageReview = mean(NumberOfRatings)) %>% ungroup()\n\n#For Plot 2 \ntop10Products <- products %>% filter(ProductRating > 4.5, NumberOfRatings > 50) %>% group_by(SubCategory) %>% arrange(desc(ProductRating)) %>% slice(1:10) %>% select(-c(ProductImage, Name, ProductLink))\n\n#unique(products$MainCategory)\n\n\nColor Theming\n\n#Plot 1 \nnum_colors <- 21 \ncolors <- c(\"#f2f2f2\", \"#ff9900\") \npal1 <- colorRampPalette(colors)(num_colors)\n\nprint(pal1)\n\n#Plot 2 \nnum_colors <- 21 \ncolors <- colors <- c(\"#f2f2f2\",\"#00a8e1\") \npal2 <- colorRampPalette(colors)(num_colors)"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#part-1-shiny-ui",
    "href": "posts/AmazonProductsEN/index.html#part-1-shiny-ui",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 1: Shiny UI",
    "text": "Part 1: Shiny UI\n\n# Define UI for application \nui <- fluidPage(\n  \n  #Background CSS\n  tags$head(tags$style(HTML('\n  @import url(https://fonts.googleapis.com/css?family=Montserrat&display=swap);\n    body {\n      font-family: Montserrat, sans-serif;\n      background-color: #FF9900;\n    }\n    .dataTables_wrapper {\n      background-color: #fff;\n    }\n    .sidebar {\n      background-color: #fff;\n      width: 3/12;\n      height: 2/12;\n    }\n    .nav-tabs > li > a {\n      color: black;\n      background-color: #00a8e1;\n      border-color: #00a8e1;\n    }'))),\n  \n  # Application title\n  titlePanel(\"Amazon Inc. Product Dashboard (EN.)\"),\n  \n  # Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"MainCategoryChoice\", \n                  label = h3(\"Select Category:\"), \n                  choices = unique(products$MainCategory), \n                  selected = \"Accessories\"),\n      \n      uiOutput(\"SubCategoryChoice\")\n    ),\n    \n    # Tabs\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Plot\", \n                 plotlyOutput(\"RatingsPlot\"),\n                 plotlyOutput(\"ReviewsPlot\")),\n        tabPanel(\"Data\", dataTableOutput(\"myDataTable\"))\n      )\n    )\n  )\n)"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#part-2-shiny-server",
    "href": "posts/AmazonProductsEN/index.html#part-2-shiny-server",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 2: Shiny Server",
    "text": "Part 2: Shiny Server\n\n# Define server logic required to draw Dashboard\nserver <- function(input, output) {\n  \n  # Subcategory choices\n  output$SubCategoryChoice <- renderUI({\n    subcategories <- unique(products$SubCategory[products$MainCategory == input$MainCategoryChoice])\n    checkboxGroupInput(\"SubCategoryChoice\", \n                       label = h3(\"Select Subcategories:\"), \n                       choices = subcategories, \n                       selected = subcategories)\n  })\n  \n  # Plot 1: Product Rating\n  output$RatingsPlot <- renderPlotly({\n    ratings %>%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %>% \n      mutate(SubCategory_ordered = factor(SubCategory, \n                                          levels = unique(SubCategory[order(AverageRating)]))) %>%\n      plot_ly(x = ~SubCategory_ordered, \n              y = ~round(AverageRating,2),\n              type = 'bar',\n              marker = list(color = ~pal1[SubCategory_ordered])) %>%\n      add_annotations(x = ~SubCategory_ordered,\n                      y = ~round(AverageRating,2),\n                      text = ~paste0(round(AverageRating,2)),\n                      font = list(color = 'black', \n                                  size = 10),\n                      showarrow = FALSE,\n                      yshift = 5) %>%\n      layout(title = paste0(\"Average Product Rating For \",input$MainCategoryChoice),\n             xaxis = list(title = \"\", tickangle = 45),\n             yaxis = list(title = \"\"),\n             showlegend = FALSE,\n             margin = list(t = 50,\n                           l = 50,\n                           r = 50,\n                           b = 50)) \n  })\n  \n  # Plot 2: Product Reviews\n  output$ReviewsPlot <- renderPlotly({\n    reviews %>%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %>% \n      mutate(SubCategory_ordered = factor(SubCategory, \n                                          levels = unique(SubCategory[order(AverageReview)]))) %>%\n      plot_ly(x = ~SubCategory_ordered, \n              y = ~round(AverageReview),\n              type = 'bar',\n              marker = list(color = ~pal2[SubCategory_ordered])) %>%\n      add_annotations(x = ~SubCategory_ordered,\n                      y = ~round(AverageReview),\n                      text = ~paste0(round(AverageReview)),\n                      font = list(color = 'black', \n                                  size = 10),\n                      showarrow = FALSE,\n                      yshift = 5) %>%\n      layout(title = paste0(\"Average Number of Reviews For \",input$MainCategoryChoice),\n             xaxis = list(title = \"\", tickangle = 45),\n             yaxis = list(title = \"\"),\n             showlegend = FALSE,\n             margin = list(t = 50,\n                           l = 50,\n                           r =50,\n                           b = 50)) \n  })\n  \n  output$myDataTable <- DT::renderDataTable({\n    products %>%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %>%\n      mutate(ProductImage = sprintf('<img src=\"%s\" width=\"75px\"/>', ProductImage)) %>%\n      DT::datatable(., escape = FALSE, options = list(\n        pageLength = 10,\n        lengthMenu = c(5, 10, 25),\n        scrollY = \"600px\",\n        scrollX = TRUE\n      )) %>%\n      DT::formatStyle(columns = colnames(products), \n                      backgroundColor = styleEqual(c(\"green\", \"white\"), c(\"rgb(51, 102, 0)\", \"rgb(255, 255, 255)\")))\n  })\n  \n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#questions",
    "href": "posts/AmazonProductsEN/index.html#questions",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average rating for each category and subcategory?\nWhat is the average review for each category and subcategory?\n\n\n\nFullscreen"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html",
    "href": "posts/AmazonProductsEN/index.html",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "",
    "text": "## Purpose\nIn this project, I developed a Shiny application that analyzed Amazon products available in India. The dataset used in this project was obtained from https://www.kaggle.com/datasets/lokeshparab/amazon-products-dataset?select=Amazon+Pharmacy.csv, and it presented several challenges during the development process. While I was not able to fix all the issues with the product categories due to time constraints, I focused on creating a concise and informative dashboard that displayed the categories with the highest average ratings and reviews.\nCompared to my previous Shiny app, this project was developed more efficiently, taking approximately 70% less time. Additionally, I was able to implement more advanced CSS theming than before, which improved the overall design of the application. While there are still additional features and visualizations that could be added to the app, I decided to leave them for others to explore in the future.\nOverall, this project was a challenging but rewarding experience that allowed me to build my skills in data analysis and Shiny app development."
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#further-improvement-questions",
    "href": "posts/AmazonProductsEN/index.html#further-improvement-questions",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Further Improvement Questions",
    "text": "Further Improvement Questions\n\nIs there a correlation between the number of ratings and the product rating?\nWhat is the average discount percentage for each main category and subcategory?\nWhat is the price range for each main category and subcategory?\nWhich products have the highest ratings and how do they compare in terms of price and number of ratings?"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#purpose",
    "href": "posts/AmazonProductsEN/index.html#purpose",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Purpose",
    "text": "Purpose\nIn this project, I developed a Shiny application that analyzed Amazon products available in India. The dataset used in this project was obtained from Kaggle, and it presented several challenges during the development process. While I was not able to fix all the issues with the product categories due to time constraints, I focused on creating a concise and informative dashboard that displayed the categories with the highest average ratings and reviews.\nCompared to my previous Shiny app, this project was developed more efficiently, taking approximately 70% less time. Additionally, I was able to implement more advanced CSS theming than before, which improved the overall design of the application. While there are still additional features and visualizations that could be added to the app, I decided to leave them for others to explore in the future.\nOverall, this project was a challenging but rewarding experience that allowed me to build my skills in data analysis and Shiny app development."
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#purpose",
    "href": "posts/AppleInc_IncomeStatement/index.html#purpose",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Purpose",
    "text": "Purpose\nThe primary objective of this undertaking was to delve into the application of R’s plotly and create dynamic and interactive visualizations of Time Series Graphs. In this regard, I opted to scrutinize the Apple stock prices spanning the period from 1981 to the present, with a keen focus on identifying any notable trends that have emerged over time. Upon a closer inspection of the data, I observed a remarkable spike in the company’s stock prices in the aftermath of 2010, which I attribute to the resounding success of Apple’s iPhone and the accompanying products.\nThe shift in Apple’s market value was not just a result of its breakthrough innovation but was also influenced by its impeccable marketing strategy that has ensured that their products always remain relevant to the ever-changing consumer needs. It is noteworthy that Apple’s transformative technology has revolutionized the tech industry and has made significant contributions to its economic growth. Overall, the utilization of R’s plotly in the visualization of Time Series Graphs proved to be an incredibly insightful undertaking, helping to identify and explain the trends in the stock market in a more meaningful way."
  },
  {
    "objectID": "posts/Customer Return ML/index.html#purpose",
    "href": "posts/Customer Return ML/index.html#purpose",
    "title": "Predicting Customer Returns",
    "section": "Purpose",
    "text": "Purpose\nIn this evaluation, I scrutinized a partially randomly generated customer returns dataset with the objective of forecasting whether a customer would opt to return their purchased item. By employing progressive techniques that advanced from Logistical to Random Forest, I achieved a robust prediction of customer behavior with respect to product returns."
  }
]