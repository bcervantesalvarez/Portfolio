[
  {
    "objectID": "posts/FraudML/index.html#abstract",
    "href": "posts/FraudML/index.html#abstract",
    "title": "Boosting Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Abstract",
    "text": "Abstract\nFraud detection is a challenging task, but advancements are being made each year. This project aims to improve fraud detection using a Gradient Boosting Machine (GBM) model. The dataset is prepared, and the model is trained and evaluated on validation and test sets. Performance metrics such as accuracy, precision, recall, AUC, and Kappa coefficient are computed. A comparison is made with the existing ‘isFlaggedFraud’ column model. The GBM model demonstrates higher performance in various aspects, highlighting its superiority in classifying fraudulent transactions. However, further improvements are needed to enhance fraud detection while maintaining precision."
  },
  {
    "objectID": "posts/FraudML/index.html#introduction",
    "href": "posts/FraudML/index.html#introduction",
    "title": "Boosting Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Introduction",
    "text": "Introduction\nFraudulent activities pose a significant threat in various industries, including financial transactions. Detecting fraudulent transactions is a complex task, but advancements in machine learning techniques have shown promise in improving accuracy and precision. In this project, we focus on enhancing fraud detection using a Gradient Boosting Machine (GBM) model."
  },
  {
    "objectID": "posts/FraudML/index.html#methodology",
    "href": "posts/FraudML/index.html#methodology",
    "title": "Boosting Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Methodology",
    "text": "Methodology\nFirst, we set up the environment by importing necessary libraries and loading the dataset. The dataset, named ‘Card-Transaction_log.csv’, contains relevant information for training and evaluation. We explore the dataset by printing the first few rows and examining its shape.\nNext, we perform data wrangling by applying one-hot encoding to the features, excluding ‘isFraud’, ‘nameOrig’, ‘nameDest’, and ‘isFlaggedFraud’ columns. This process prepares the data for training the GBM model.\nTo evaluate the model’s performance, we split the data into training, validation, and test sets. The random seed is set for reproducibility. The GBM model is trained on the training set using the specified parameters, such as loss function, learning rate, number of estimators, and maximum depth. The model’s training time is recorded for analysis.\nWe then evaluate the GBM model on the validation set to measure its accuracy, precision, recall, confusion matrix, AUC, and Kappa coefficient. These metrics provide insights into the model’s performance in classifying fraudulent and non-fraudulent transactions.\nFurther, we assess the GBM model’s performance on the test set and compute the corresponding metrics. The test accuracy, precision, recall, confusion matrix, AUC, and Kappa coefficient provide a comprehensive evaluation of the model’s ability to classify transactions accurately.\nTo compare the GBM model with the existing ‘isFlaggedFraud’ column model, we calculate the accuracy, precision, and recall scores for the ‘isFlaggedFraud’ column. This analysis allows us to assess the GBM model’s superiority in detecting fraudulent transactions.\nBased on the results, we discuss the GBM model’s performance, highlighting its accuracy, precision, recall, AUC, and Kappa coefficient. We analyze the confusion matrix to identify areas where the model misclassifies transactions. While the GBM model outperforms the ‘isFlaggedFraud’ column model in several aspects, it still has room for improvement in detecting fraudulent instances while maintaining high precision.\n\n\n\n\n\n\nCaution\n\n\n\nEnsure you are using the correct Python version: Python 3.10.9 (‘base’) ~/anaconda3/bin/python\n\n\n\nSet up\n\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.ensemble import GradientBoostingClassifier  # For classification tasks\nfrom sklearn.ensemble import GradientBoostingRegressor  # For regression tasks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n    roc_curve,\n    cohen_kappa_score,\n)\n\nds = pd.read_csv('Card-Transaction_log.csv')\n\n# Print the first 5 rows\nprint(ds.head(5))\n\n# Print the shape of the frame\nprint(ds.shape)\n\n   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n\n      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n0  M1979787155             0.0             0.0        0               0  \n1  M2044282225             0.0             0.0        0               0  \n2   C553264065             0.0             0.0        1               0  \n3    C38997010         21182.0             0.0        1               0  \n4  M1230701703             0.0             0.0        0               0  \n(6362620, 11)\n\n\n\n\nData Wrangling\n\nX = pd.get_dummies(ds.drop(['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1))   # Perform one-hot encoding on the features\ny = ds['isFraud']\n\n\n\n\n\n\n\nNote\n\n\n\nThis dataset was already cleaned and tidy, so no additional wrangling was needed.\n\n\n\n\nSplit the data into training, validation, and test sets\n\n# Set the random seed\nnp.random.seed(57)\n\n# Training split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n\n\n# Train the model\nstart_time = time.time()  # Start the timer\n\ngbm = GradientBoostingClassifier(loss='log_loss', learning_rate=0.1, n_estimators=100, max_depth=3)\ngbm.fit(X_train, y_train)\n\nend_time = time.time()  # Stop the timer\n\nelapsed_time = end_time - start_time\nprint(f\"Elapsed Time: {elapsed_time} seconds\")\n\nElapsed Time: 1025.5563502311707 seconds\n\n\n\n\nEvaluate on the validation set\n\ny_pred_val = gbm.predict(X_val)\naccuracy_val = accuracy_score(y_val, y_pred_val)\nprecision_val = precision_score(y_val, y_pred_val)\nrecall_val = recall_score(y_val, y_pred_val)\nconfusion_matrix_val = confusion_matrix(y_val, y_pred_val)\nauc_val = roc_auc_score(y_val, y_pred_val)\nfpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_pred_val)\nkappa_val = cohen_kappa_score(y_val, y_pred_val)\n\nprint(f\"Validation Accuracy: {accuracy_val}\")\nprint(f\"Validation Precision: {precision_val}\")\nprint(f\"Validation Recall: {recall_val}\")\nprint(\"Validation Confusion Matrix:\")\nprint(confusion_matrix_val)\nprint(f\"Validation AUC: {auc_val}\")\nprint(f\"Validation Kappa: {kappa_val}\")\n\nValidation Accuracy: 0.9993241777758219\nValidation Precision: 0.9693396226415094\nValidation Recall: 0.4963768115942029\nValidation Confusion Matrix:\n[[635421     13]\n [   417    411]]\nValidation AUC: 0.7481781765679447\nValidation Kappa: 0.6562465275067701\n\n\n\n\nEvaluate on the test set\n\ny_pred_test = gbm.predict(X_test)\naccuracy_test = accuracy_score(y_test, y_pred_test)\nprecision_test = precision_score(y_test, y_pred_test)\nrecall_test = recall_score(y_test, y_pred_test)\nconfusion_matrix_test = confusion_matrix(y_test, y_pred_test)\nauc_test = roc_auc_score(y_test, y_pred_test)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_test)\nkappa_test = cohen_kappa_score(y_test, y_pred_test)\n\nprint(f\"Test Accuracy: {accuracy_test}\")\nprint(f\"Test Precision: {precision_test}\")\nprint(f\"Test Recall: {recall_test}\")\nprint(\"Test Confusion Matrix:\")\nprint(confusion_matrix_test)\nprint(f\"Test AUC: {auc_test}\")\nprint(f\"Test Kappa: {kappa_test}\")\n\nTest Accuracy: 0.9992958875431819\nTest Precision: 0.9455958549222798\nTest Recall: 0.46085858585858586\nTest Confusion Matrix:\n[[635449     21]\n [   427    365]]\nTest AUC: 0.7304127697259946\nTest Kappa: 0.6193839067580158\n\n\n\n\nHow does this compare to the current model?\n\n# Comparison with 'isFlaggedFraud' column\nis_flagged_fraud_accuracy = accuracy_score(ds['isFraud'], ds['isFlaggedFraud'])\nis_flagged_fraud_precision = precision_score(ds['isFraud'], ds['isFlaggedFraud'])\nis_flagged_fraud_recall = recall_score(ds['isFraud'], ds['isFlaggedFraud'])\n\nprint(f\"Accuracy (isFlaggedFraud): {is_flagged_fraud_accuracy}\")\nprint(f\"Precision (isFlaggedFraud): {is_flagged_fraud_precision}\")\nprint(f\"Recall (isFlaggedFraud): {is_flagged_fraud_recall}\")\n\nAccuracy (isFlaggedFraud): 0.9987116942391656\nPrecision (isFlaggedFraud): 1.0\nRecall (isFlaggedFraud): 0.0019481310118105442"
  },
  {
    "objectID": "posts/FraudML/index.html#results",
    "href": "posts/FraudML/index.html#results",
    "title": "Boosting Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Results",
    "text": "Results\nBased on the evaluation of the GBM (Gradient Boosting Machine) model on the test set, the following performance metrics were obtained:\n\nTest Accuracy: 0.9992958875431819\nTest Precision: 0.9455958549222798\nTest Recall: 0.46085858585858586\nTest AUC: 0.7304127697259946\nTest Kappa: 0.6193839067580158\n\nThe GBM model achieved a high accuracy score of 0.999, indicating a strong ability to correctly classify transactions in the test set. With a precision of 0.946, the GBM model accurately identified approximately 94.6% of the predicted fraudulent transactions. However, the recall score of 0.461 suggests that the GBM model only captured around 46.1% of the actual fraudulent transactions.\nAnalyzing the confusion matrix, the GBM model correctly classified a large number of non-fraudulent transactions (true negatives) and a significant portion of fraudulent transactions (true positives). However, there were some instances where the GBM model misclassified non-fraudulent transactions as fraudulent (false positives) and failed to identify certain fraudulent transactions (false negatives).\nThe AUC score of 0.730 indicates a moderate level of discrimination between fraudulent and non-fraudulent transactions for the GBM model. Although the GBM model demonstrates substantial agreement beyond chance with a Kappa coefficient of 0.619, there is room for improvement in its ability to detect fraudulent instances while maintaining high precision.\n\nComparison with Original Model\nComparing the GBM model with the ‘isFlaggedFraud’ column model, the GBM model outperforms in several key aspects. It achieves higher accuracy, precision, recall, and AUC scores, indicating superior overall performance in classifying fraudulent transactions. The ‘isFlaggedFraud’ column model, although having perfect precision, only detects a very small number of actual fraud cases, resulting in low recall."
  },
  {
    "objectID": "posts/FraudML/index.html#limitations-and-further-study",
    "href": "posts/FraudML/index.html#limitations-and-further-study",
    "title": "Boosting Fraud Detection: Enhancing Accuracy with GBM",
    "section": "Limitations and Further Study",
    "text": "Limitations and Further Study\nIt is important to consider the specific goals and requirements of the application. While the GBM model provides better overall performance, it may have a higher number of false positives compared to the ‘isFlaggedFraud’ column model. Further enhancements can be made to improve the GBM model’s ability to detect fraudulent transactions while maintaining high precision.\nThese results provide valuable insights into the GBM model’s performance on unseen data, indicating its generalization capability. It is crucial to consider the test set performance as the final evaluation of the GBM model’s effectiveness in real-world scenarios."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Cervantes Alvarez",
    "section": "",
    "text": "Mathematician by Training, Data Scientist by Passion: My Journey of Self-Discovery\nWith a solid foundation in mathematics from my Bachelor of Arts degree and nearing completion of my Master’s degree in Data Science, I have embarked on an exhilarating path towards becoming a next-generation data scientist. My journey has been fueled by a deep-rooted passion for unraveling insights hidden within data.\nThroughout my academic pursuits, I have delved into a multitude of areas, including machine learning and survival analysis, honing my analytical skills and developing a keen eye for patterns and trends. Equipped with the ability to effectively communicate data-driven insights, I strive to bridge the gap between complex datasets and actionable solutions.\nMy journey has been characterized by a relentless pursuit of knowledge and growth. I am dedicated to staying abreast of the latest advancements in the dynamic field of data science, as I believe that continuous learning is the key to unlocking new possibilities. Embracing challenges with enthusiasm, I am eager to apply my expertise in statistical analysis, data visualization, and problem-solving to tackle real-world complexities.\nAs I near the culmination of my master’s degree, I am poised to embark on a professional career where I can leverage my skills to make a meaningful impact. I am eager to collaborate with experienced professionals in the industry, learn from their expertise, and contribute to innovative projects that push the boundaries of data science.\nMy journey as a mathematician-turned-data scientist has been one of self-discovery, empowerment, and unwavering determination. I am excited to forge ahead, armed with a potent combination of mathematical acumen and a passion for data science, ready to create solutions that drive progress and shape a data-powered future.\n\n\nFrom Playgrounds to Algorithms: Charting the Evolution of a Data Science Whiz\nI was born in Pasco, Washington, a small city in the southeastern part of the state. Pasco is part of the Tri-Cities area, along with Richland and Kennewick, and is known for its agricultural industry, particularly the cultivation of potatoes, apples, and wine grapes.\n\n\n\nPasco, Washington\n\n\nI enjoyed the rural atmosphere of Pasco, with its open fields, orchards, and vineyards.\nBefore finishing 1st grade, I moved to Salem, Oregon, a mid-sized city in the heart of the Willamette Valley. Salem is known for its farms and vineyards that produce some of the best wine and food in the state.\n\n\n\nSalem, Oregon\n\n\nGrowing up in Salem, I was immersed in the outdoors and enjoyed activities such as hiking, camping, and running. Salem also has a rich history, with landmarks like the Oregon State Capitol and Willamette University, which is the oldest university in the west. However, as I got older, I also became aware of some of the social and economic challenges faced by many families in the area, such as poverty, limited job opportunities, and inadequate access to healthcare.\nMy family and I suffered poverty and food instability before I graduated high school. Despite these challenges, I feel a strong connection to Salem and its people, and I am grateful for the experiences and values that I gained from growing up in this community."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis On A Rare Disease\n\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\nRandom Forest\n\n\nClassification\n\n\n\n\nUsing symptom analysis, this study employs a Random Forest approach to predict severity levels in patients, aiming to enhance healthcare decision-making.\n\n\n\n\n\n\nJune 26, 2023\n\n\nBrian Cervantes Alvarez\n\n\n20 min\n\n\n\n\n\n\n  \n\n\n\n\nBoosting Fraud Detection: Enhancing Accuracy with GBM\n\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\nGradient Boosting Machines\n\n\n\n\nI developed a model that enhances fraud detection in this dataset, contributing to ongoing advancements in this challenging task\n\n\n\n\n\n\nMay 30, 2023\n\n\nBrian Cervantes Alvarez\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nBuilding A Quick Dashboard For Amazon Products (EN.)\n\n\n\n\n\n\n\nR\n\n\nShiny\n\n\nCSS\n\n\nData Tables\n\n\nData Visualization\n\n\nDashboard\n\n\n\n\nExplore Amazon products in India with our Shiny app. Discover categories with the highest ratings and reviews. Efficient, informative, and visually appealing.\n\n\n\n\n\n\nApril 21, 2023\n\n\nBrian Cervantes Alvarez\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Individuals Who Earn More Than 50K\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nGradient Boosting Machines\n\n\nLogistic Regression\n\n\nClassification\n\n\n\n\nWe employed advanced techniques in our project, using a random forest model to identify important variables and selecting the top 8 features.\n\n\n\n\n\n\nApril 11, 2023\n\n\nBrian Cervantes Alvarez, Willa Van Liew\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nGot Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress\n\n\n\n\n\n\n\nR\n\n\nPlotly\n\n\nReports\n\n\nTime Series\n\n\nData Visualization\n\n\n\n\nEnhancing Report Generation Efficiency: Automating Parameterized Rmarkdown to Save Time. Streamline daily reports with automated updates and comparable outputs over time.…\n\n\n\n\n\n\nApril 3, 2023\n\n\nBrian Cervantes Alvarez\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nApple’s Journey In The Stock Market\n\n\n\n\n\n\n\nR\n\n\nTime Series\n\n\nData Visualization\n\n\n\n\nExplore the dynamic world of Time Series Graphs with R’s plotly. Unveil trends in Apple’s stock market and the impact of groundbreaking innovations\n\n\n\n\n\n\nMarch 29, 2023\n\n\nBrian Cervantes Alvarez\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Customer Returns\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nRandom Forest\n\n\n\n\nEvaluating customer returns for predicting item returns using logistic regression and random forest techniques. Achieved robust predictions with moderate predictive power.\n\n\n\n\n\n\nMarch 11, 2023\n\n\nBrian Cervantes Alvarez\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nPredicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nRandom Forest\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nMarch 1, 2023\n\n\nBrian Cervantes Alvarez\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nInvestigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach\n\n\n\n\n\n\n\nR\n\n\nReveal.JS\n\n\nSurvival Analysis\n\n\n\n\nUncover the hidden impact of temperature on pesticide toxicity in a captivating quest that reveals the implications for our changing world.\n\n\n\n\n\n\nFebruary 27, 2023\n\n\nBrian Cervantes Alvarez, Willa Van Liew, Hans Lehndorff\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nDo You Like Stretching? I Would Reconsider!\n\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nPlotly\n\n\n\n\nStretching enhances flexibility but lacks muscle-building benefits. Weight training and resistance exercises stimulate muscle growth. Supplement, not substitute, for…\n\n\n\n\n\n\nFebruary 24, 2023\n\n\nBrian Cervantes Alvarez\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nRating Pinot Wines: Is More Expensive Better?\n\n\n\n\n\n\n\nR\n\n\nData Tables\n\n\n\n\nRevolutionizing Data Tables: Visual Appeal & Comprehension. Explore techniques for aesthetically pleasing & informative tables. Enhance data presentation with design…\n\n\n\n\n\n\nFebruary 20, 2023\n\n\nBrian Cervantes Alvarez\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nHealthcare Spending Is Only Getting Worse\n\n\n\n\n\n\n\nR\n\n\nLinear Regression\n\n\nStatistics\n\n\n\n\nCracking the Code: Unmasking the Skyrocketing US Healthcare Costs, Unveiling the Factors and Sector Impact since 1980!\n\n\n\n\n\n\nDecember 12, 2022\n\n\nBrian Cervantes Alvarez\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nPokédex Database\n\n\n\n\n\n\n\nR\n\n\nPostgreSQL\n\n\nData Visualization\n\n\nData Engineering\n\n\n\n\nUnveil the power of PostgreSQL! Explore the intricate ETL process, advanced tools, and meticulous schema design that created a functional database. Join the journey of…\n\n\n\n\n\n\nDecember 5, 2022\n\n\nBrian Cervantes Alvarez\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nResources for Prospective College Students\n\n\n\n\n\n\n\nR\n\n\nShiny\n\n\nData Visualization\n\n\nDashboard\n\n\n\n\nUnleash your future with our interactive tool! Estimate salaries, tuition, and student debt for majors. Plan your journey and make informed career decisions. Essential for…\n\n\n\n\n\n\nDecember 2, 2022\n\n\nBrian Cervantes Alvarez, Corey Cassell\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs\n\n\n\n\n\n\n\nPython\n\n\nStatistics\n\n\nJupyter Notebook\n\n\nData Visualization\n\n\n\n\nIn this captivating investigation, we dive deep into the impact of smoking habits on medical insurance expenses, carefully examining the nuanced differences between genders…\n\n\n\n\n\n\nJuly 14, 2022\n\n\nBrian Cervantes Alvarez\n\n\n26 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Presentations",
    "section": "",
    "text": "This presentation not only serves as an introduction to my persona but also offers a glimpse into a selection of my projects that I have found immensely gratifying. Utilizing an array of programming languages ranging from R to SQL, I am thrilled to embark on my journey as a Data Scientist and become an integral part of the ever-evolving field of Data Science."
  },
  {
    "objectID": "resume.html#data-scientist-skillset",
    "href": "resume.html#data-scientist-skillset",
    "title": "Curriculum Vitae",
    "section": "Data Scientist Skillset",
    "text": "Data Scientist Skillset\nAs a Data Scientist, I possess a diverse skillset that enables me to excel in my role. Here are some key skills I have acquired:\n\nData Storytelling and Communication:\n\nI am adept at effectively conveying complex insights and findings to both technical and non-technical stakeholders through compelling narratives, data visualization, and clear presentations.\n\nBayesian Statistics:\n\nI have a strong grasp of Bayesian statistics, allowing me to make robust inferences, predictions, and decisions by integrating prior knowledge with observed data, even in uncertain environments or with limited data.\n\nData Wrangling and Cleaning:\n\nI possess excellent skills in transforming raw data into a usable format for analysis. I can handle missing data, outliers, and ensure data integrity, thereby maintaining the quality and accuracy of the data throughout the analysis process.\n\nA/B Testing:\n\nI am skilled in designing and analyzing A/B tests, enabling me to compare the performance of different versions of a product, webpage, or strategy. I can derive valid conclusions and make data-driven decisions based on experimental results.\n\nTime Series Analysis:\n\nI have expertise in analyzing time-dependent data, such as stock prices or sales figures. By identifying patterns, trends, and seasonality within the data, I can provide accurate predictions and forecasts crucial for decision-making in industries reliant on temporal patterns.\n\nSurvival Analysis:\n\nI possess a deep understanding of survival analysis techniques, allowing me to analyze time-to-event data effectively. I can estimate survival probabilities, account for competing risks, and draw insightful conclusions about event occurrences.\n\n\nThese skills collectively empower me to excel as a Data Scientist, enabling me to derive meaningful insights, make informed decisions, and effectively communicate complex information to drive actionable results."
  },
  {
    "objectID": "resume.html#machine-learning-scientist-skillset",
    "href": "resume.html#machine-learning-scientist-skillset",
    "title": "Curriculum Vitae",
    "section": "Machine Learning Scientist Skillset",
    "text": "Machine Learning Scientist Skillset\nAs a Machine Learning Scientist, I possess a range of skills that allow me to thrive in my role. Here are some additional skills I have acquired:\n\nNatural Language Processing (NLP):\n\nI am familiar with NLP techniques, enabling me to work with text data and perform sentiment analysis. I leverage text classification, named entity recognition, and sentiment analysis to extract valuable information from unstructured text data.\n\nDeep Learning:\n\nI have a strong understanding of deep learning techniques and frameworks, such as TensorFlow and PyTorch. I can work with neural networks, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n\nData Engineering:\n\nI am familiar with data engineering concepts and tools such as PostgreSQL. I can preprocess, transform, and engineer features from large datasets, ensuring optimal data quality and efficiency for machine learning tasks.\n\nPerformance Optimization:\n\nI have expertise in optimizing machine learning models for performance, speed, and efficiency. I can fine-tune hyperparameters, handle issues like overfitting or underfitting, and implement techniques such as regularization and ensemble learning. Proficiency in libraries like scikit-learn and XGBoost enables me to achieve optimal model performance.\n\nModel Evaluation and Validation:\n\nI have expertise in evaluating and validating machine learning models. I can employ techniques like cross-validation, ROC curves, and precision-recall analysis to assess model performance, select the best models, and ensure their generalizability.\n\n\nThese skills collectively empower me to excel as a Machine Learning Scientist, allowing me to develop and deploy cutting-edge machine learning systems and solve complex problems across various domains."
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#abstract",
    "href": "posts/US_HealthIns_Costs/index.html#abstract",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Abstract:",
    "text": "Abstract:\nThis study confirms the significant impact of smoking on the escalation of health insurance premiums. Male and female smokers with a body mass index (BMI) of 30 or higher face additional charges, compounding their financial burden. Male smokers experience a 52% increase, while female smokers face a 49% rise in insurance charges, in addition to the base premium for smokers.\nThe severity of the situation is evident, as male smokers pay 408.57% more than non-smokers, and female smokers pay 350.12% more. The data unequivocally supports the notion that unhealthy lifestyle choices, such as smoking and high BMI, result in higher health insurance premiums. It is important to note that premiums also increase gradually over time with age.\nWhile this project provides valuable insights, further exploration opportunities exist. Applying machine learning techniques to assess the representativeness of the sample could enhance the accuracy of conclusions and foster advancements in the field of health insurance."
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#data-setup-and-import",
    "href": "posts/US_HealthIns_Costs/index.html#data-setup-and-import",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Data Setup and Import",
    "text": "Data Setup and Import\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n#read in the csv file\ndf = pd.read_csv('insurance.csv')\nprint(df.head())\n\n   age     sex     bmi  children smoker     region      charges\n0   19  female  27.900         0    yes  southwest  16884.92400\n1   18    male  33.770         1     no  southeast   1725.55230\n2   28    male  33.000         3     no  southeast   4449.46200\n3   33    male  22.705         0     no  northwest  21984.47061\n4   32    male  28.880         0     no  northwest   3866.85520"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#analysis-of-factors-affecting-insurance-costs",
    "href": "posts/US_HealthIns_Costs/index.html#analysis-of-factors-affecting-insurance-costs",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Analysis of Factors Affecting Insurance Costs:",
    "text": "Analysis of Factors Affecting Insurance Costs:\nThe objective of this analysis is to identify the factors influencing insurance costs and determine their impact on individuals’ financial burden. Specifically, we investigate the relationship between smoking habits and increased medical insurance expenses.\nInitially, we examine the prevalence of smoking within the dataset. Subsequently, we conduct a comprehensive assessment of the distribution of charges among smokers, employing visual aids like box plots to illustrate quartiles and identify any outliers. Statistical measures, including minimum, maximum, mean, and median, are then employed to provide a more comprehensive understanding of the data.\nFurthermore, we conduct a gender-based analysis to ascertain if there are notable differences in expenses between male and female smokers. This allows us to evaluate any significant variations in insurance costs based on gender.\n\nHistogram Plot of Smokers vs Nonsmokers\n\nSmoker Count: 274\nNonsmoker Count: 1064\n\nTo visually illustrate the distribution of smokers and nonsmokers within the dataset, a histogram plot was generated. The plot showcases the count of individuals categorized as smokers or nonsmokers. The number of smokers is 274, while the count of nonsmokers is 1064.\n\n#while a histogram is not necessary, it can really show in the visual count of smokers vs nonsmokers\nplt.figure(figsize = (8, 6))\nsns.set(font_scale = 1.5)\nsns.histplot(df.smoker)\nplt.title('Smoker vs Nonsmoker Count')\n\nplt.tight_layout(pad = 2)\nplt.show()\nplt.clf()\n\n#num_smokers = df.smoker.value_counts()['yes']\n#num_nonsmokers = df.smoker.value_counts()['no']\n#print(\"Exact number of smokers: {num_smokers}\".format(num_smokers = num_smokers))\n#print(\"Exact number of nonsmokers: {num_nonsmokers}\".format(num_nonsmokers = num_nonsmokers))\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nBoxplot and Histogram Distribution of Insurance Costs for Smokers\nUpon analyzing the insurance costs specifically for smokers, a boxplot and histogram were constructed. The interquartile range indicates a substantial spread of $20,125.33 between the first and third quartiles.\nNotably, the distribution of insurance costs for smokers exhibits a bimodal pattern, suggesting the presence of a contributing factor that significantly impacts insurance expenses. Further investigation is required to identify the specific variable responsible for the observed increase in insurance costs among smokers. Potential factors to consider include region, BMI, sex, number of children, and age.\n\ndf_smokers = df[df.smoker == 'yes']\n#print(df_smokers.head())\nsns.set(font_scale = 0.8)\nIQR = stats.iqr(df_smokers.charges, interpolation = 'midpoint')\n#print(round(IQR,2))\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,6))\n#boxplot\nsns.boxplot(y = df_smokers.charges, ax = ax1)\nax1.set_ylabel('Insurance cost (USD)')\nax1.set_title('Insurance Cost Boxplot for Smokers')\n\n#histogram\nsns.histplot(df_smokers.charges, bins = 40, ax = ax2)\nax2.set_xlabel('Insurance Cost (USD)')\nax2.set_title('Insurance Cost Distribution for Smokers')\n\nplt.tight_layout(pad = 3)\n\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nInsurance Cost Statistics for Smokers\nSummary Stats:\n\nMean: $32,050.23\nMedian: $34,456.35\nMax: $63,770.43\nMin: $12,829.46\nStandard Deviation: $11,520.47\nVariance: 132,721,153.14\n\n\n#find mean of insurance cost for smokers\nsmoker_mean = round(np.mean(df_smokers.charges), 2)\n#print('The average insurance cost for being a smoker: ${mean}'.format(mean = smoker_mean))\n\n#find median of insurance cost for smokers\nsmoker_median = round(np.median(df_smokers.charges), 2)\n#print('The median insurance cost for being a smoker: ${median}'.format(median = smoker_median))\n\n#find standard deviation...\nsmoker_std = round(np.std(df_smokers.charges),2)\n#print('The standard deviation is: ${std}'.format(std = smoker_std))\n\n#let's see the variance\nsmoker_var = round(np.var(df_smokers.charges),2)\n#print('The variance is: {var}'.format(var = smoker_var))\n\n#maximum cost\nsmoker_max = round(np.max(df_smokers.charges), 2)\n#print('The maximum insurance cost for being a smoker is: ${max}'.format(max = smoker_max))\n\n#minimum cost\nsmoker_min = round(np.min(df_smokers.charges), 2)\n#print('The minimum insurance cost for being a smoker is: ${min}'.format(min = smoker_min))\n\nThe summary statistics provide valuable insights into the distribution of insurance costs for smokers. The mean insurance cost for smokers is calculated to be $32,050.23, representing the average expense in this category. The median insurance cost, at $34,456.35, serves as the central value around which the data is evenly distributed.\nAdditionally, the maximum observed insurance cost for smokers is $63,770.43, indicating the highest expense recorded within this group. Conversely, the minimum cost stands at $12,829.46, representing the lowest recorded expense.\nThe standard deviation of $11,520.47 measures the degree of variability or dispersion among the insurance costs for smokers. Furthermore, the variance of 132,721,153.14 quantifies the average squared deviation from the mean.\nThese statistical measures provide a comprehensive overview of the distribution of insurance costs for smokers, enabling a better understanding of the financial implications associated with smoking.\n\n\nSeparation of Male and Female Smokers:\nTo examine potential differences in insurance costs, a segmentation analysis was conducted, specifically focusing on male and female smokers. This approach aims to discern any notable variations in expenses between the two genders.\nThe analysis reveals significant insights, as demonstrated in the accompanying graphs. It becomes evident that the primary contributor to the observed bimodal distribution is the BMI (Body Mass Index). Notably, when the BMI reaches or exceeds 30, which falls within the obese category, a substantial increase in insurance costs occurs. This finding highlights the compounding effect of both obesity and smoking on medical insurance expenses.\nFurthermore, it is worth noting that insurance costs gradually increase as age advances. This correlation aligns with the expected progression of health issues that typically arise with aging. Consequently, there is a gradual rise in insurance costs as individuals age, reflecting the natural occurrence of age-related health conditions.\nThis segmentation analysis underscores the interplay between gender, BMI, age, and insurance costs, providing valuable insights for the medical industry in understanding the multifaceted factors that influence insurance expenses.\n\n\nAnalysis of Male Smokers:\nGraph One: Cost Distribution The distribution of insurance costs for male smokers exhibits a bimodal pattern, indicating the presence of two distinct clusters. This suggests the influence of an underlying variable that contributes to the observed cost disparity.\nGraph Two: Average Cost per Child The bar plot showcasing the average insurance costs per child does not reveal any clear trends. However, it is noteworthy that individuals with four children tend to have a slightly lower average insurance cost.\nGraph Three: Cost vs BMI with Region The scatterplot depicting insurance cost against BMI, considering different regions, demonstrates a visible linear relationship. The plot shows two clusters of individuals, but both clusters follow the same linear pattern. Notably, when the BMI reaches or exceeds 30 (indicating obesity), there is a significant and distinct increase in insurance costs for male smokers. It is important to note that BMI is not always an ideal measurement; however, in this analysis, it serves as an indicator of obesity. No notable trends were observed concerning the region.\nGraph Four: Cost vs Age with Children The scatterplot illustrating insurance cost against age, accounting for the presence of children, exhibits a linear trend split into two distinct clusters. This finding suggests that the observed difference is likely attributed to variations in BMI among individuals. As expected, insurance costs tend to increase gradually over time.\nGraph Five: Average Cost per Region The bar plot representing the average insurance costs per region does not yield any significant insights.\nGraph Six: Boxplot of Male Smoker Insurance Cost The boxplot provides valuable visual information regarding the spread of insurance costs among male smokers. It allows for a better understanding of the distribution and variability within this group.\nThese analyses shed light on the various factors impacting insurance costs for male smokers, including BMI, age, and the presence of children. By comprehensively examining these relationships, the medical industry can gain valuable insights to inform decision-making and enhance understanding of cost dynamics.\n\n#seperate all male smokers from the main data set\ndf_male_smoker = df_smokers[df_smokers.sex == 'male']\ndf_nonsmokers = df[df.smoker == 'no']\n#print(df_nonsmokers.head())\n#print(df_male_smoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male smoker\nsns.histplot(df_male_smoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Male Smoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of male smokers per child\ndf_children = df_male_smoker.groupby('children').mean().reset_index()\n#print(df_children.head())\n\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Male Smoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of male smokers by BMI\nsns.scatterplot(x = df_male_smoker.bmi, y = df_male_smoker.charges,\n                hue = df_male_smoker.region,\n                size = df_male_smoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Male Smoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of male smokers by age\nsns.scatterplot(x = df_male_smoker.age, y = df_male_smoker.charges,\n                hue = df_male_smoker.children,\n                size = df_male_smoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Male Smoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of male smokers per region\ndf_region = df_male_smoker.groupby('region').mean().reset_index()\n#print(df_region.head())\n\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Male Smoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of male smoker boxplot\nsns.boxplot(x = df_male_smoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Smoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\n\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nAnalysis of Male Smokers: Highlighting BMI\nAs previously mentioned, the presence of two clusters in the cost distribution of male smokers can be attributed to variations in BMI. Notably, male smokers with a BMI greater than or equal to 30 experience a significant increase in expenses, approximately 47%, compared to their counterparts with a lower BMI. This finding underscores the impact of obesity on insurance costs for male smokers in the medical industry.\n\n#highlight BMI graph\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = df_male_smoker.bmi, y = df_male_smoker.charges,\n                hue = df_male_smoker.region,\n                size = df_male_smoker.region,\n                sizes = (7.5, 25))\nplt.axvline(x = 30, color = 'gray', label = 'Obese BMI')\nplt.title('Male Smoker Insurance Cost vs BMI')\nplt.xlabel('BMI')\nplt.ylabel('Insurance Costs (USD)')\n\nText(0, 0.5, 'Insurance Costs (USD)')\n\n\n\n\n\n\n#analyze BMI < 30\n\nmale_df_bmi = df_male_smoker[df_male_smoker.bmi < 30]\nmale_avg_cost = round(np.mean(male_df_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for male smokers with a BMI of less than 30 equates to: ${charge}\".format(charge = male_avg_cost))\n\n#analyze BMI >= 30\n\nmale_df2_bmi = df_male_smoker[df_male_smoker.bmi >= 30]\nmale_avg_cost_2 = round(np.mean(male_df2_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for male smokers with a BMI of greater than or equal to 30 equates to: ${charge}\".format(charge = male_avg_cost_2))\n\n#Difference of Insurance Cost\nmale_diff_cost = male_avg_cost_2 - male_avg_cost\n#print(\"The difference in insurance cost from being at or over 30 BMI compared to being under 30 BMI for males is: ${diff}\".format(diff = male_diff_cost))\n\nmale_diff_cost_percent = 100*round(male_diff_cost / male_avg_cost_2, 2)\n#print('A male smoker will suffer an increase of {percent}% in their insurance cost by being obese (BMI >= 30)'.format(percent = male_diff_cost_percent))\n\n\n\nThe analysis of female smokers reveals the following insights:\nCost Distribution: Similar to the previous analysis, there is a bimodal distribution in the insurance cost for female smokers, indicating the presence of underlying factors contributing to the cost variations.\nAverage Cost per Child: The barplot does not show a clear trend between the number of children and the average insurance cost for female smokers. However, it is worth noting that having five children seems to be associated with a slightly lower average insurance cost.\nScatterplot of Cost vs BMI with Region: The scatterplot demonstrates a visible linear relationship between BMI and insurance cost for female smokers. There are two clusters of individuals, but they both follow the linear relationship. Notably, when the BMI is greater than or equal to 30 (indicating obesity), there is a significant increase in insurance cost for female smokers. It is important to consider that BMI may not be a perfect measurement of obesity (as discussed in the results). No specific trends are observed with respect to the region.\nScatterplot of Cost vs Age with Children: The scatterplot shows a linear trend in insurance cost based on age, but it is split into two linear clusters. This finding aligns with the earlier observation that differences in BMI among individuals may contribute to the clusters. As expected, insurance cost gradually increases over time.\nAverage Cost per Region: The barplot of average cost per region does not provide any valuable insights for female smokers.\nBoxplot of Female Smoker Insurance Cost: The boxplot displays the spread of insurance cost among female smokers, highlighting the range of variation in costs.\nOverall, the analysis of female smokers provides insights into the relationship between factors such as BMI, age, and insurance cost.\n\n#seperate all female smokers from the main data set\ndf_female_smoker = df_smokers[df_smokers.sex == 'female']\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male smoker\nsns.histplot(df_female_smoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Female Smoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of female smokers per child\ndf_children = df_female_smoker.groupby('children').mean().reset_index()\n#print(df_children.head())\n\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Female Smoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of female smokers by BMI\nsns.scatterplot(x = df_female_smoker.bmi, y = df_female_smoker.charges,\n                hue = df_female_smoker.region,\n                size = df_female_smoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Female Smoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of female smokers by age\nsns.scatterplot(x = df_female_smoker.age, y = df_female_smoker.charges,\n                hue = df_female_smoker.children,\n                size = df_female_smoker.children,\n                sizes = (12.5, 25), ax = ax[1,0])\nax[1,0].set_title('Female Smoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of female smokers per region\ndf_region = df_female_smoker.groupby('region').mean().reset_index()\n#print(df_region.head())\n\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average. Female Smoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of female smoker age vs bmi\nsns.boxplot(x = df_female_smoker.charges, ax = ax[1,2])\nax[1,2].set_title('Female Smoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\nThe analysis of female smokers reveals findings that align with the observations made for male smokers. Specifically, the relationship between insurance cost and BMI for female smokers follows a similar pattern as observed in male smokers. Therefore, the conclusions drawn from the previous analysis regarding the impact of BMI on insurance cost can be applied to female smokers as well.\n\n#analyze BMI < 30\n\nfemale_df_bmi = df_female_smoker[df_female_smoker.bmi < 30]\nfemale_avg_cost = round(np.mean(female_df_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for female smokers with a BMI of less than 30 equates to: ${charge}\".format(charge = female_avg_cost))\n\n#analyze BMI >= 30\n\nfemale_df2_bmi = df_female_smoker[df_female_smoker.bmi >= 30]\nfemale_avg_cost_2 = round(np.mean(female_df2_bmi.charges, axis = 0),2)\n#print(\"The average insurance cost for female smokers with a BMI of greater than or equal to 30 equates to: ${charge}\".format(charge = female_avg_cost_2))\n\n#Difference of Insurance Cost\nfemale_diff_cost = female_avg_cost_2 - female_avg_cost\n#print(\"The difference in cost from being at or over 30 BMI compared to being under 30 BMI for females is: ${diff}\".format(diff = female_diff_cost))\n\n\n\nComparing Average Insurance Costs: Male vs. Female Smokers\nWhen comparing the average insurance costs of male and female smokers, some notable observations emerge. Among smokers with a BMI below 30, male smokers pay approximately 1.34% more than their female counterparts. However, for smokers with a BMI of 30 or higher, male smokers pay approximately 1.42% less than their female counterparts. These findings indicate a gender-based variation in insurance costs, influenced by both BMI and smoking status.\n\n#Plot average insurance cost for male and female \ndf_bmi_less_than_30 = df_smokers[df_smokers.bmi < 30]\ndf_one = df_bmi_less_than_30.groupby('sex').mean().reset_index()\n\n#Get value\nmale_bmi_less_than_30 = df_one[df_one.sex == 'male'].charges.sum()\nfemale_bmi_less_than_30 = df_one[df_one.sex == 'female'].charges.sum()\n#print value\n\nmale_female_sum = male_bmi_less_than_30 + female_bmi_less_than_30\npor_male_less_30 = male_bmi_less_than_30 / male_female_sum\npor_female_less_30 = female_bmi_less_than_30 / male_female_sum\n\n\ndiff_por = round((por_male_less_30 - por_female_less_30), 4) * 100\n#print(\"Male smokers are expected to pay approximately {diff_por}% more than their female counterparts when their BMI < 30 \".format(diff_por = diff_por))\n\ndf_bmi_greater_than_or_equal_30 = df_smokers[df_smokers.bmi >= 30]\ndf_two = df_bmi_greater_than_or_equal_30.groupby('sex').mean().reset_index()\n\nmale_bmi_greq_30 = df_two[df_two.sex == 'male'].charges.sum()\nfemale_bmi_greq_30 = df_two[df_two.sex == 'female'].charges.sum()\n\n\nmale_female_sum = male_bmi_greq_30 + female_bmi_greq_30\npor_male_greq_30 = male_bmi_greq_30 / male_female_sum\npor_female_greq_30 = female_bmi_greq_30 / male_female_sum\n\ndiff_por_greq = round((por_male_greq_30 - por_female_greq_30), 4) * 100\ndiff_por_greq = abs(round(diff_por_greq, 4))\n#print(\"Male smokers are expected to pay approximately {diff_por_greq}% less than their female counterparts when their BMI >= 30\".format(diff_por_greq = diff_por_greq))\n\n#print(df_one)\n#print(df_two)\n#df3 = pd.concat([df_one, df_two])\n#print(df3)\n\nfig, ax = plt.subplots(1,2,figsize=(8,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.8)\n\n#BMI is less than 30\nsns.set_palette('bright')\nsns.barplot(x = df_one.sex, y = df_one.charges, ax = ax[0])\nax[0].set_xlabel('Sex')\nax[0].set_ylabel('Insurance Cost (USD)')\nax[0].set_title('Average Smoker Insurance Cost with BMI < 30')\n\n\n#BMI is greater than or equal to 30\nsns.set_palette('dark')\nsns.barplot(x = df_two.sex, y = df_two.charges, ax = ax[1])\nax[1].set_xlabel('Sex')\nax[1].set_ylabel('Insurance Cost (USD)')\nax[1].set_title('Average Smoker Insurance Cost with BMI >= 30')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf\n\n\n\n\n<function matplotlib.pyplot.clf()>"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#approaching-the-analysis-of-nonsmokers",
    "href": "posts/US_HealthIns_Costs/index.html#approaching-the-analysis-of-nonsmokers",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Approaching the Analysis of Nonsmokers",
    "text": "Approaching the Analysis of Nonsmokers\nConsidered Factors: To understand the underlying causes of outliers or extreme values, several key factors warrant examination, including region, BMI, sex, children, and age. However, it is important to acknowledge that identifying the precise factors contributing to these outliers may present challenges and require further investigation.\n\nAnalysis of Insurance Costs for Nonsmokers: Boxplot and Histogram Distribution\nThe distribution of insurance costs for nonsmokers exhibits a right-skewed pattern, indicating a higher concentration of lower-cost cases. The interquartile range suggests a relatively narrow spread between the first and third quartiles.\n\nIQR = stats.iqr(df_nonsmokers.charges, interpolation = 'midpoint')\nIQR = round(IQR, 2)\nprint(IQR)\n\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,6))\n#boxplot\nsns.boxplot(y = df_nonsmokers.charges, ax = ax1)\nax1.set_ylabel('Insurance cost (USD)')\nax1.set_title('Insurance Cost Boxplot for Nonsmokers')\n\n#histogram\nsns.histplot(df_nonsmokers.charges, bins = 40, ax = ax2)\nax2.set_xlabel('Insurance Cost (USD)')\nax2.set_title('Insurance Cost Distribution for Nonsmokers')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n#right skewed distribution...explain potential causes:  Male and Female? BMI?\n\n7378.07\n\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nAnalysis of Insurance Costs for Nonsmokers\nThe interquartile range of insurance costs for nonsmokers is $7,378.07, indicating a relatively narrow spread of data between the first and third quartiles.\n\n#find mean of insurance cost for nonsmokers\nnonsmoker_mean = round(np.mean(df_nonsmokers.charges), 2)\n#print('The nonsmoker average insurance cost for being a smoker: ${mean}'.format(mean = nonsmoker_mean))\n\n#find median of insurance cost for nonsmokers\nnonsmoker_median = round(np.median(df_nonsmokers.charges), 2)\n#print('The nonsmoker median insurance cost for being a smoker: ${median}'.format(median = nonsmoker_median))\n\n#find standard deviation...\nnonsmoker_std = round(np.std(df_nonsmokers.charges),2)\n#print('The nonsmoker standard deviation is: ${std}'.format(std = nonsmoker_std))\n\n#let's see the variance\nnonsmoker_var = round(np.var(df_nonsmokers.charges),2)\n#print('The nonsmoker variance is: {var}'.format(var = nonsmoker_var))\n\n#maximum cost\nnonsmoker_max = round(np.max(df_nonsmokers.charges), 2)\n#print('The nonsmoker maximum insurance cost is: ${max}'.format(max = nonsmoker_max))\n\n#minimum cost\nnonsmoker_min = round(np.min(df_nonsmokers.charges), 2)\n#print('The nonsmoker minimum insurance cost is: ${min}'.format(min = nonsmoker_min))\n\n#For explaination a high standard deviation tells us that here are other reasons the cost is so high since it varies signifcantly\n\n\n\nAnalyzing Male Nonsmokers: Key Findings\nGraph One: Cost Distribution The cost distribution for male nonsmokers exhibits a right-skewed pattern. It is important to consider the median as the most appropriate measure of central tendency, given the presence of skewness in the distribution.\nGraph Two: Average Cost per Child There is a notable trend indicating that having more children is associated with higher average insurance costs for male nonsmokers. However, an interesting exception is observed for individuals with five or more children, as their insurance costs show a significant decrease. The lowest average cost is observed for individuals with no children.\nGraph Three: Cost vs BMI with Region No correlation is found between the insurance cost and BMI of male nonsmokers. This finding highlights the absence of a relationship between these two variables. Similarly, no discernible trends are observed with respect to different regions.\nGraph Four: Cost vs Age with Children A strong linear relationship is observed between insurance cost and age for male nonsmokers. Additionally, the number of children shows a secondary trend, where fewer children correspond to lower insurance costs, while more children align with higher insurance costs. It is worth noting that some outliers deviate from the overall trend line.\nGraph Five: Average Cost per Region No significant insights can be drawn from the barplot of average cost per region for male nonsmokers. The data does not reveal any distinct variations among different regions in terms of insurance costs.\nGraph Six: Boxplot of Male Nonsmoker Insurance Cost The boxplot provides valuable information about the spread of insurance costs for male nonsmokers. It allows for visualizing the range, quartiles, and potential outliers within the data.\nOverall, these analyses shed light on various factors influencing insurance costs for male nonsmokers, such as the number of children and age. However, the relationship between BMI and insurance cost appears to be inconclusive, while regional differences do not show significant variations.\n\n#seperate all male nonsmokers from the main data set\ndf_male_nonsmoker = df_nonsmokers[df_nonsmokers.sex == 'male']\n\n#print(male_nonsmoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram male nonsmoker\nsns.histplot(df_male_nonsmoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Male Nonsmoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of male nonsmokers per child\ndf_children = df_male_nonsmoker.groupby('children').mean().reset_index()\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Male Nonsmoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of male nonsmokers by BMI\nsns.scatterplot(x = df_male_nonsmoker.bmi, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.region,\n                size = df_male_nonsmoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Male Nonsmoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\n\n#scatterplot for insurance cost of male nonsmokers by age\nsns.scatterplot(x = df_male_nonsmoker.age, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.children,\n                size = df_male_nonsmoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Male Nonsmoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of male nonsmokers per region\ndf_region = df_male_nonsmoker.groupby('region').mean().reset_index()\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Male Nonsmoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of male nonsmoker boxplot\nsns.boxplot(x = df_male_nonsmoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Nonsmoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n#highlight scatter plot for linear trend\n\nsns.set(font_scale = 0.8)\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = df_male_nonsmoker.age, y = df_male_nonsmoker.charges,\n                hue = df_male_nonsmoker.children,\n                size = df_male_nonsmoker.children,\n                sizes = (7.5, 20))\nplt.title('Male Nonsmoker Insurance Cost vs Age')\nplt.xlabel('Age')\nplt.ylabel('Insurance Costs (USD)')\nplt.legend(bbox_to_anchor = (1.0, 1))\n\nplt.show()\nplt.clf()\n#highlight this\n#solid evidence that as age increases, your insurance cost increases as well.\n#people with more children see a slightly higher cost overall \n\n\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\nAnalyzing Female Nonsmokers: A Comparison with Male Nonsmokers\nGraph One: Cost Distribution Similar to male nonsmokers, the cost distribution for female nonsmokers exhibits a right-skewed pattern. This suggests that the median is a more appropriate measure of central tendency than the mean, considering the skewness in the distribution.\nGraph Two: Average Cost per Child Similarly to male nonsmokers, there is a discernible trend indicating that having more children is associated with higher average insurance costs for female nonsmokers. However, consistent with the findings among smokers, females with five or more children experience a decrease in average costs. Notably, the lowest average cost is observed among females with no children.\nGraph Three: Cost vs BMI with Region As with male nonsmokers, there is no correlation between the insurance cost and BMI among female nonsmokers. This lack of relationship remains consistent, and no significant trends are observed across different regions.\nGraph Four: Cost vs Age with Children In line with the observations for male nonsmokers, there is a strong linear relationship between insurance cost and age among female nonsmokers. A secondary trend is also evident, indicating that insurance costs tend to be lower for females with fewer children and higher for those with more children. It is worth noting that outliers do not conform to the overall trend line.\nGraph Five: Average Cost per Region There is a slight trend suggesting regional differences in insurance costs among female nonsmokers. On average, the northeast region exhibits higher costs compared to the southwest region.\nGraph Six: Boxplot of Male Smoker Insurance Cost While not directly related to the analysis of female nonsmokers, it is noteworthy to examine the spread of insurance costs among male smokers for comparative purposes\n\n#seperate all female nonsmokers from the main data set\ndf_female_nonsmoker = df_nonsmokers[df_nonsmokers.sex == 'female']\n\n#print(df_female_nonsmoker.head())\n\nfig, ax = plt.subplots(2,3,figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set(font_scale = 0.5)\n\n#histogram Female nonsmoker\nsns.histplot(df_female_nonsmoker.charges, bins = 40, ax = ax[0,0])\nax[0,0].set_title('Female Nonsmoker Insurance Cost Distribution')\nax[0,0].set_xlabel('Insurnance Cost (USD)')\n\n#barplot for insurance cost of Female nonsmokers per child\ndf_children = df_female_nonsmoker.groupby('children').mean().reset_index()\nsns.barplot(x = df_children.children, y = df_children.charges, ax = ax[0,1])\nax[0,1].set_title('Average Female Nonsmoker Insurance Cost Per Child')\nax[0,1].set_xlabel('Number of Children')\nax[0,1].set_ylabel('Insurance Cost (USD)')\n\n#scatterplot for insurance cost of Female nonsmokers by BMI\nsns.scatterplot(x = df_female_nonsmoker.bmi, y = df_female_nonsmoker.charges,\n                hue = df_female_nonsmoker.region,\n                size = df_female_nonsmoker.region,\n                sizes = (7.5, 25), ax = ax[0,2])\nax[0,2].axvline(x = 30, color = 'gray', label = 'Obese BMI')\nax[0,2].set_title('Female Nonsmoker Insurance Cost vs BMI')\nax[0,2].set_xlabel('BMI')\nax[0,2].set_ylabel('Insurance Costs (USD)')\nax[0,2].legend(bbox_to_anchor = (1.0, 1))\n\n#scatterplot for insurance cost of Female nonsmokers by age\nsns.scatterplot(x = df_female_nonsmoker.age, y = df_female_nonsmoker.charges,\n                hue = df_female_nonsmoker.children,\n                size = df_female_nonsmoker.children, sizes = (12.5,25),\n                ax = ax[1,0])\nax[1,0].set_title('Female Nonsmoker Insurance Cost vs Age')\nax[1,0].set_xlabel('Age')\nax[1,0].set_ylabel('Insurance Costs (USD)')\n\n#barplot for insurance cost of Female nonsmokers per region\ndf_region = df_female_nonsmoker.groupby('region').mean().reset_index()\nsns.barplot(x = df_region.region,  y = df_region.charges, ax = ax[1,1])\nax[1,1].set_title('Average Female Nonsmoker Insurance Cost Per Region')\nax[1,1].set_xlabel('Region')\nax[1,1].set_ylabel('Insurance Cost (USD)')\n\n#boxplot of Female nonsmoker boxplot\nsns.boxplot(x = df_male_nonsmoker.charges, ax = ax[1,2])\nax[1,2].set_title('Male Nonsmoker Insurance Cost Boxplot')\nax[1,2].set_xlabel('Insurance Cost (USD)')\n\nplt.tight_layout(pad = 4)\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 672x480 with 0 Axes>"
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#nonsmokers-vs-smokers",
    "href": "posts/US_HealthIns_Costs/index.html#nonsmokers-vs-smokers",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Nonsmokers vs Smokers",
    "text": "Nonsmokers vs Smokers\nBased on the analysis conducted on smokers and nonsmokers, it can be observed that smoking status significantly impacts health insurance costs. Both male and female smokers experience higher insurance costs compared to their nonsmoking counterparts. Furthermore, nonsmokers tend to have a more favorable cost distribution, characterized by lower median costs and less variability. This highlights the importance of smoking cessation interventions and promoting a nonsmoking lifestyle to mitigate the financial burden associated with health insurance for both individuals and the healthcare system.\n\n#Nonsmokers\ndf_avg = df_nonsmokers.groupby('sex').mean().reset_index()\n\n\nnonsmoker_male =  round((df_avg[df_avg.sex == 'male'].charges.sum()),2)\nnonsmoker_female = round((df_avg[df_avg.sex == 'female'].charges.sum()),2)\ndiff = round((nonsmoker_female - nonsmoker_male), 2)\ntotal = nonsmoker_male + nonsmoker_female\nnonsmoker_percent_diff = round((((nonsmoker_female - nonsmoker_male) / total) * 100),2) \n\n\nprint(\"The average insurance cost for a nonsmoker male is ${avg}.\".format(avg = nonsmoker_male))\nprint(\"The average insurance cost for a nonsmoker female is ${avg}.\".format(avg = nonsmoker_female))\nprint(\"The average difference in insurance cost for nonsmoker male and female is ${diff}.\\n\".format(diff = diff))\nprint(\"The nonsmoker males are charged {percent}% less than their female counterpart.\\n\".format(percent = nonsmoker_percent_diff))\n\n#Smokers\ndf_avg2 = df_smokers.groupby('sex').mean().reset_index()\n\n\nsmoker_male =  round((df_avg2[df_avg2.sex == 'male'].charges.sum()),2)\nsmoker_female = round((df_avg2[df_avg2.sex == 'female'].charges.sum()),2)\ndiff2 = round((smoker_male - smoker_female), 2)\ntotal = smoker_male + smoker_female\nsmoker_percent_diff = round((((smoker_male - smoker_female) / total) * 100),2) \n\n\nprint(\"The average insurance cost for a smoker male is ${avg}.\".format(avg = smoker_male))\nprint(\"The average insurance cost for a smoker female is ${avg}.\".format(avg = smoker_female))\nprint(\"The average difference in insurance cost for smoker male and female is ${diff}.\\n\".format(diff = diff2))\nprint(\"The smoker males are charged {percent}% more than their female counterpart.\\n\".format(percent = smoker_percent_diff))\n\n#Comparision\n\nins_cost_diff_male = smoker_male - nonsmoker_male\nins_cost_diff_female = smoker_female - nonsmoker_female\nins_cost_avg_diff = diff2 - diff\n\n\nins_cost_percent_male = round((smoker_male / nonsmoker_male) * 100, 2)\nins_cost_percent_female = round((smoker_female / nonsmoker_female) * 100, 2)\n\n\n\nprint(\"On average, a male nonsmoker pays ${nonsmoker}, while a female smoker pays ${smoker}.\".format(nonsmoker = nonsmoker_male, smoker = smoker_male))\nprint(\"The difference in their insurance charges is ${diff_male}.\".format(diff_male = ins_cost_diff_male))\nprint(\"A male smoker will get charged approxmiately {percent}% more than a nonsmoker.\\n\".format(percent = ins_cost_percent_male))\n\nprint(\"On average, a female nonsmoker pays ${nonsmoker}, while a female smoker pays ${smoker}.\".format(nonsmoker = nonsmoker_female, smoker = smoker_female))\nprint(\"The difference in their insurance charges is ${diff_female}.\".format(diff_female = ins_cost_diff_female))\nprint(\"A female smoker will get charged approxmiately {percent}% more than a nonsmoker.\".format(percent = ins_cost_percent_female))\n\nThe average insurance cost for a nonsmoker male is $8087.2.\nThe average insurance cost for a nonsmoker female is $8762.3.\nThe average difference in insurance cost for nonsmoker male and female is $675.1.\n\nThe nonsmoker males are charged 4.01% less than their female counterpart.\n\nThe average insurance cost for a smoker male is $33042.01.\nThe average insurance cost for a smoker female is $30679.0.\nThe average difference in insurance cost for smoker male and female is $2363.01.\n\nThe smoker males are charged 3.71% more than their female counterpart.\n\nOn average, a male nonsmoker pays $8087.2, while a female smoker pays $33042.01.\nThe difference in their insurance charges is $24954.81.\nA male smoker will get charged approxmiately 408.57% more than a nonsmoker.\n\nOn average, a female nonsmoker pays $8762.3, while a female smoker pays $30679.0.\nThe difference in their insurance charges is $21916.7.\nA female smoker will get charged approxmiately 350.12% more than a nonsmoker."
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#results",
    "href": "posts/US_HealthIns_Costs/index.html#results",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Results",
    "text": "Results\nThe impact of smoking on health insurance premiums is undeniable, as demonstrated by the substantial increases in charges for both male and female smokers. When coupled with a BMI of 30 or higher, smokers face an additional financial burden. Male smokers experience a significant 52% increase in insurance charges, while female smokers face a 49% rise. It is important to note that these increases are in addition to the base premium already charged to smokers.\nThe severity of the situation becomes apparent when comparing the costs borne by smokers and nonsmokers. Male smokers pay a staggering 408.57% more than their nonsmoking counterparts, while female smokers face an astronomical 350.12% increase. This data reinforces the notion that unhealthy lifestyle choices, such as smoking and an unhealthy BMI, result in punitive measures in the form of higher health insurance premiums. It is worth mentioning that insurance premiums gradually increase over time in line with the aging process.\nWhile this project provides valuable insights, there are still numerous avenues for further exploration. One potential direction involves applying machine learning techniques to assess the representativeness of the sample and refine the accuracy of the conclusions. Such analyses have the potential to drive advancements in the field of health insurance."
  },
  {
    "objectID": "posts/US_HealthIns_Costs/index.html#further-study",
    "href": "posts/US_HealthIns_Costs/index.html#further-study",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Further Study",
    "text": "Further Study\nAccording to a reputable source (Medical News Today), BMI, which relies on height and weight measurements, is an inadequate indicator of body fat content. It fails to consider important factors such as muscle mass, bone density, overall body composition, and variations based on race and sex.\nFurthermore, health insurance companies employ a practice known as tobacco rating, as outlined in the Affordable Care Act (ACA). Smokers can be charged a tobacco surcharge of up to 50% (or premiums 1.5 times higher) compared to non-smokers. This allows insurance providers to adjust prices based on tobacco usage and the associated health risks.\nReferences:\nMedical News Today: “BMI (body mass index): What is it and is it useful?”\nHealthMarkets: “Smoking and Health Insurance”"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#purpose",
    "href": "posts/PredictPinotWine_ML/index.html#purpose",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of this project was to develop a predictive model for identifying the province of origin for wines based on descriptions provided by critics. To achieve this goal, a random forest model was built and evaluated for its performance, achieving a kappa score of 0.82. This project aimed to provide a useful tool for wine connoisseurs and industry professionals in identifying the origin of wines based on their sensory characteristics."
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#setup",
    "href": "posts/PredictPinotWine_ML/index.html#setup",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#feature-engineering",
    "href": "posts/PredictPinotWine_ML/index.html#feature-engineering",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nwine = read_rds(\"pinot.rds\") \n\nwine_words <- function(df, j, stem = T){ \n  data(stop_words)\n  words <- df %>%\n    unnest_tokens(word, description) %>%\n    anti_join(stop_words) %>%\n    filter(str_detect(string = word, pattern = \"[a-z+]\")) %>% # get rid weird non alphas \n    filter(str_length(word) >= 3) %>% # get rid of strings shorter than 3 characters \n    filter(!(word %in% c(\"wine\",\"pinot\", \"vineyard\"))) %>%\n    group_by(word) %>%\n    mutate(total=n()) %>%\n    ungroup()\n  \n  if(stem){\n    words <- words %>% \n      mutate(word = wordStem(word))\n  }\n  \n  words <- words %>% \n    count(id, word) %>% \n    group_by(id) %>% \n    mutate(exists = (n>0)) %>% \n    ungroup %>% \n    group_by(word) %>% \n    mutate(total = sum(n)) %>% \n    filter(total > j) %>% \n    pivot_wider(id_cols = id,\n                names_from = word,\n                values_from = exists,\n                values_fill = list(exists=0)) %>% \n    right_join(select(df,id,province)) %>% \n    select(-id) %>% \n    mutate(across(-province, ~replace_na(.x, F)))\n}\n\nwino <- wine_words(wine, j = 190, stem = T)\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(id)`"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#specification",
    "href": "posts/PredictPinotWine_ML/index.html#specification",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Specification",
    "text": "Specification\n\nset.seed(504) \n\nctrl <- trainControl(method = \"cv\", number = 3)\n\n\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\ntest <- wino[-wine_index, ]\n\nfit <- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 100,\n             tuneLength = 15,\n             nodesize = 10,\n             verbose = TRUE,\n             trControl = ctrl,\n             metric = \"Kappa\")"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#model-performance",
    "href": "posts/PredictPinotWine_ML/index.html#model-performance",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Model Performance",
    "text": "Model Performance\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               216          5                 0           0        1\n  California              13        758                14          23       19\n  Casablanca_Valley        0          0                10           0        0\n  Marlborough              0          0                 0          12        0\n  New_York                 0          0                 0           0        0\n  Oregon                   9         28                 2          10        6\n                   Reference\nPrediction          Oregon\n  Burgundy              10\n  California            62\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               475\n\nOverall Statistics\n                                          \n               Accuracy : 0.8793          \n                 95% CI : (0.8627, 0.8945)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8069          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9076            0.9583                 0.384615\nSpecificity                   0.9889            0.8515                 1.000000\nPos Pred Value                0.9310            0.8526                 1.000000\nNeg Pred Value                0.9847            0.9579                 0.990379\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1291            0.4531                 0.005977\nDetection Prevalence          0.1387            0.5314                 0.005977\nBalanced Accuracy             0.9482            0.9049                 0.692308\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.266667         0.00000        0.8684\nSpecificity                    1.000000         1.00000        0.9512\nPos Pred Value                 1.000000             NaN        0.8962\nNeg Pred Value                 0.980132         0.98446        0.9370\nPrevalence                     0.026898         0.01554        0.3270\nDetection Rate                 0.007173         0.00000        0.2839\nDetection Prevalence           0.007173         0.00000        0.3168\nBalanced Accuracy              0.633333         0.50000        0.9098"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#re-fit-and-evaluation",
    "href": "posts/PredictPinotWine_ML/index.html#re-fit-and-evaluation",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Re-fit and evaluation",
    "text": "Re-fit and evaluation\n\nset.seed(1504)\n\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\ntest <- wino[-wine_index, ]\n\n# example spec for knn\nfit_final <- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             tuneGrid = fit$bestTune) \n# The last line means we will fit a model using the best tune parameters your CV found above."
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#final-model-performance",
    "href": "posts/PredictPinotWine_ML/index.html#final-model-performance",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Final Model Performance",
    "text": "Final Model Performance\n\nconfusionMatrix(predict(fit_final, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               219          7                 0           0        0\n  California               7        752                12          13       20\n  Casablanca_Valley        0          0                12           0        0\n  Marlborough              0          1                 0          22        0\n  New_York                 0          0                 0           0        1\n  Oregon                  12         31                 2          10        5\n                   Reference\nPrediction          Oregon\n  Burgundy               7\n  California            55\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               485\n\nOverall Statistics\n                                          \n               Accuracy : 0.8912          \n                 95% CI : (0.8753, 0.9057)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8274          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9202            0.9507                 0.461538\nSpecificity                   0.9902            0.8787                 1.000000\nPos Pred Value                0.9399            0.8754                 1.000000\nNeg Pred Value                0.9868            0.9521                 0.991571\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1309            0.4495                 0.007173\nDetection Prevalence          0.1393            0.5134                 0.007173\nBalanced Accuracy             0.9552            0.9147                 0.730769\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                     0.48889       0.0384615        0.8867\nSpecificity                     0.99939       1.0000000        0.9467\nPos Pred Value                  0.95652       1.0000000        0.8899\nNeg Pred Value                  0.98606       0.9850478        0.9450\nPrevalence                      0.02690       0.0155409        0.3270\nDetection Rate                  0.01315       0.0005977        0.2899\nDetection Prevalence            0.01375       0.0005977        0.3258\nBalanced Accuracy               0.74414       0.5192308        0.9167"
  },
  {
    "objectID": "posts/PredictPinotWine_ML/index.html#conclusion",
    "href": "posts/PredictPinotWine_ML/index.html#conclusion",
    "title": "Predicting the Province of Origin for Pinot Wines Based on Their Individual Descriptions",
    "section": "Conclusion",
    "text": "Conclusion\nA kappa value of 0.82 indicates a very good level of agreement between the predictions of the random forest model and the actual outcomes. Kappa is a statistical measure of inter-rater agreement, which is commonly used to evaluate the performance of classification models.\nIn the context of a random forest model, the kappa value measures how well the model predicts the correct class labels for a given set of data. A kappa value of 0.82 indicates that the model’s predictions are in very good agreement with the true class labels, with a high degree of precision and accuracy.\nOverall, a kappa value of 0.82 suggests that the random forest model is performing very well, and can be considered a reliable predictor of the target variable in the dataset."
  },
  {
    "objectID": "posts/FancyTables/index.html#abstract",
    "href": "posts/FancyTables/index.html#abstract",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Abstract",
    "text": "Abstract\nThis project aims to explore alternative techniques for designing visually appealing and comprehensible data tables. Traditional Excel spreadsheets often lack readability and visual impact. By incorporating design principles such as color theory, typography, and layout, we aim to create visually striking data tables that effectively convey information. Additionally, we will evaluate innovative software tools and platforms that offer user-friendly options for creating functional and aesthetically pleasing data tables. Enhancing data presentation is crucial for improving interpretation and understanding."
  },
  {
    "objectID": "posts/FancyTables/index.html#introduction",
    "href": "posts/FancyTables/index.html#introduction",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Introduction",
    "text": "Introduction\nExcel spreadsheets are widely used for organizing and presenting data. However, their conventional format can be tedious and challenging to read, hindering data comprehension. This project seeks to address this limitation by exploring various techniques to design visually appealing and comprehensible data tables.\nThe primary focus is to create data tables that are not only aesthetically pleasing but also convey information effectively. By employing design principles such as color theory, typography, and layout, we aim to enhance the visual impact and readability of data tables. This will involve experimenting with different combinations of colors, fonts, and arrangement patterns to find the most optimal design choices.\nIt is crucial to recognize that the presentation of data plays a significant role in its interpretation and understanding. The traditional Excel format often lacks visual cues to highlight key data points or insights. Therefore, this project seeks to explore new and innovative methods of presenting data tables that not only serve their functional purpose but also captivate the audience with their visual appeal."
  },
  {
    "objectID": "posts/FancyTables/index.html#advanced-data-tables",
    "href": "posts/FancyTables/index.html#advanced-data-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Advanced Data Tables",
    "text": "Advanced Data Tables\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(DT)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(countrycode)\nlibrary(ggflags)\nlibrary(downloadthis)\n\nds <- read_rds(\"pinot.rds\")\n\n# head(ds)\n\n\nds_starter <- ds %>%\n    mutate(\n        province = as.factor(province),\n        price = price,\n        thetaPointMean = mean(points),\n        thetaPriceMean = mean(price)\n    )\n\nds_starter %>%\n    arrange(province, year) %>%\n    select(\n        Province = province,\n        Year = year,\n        Price = price,\n        Points = points,\n        Description = description\n    ) %>%\n    datatable(.,\n        filter = \"bottom\",\n        extensions = \"Buttons\",\n        options = list(\n            dom = \"Bfrtip\",\n            buttons = c(\"copy\", \"csv\", \"excel\"),\n            initComplete = JS(\n                \"function(settings, json) {\",\n                \"$(this.api().table().header()).css({'background-color': '#131F4F', 'color': '#fff'});\",\n                \"}\"\n            )\n        )\n    )\n\n\n\n\n\n\n\nds_summary <- ds_starter %>%\n    group_by(province) %>%\n    arrange(year) %>%\n    summarise(\n        pointsMean = mean(points, na.rm = TRUE),\n        pointsSD = sd(points),\n        priceMean = mean(price, na.rm = TRUE),\n        priceSD = sd(price),\n        points = list(points),\n        price = list(price),\n        .groups = \"drop\"\n    )\n\n\nexcel_file_attachment <- ds_summary %>%\n    download_this(\n        output_name = \"Pinot_Noir_Summary\",\n        output_extension = \".xlsx\", # Excel file type\n        button_label = \"Download Excel\",\n        button_type = \"primary\", # change button type\n    )"
  },
  {
    "objectID": "posts/FancyTables/index.html#adding-trend-lines-to-summary-tables",
    "href": "posts/FancyTables/index.html#adding-trend-lines-to-summary-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Adding Trend Lines To Summary Tables",
    "text": "Adding Trend Lines To Summary Tables\n\nfancyTbl <- ds_summary %>%\n    gt() %>%\n    # format the numeric output to 3 digit rounding\n    fmt_number(\n        columns = c(pointsMean, pointsSD, priceMean, priceSD),\n        decimals = 3\n    ) %>%\n    # create nice labels for a few ugly variable names\n    cols_label(\n        province = \"Province\",\n        pointsMean = \"Avg. Points\",\n        pointsSD = \"Std. Dev. Points\",\n        priceMean = \"Avg. Price\",\n        priceSD = \"Std. Dev. Price\",\n        points = \"Points Trend\",\n        price = \"Price Trend\",\n    ) %>%\n    # Plot the sparklines from the list column\n    gt_plt_sparkline(points,\n        type = \"ref_median\",\n        same_limit = TRUE\n    ) %>%\n    gt_plt_sparkline(price,\n        type = \"ref_median\",\n        same_limit = TRUE\n    ) %>%\n    # use the guardian's table theme\n    gt_theme_guardian() %>%\n    # give hulk coloring to the Mean Human Rights Score\n    gt_hulk_col_numeric(pointsMean) %>%\n    gt_hulk_col_numeric(priceMean) %>%\n    # create a header and subheader\n    tab_header(title = \"Province Pinot Wine Summary\", subtitle = \"Source: Dr. Hendrick\") %>%\n    # attach excel file\n    tab_source_note(excel_file_attachment)\n# save the original as an image\n# gtsave(fancyTbl, \"table.png\")\n# show the table themed in accordance with the page\nfancyTbl\n\n\n\n\n\n  \n    \n      Province Pinot Wine Summary\n    \n    \n      Source: Dr. Hendrick\n    \n    \n      Province\n      Avg. Points\n      Std. Dev. Points\n      Avg. Price\n      Std. Dev. Price\n      Points Trend\n      Price Trend\n    \n  \n  \n    Burgundy\n90.438\n2.989\n98.035\n132.856\n          89.0\n          83.0\n    California\n90.517\n2.831\n47.465\n18.553\n          91.0\n          34.0\n    Casablanca_Valley\n86.282\n2.428\n21.107\n11.953\n          87.0\n          30.0\n    Marlborough\n87.550\n2.245\n27.668\n13.833\n          85.0\n          25.0\n    New_York\n87.748\n2.268\n25.679\n9.565\n          88.0\n          35.0\n    Oregon\n89.489\n2.663\n44.856\n20.209\n          90.0\n          22.0\n  \n  \n    \n      \n   Download Excel"
  },
  {
    "objectID": "posts/FancyTables/index.html#conclusion",
    "href": "posts/FancyTables/index.html#conclusion",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Conclusion",
    "text": "Conclusion\nThis project highlights the importance of visually appealing and comprehensible data tables as an alternative to Excel. By incorporating design principles and exploring innovative tools, we enhance data presentation and interpretation. It calls for adopting alternative techniques to design data tables. By embracing visually appealing formats, we improve data comprehension, communication, and unlock new possibilities for visualization and analysis."
  },
  {
    "objectID": "posts/FancyTables/index.html#data-references",
    "href": "posts/FancyTables/index.html#data-references",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#purpose",
    "href": "posts/Param_Report_Retail/index.html#purpose",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Purpose",
    "text": "Purpose\nFor this particular undertaking, I focused on the utilization of parameterized Rmarkdown with the aim of enhancing the efficacy of report generation. This approach involves setting predetermined values for parameters and creating a function that automatically updates the information, thereby mitigating the need for manual modification of parameters for daily reports. The primary objective of this system is to generate reports more efficiently, which in turn helps businesses save time in the long run. Furthermore, data scientists, analysts, and engineers can benefit from this system as it enables them to produce comparable reports over time with ease. The automation of the report generation process through the use of parameterized Rmarkdown contributes to increased productivity and accuracy, which can be highly beneficial to the success of a business."
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#required-libraries",
    "href": "posts/Param_Report_Retail/index.html#required-libraries",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Required Libraries",
    "text": "Required Libraries\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)\nlibrary(rmarkdown)\nlibrary(purrr)"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#load-dataset-wrangle",
    "href": "posts/Param_Report_Retail/index.html#load-dataset-wrangle",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Load Dataset & Wrangle",
    "text": "Load Dataset & Wrangle\n\nds <- read_csv(\"retail.csv\")\n\n#head(ds)\n\nds <- ds %>% \n  rename(ID = ...1) %>%\n  mutate(Month = lubridate::floor_date(Date, 'month')) %>%\n  filter(year(Month) == params$year)\n\nglimpse(ds)\n\nRows: 10,042\nColumns: 9\n$ ID         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ DocumentID <dbl> 716, 716, 716, 716, 716, 716, 716, 460, 461, 462, 463, 464,…\n$ Date       <date> 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23…\n$ SKU        <dbl> 1039, 853, 862, 868, 2313, 2355, 2529, 2361, 2723, 655, 254…\n$ Price      <dbl> 381.78, 593.22, 423.73, 201.70, 345.76, 406.78, 542.38, 139…\n$ Discount   <dbl> 67.37254, 0.00034, -0.00119, 35.58814, 61.01966, 101.69458,…\n$ Customer   <dbl> 1, 1, 1, 1, 1, 1, 1, 460, 479, 26, 580, 311, 311, 311, 311,…\n$ Quantity   <dbl> 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 4,…\n$ Month      <date> 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01…"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#visualize-the-report",
    "href": "posts/Param_Report_Retail/index.html#visualize-the-report",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Visualize The Report",
    "text": "Visualize The Report\nI utilized ggplotly, a graphical representation tool, to create an interactive visualization of monthly sales time series data for “CRM and Invoicing system,” which is a wholesale company owned by Sadi Evren. The data for this analysis was obtained from the following Kaggle dataset: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv.\nThe resulting plot provided an insightful representation of the monthly sales data, showcasing trends and patterns in the data that could potentially provide useful information for decision making in the business.\nIn addition to the initial plot, I implemented a for loop to automatically generate multiple reports based on the time series data for each year. This approach eliminated the need for manual report generation, thereby saving time and reducing the risk of errors. The loop enabled the automated generation of separate reports for each year, which provided a comprehensive view of the sales trends over time.\nOverall, the use of ggplotly for data visualization and automation of report generation using a for loop demonstrated an effective approach for efficiently analyzing and presenting data.\n\np <- ds %>%\n  group_by(Month) %>%\n  summarize(AvgSales = round(mean(Price * Quantity),2) ) %>%\n  ggplot(aes(x = Month, \n             y = AvgSales,\n             group = 1,                 #Necessary or else line plot disappears\n             text = paste0(\"Monthly Sales: $\", (round(AvgSales/1000,2)),\"K\" ))) +\n  geom_line(size = 1) + \n  scale_y_continuous(labels = scales::dollar_format(scale = .001, suffix = \"K\")) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%B\") + \n  labs(title = paste0(\"CRM and Invoicing System Sales For FY: \", params$year),\n       caption = \"Source: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv\",\n       x = NULL,\n       y = NULL) +\n  myTheme()\n\nggplotly(p, tooltip = c(\"text\")) %>% \n  layout(hovermode = \"x unified\")"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#other-reports-generated",
    "href": "posts/Param_Report_Retail/index.html#other-reports-generated",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Other Reports Generated",
    "text": "Other Reports Generated\n\n2020 Report\n\n\n\n\n2021 Report\n\n\n\n\n2022 Report"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#function-to-run-parameterized-reports",
    "href": "posts/Param_Report_Retail/index.html#function-to-run-parameterized-reports",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Function To Run Parameterized Reports",
    "text": "Function To Run Parameterized Reports\n\nrenderReport <- function(year) {\n  rmarkdown::render(\n    'input.Rmd',\n    output_file = paste0(year, '.html'),\n    params = list(year = year),\n    envir = parent.frame()\n  )\n}"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#render-all-reports",
    "href": "posts/Param_Report_Retail/index.html#render-all-reports",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Render All Reports",
    "text": "Render All Reports\n\n# Renders all 4 Reports (dates range from 2019-2022)\nfor (year in 2019:2022) {\n    renderReport(year)\n}"
  },
  {
    "objectID": "posts/Param_Report_Retail/index.html#data-references",
    "href": "posts/Param_Report_Retail/index.html#data-references",
    "title": "Got Daily Reports? Use Parameterized RMarkdown To Alleviate That Stress",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#purpose",
    "href": "posts/Predicting_Income_Bracket/index.html#purpose",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Purpose",
    "text": "Purpose\nIn this project, we conducted a comprehensive machine learning analysis, encompassing various stages of the data science workflow, including data preprocessing, feature engineering, and feature selection through PCA. Collaboratively, we sought to predict whether individuals had an income greater than 50K. By incorporating hyperparameter optimization and deploying a robust model pipeline, we achieved a final Kappa score of 0.9543. This project enabled us to hone our skills as aspiring Data Scientists and paves the way for future machine learning endeavors."
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#highlight-of-the-project",
    "href": "posts/Predicting_Income_Bracket/index.html#highlight-of-the-project",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Highlight Of The Project",
    "text": "Highlight Of The Project\nWilla was instrumental in laying the foundation for the models and identifying the PCA features. Without her exceptional work, we would not have been able to create such a comprehensive and outstanding model for predicting income above $50,000. Her valuable contributions provided a critical starting point for our team, enabling us to add the feature importance at the start and optimize the models to their best parameters. Willa’s dedication, skill, and hard work were truly impressive, and I feel grateful to have had her as a partner on this project.\nHere is the direct link to her website. Take a look at her data science projects!"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#predicting-income-50k",
    "href": "posts/Predicting_Income_Bracket/index.html#predicting-income-50k",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Predicting Income >50K",
    "text": "Predicting Income >50K\n\nLoad Libraries\n\n#knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(caret)\nlibrary(tidymodels)\nlibrary(fastDummies)\nlibrary(randomForest)\n\n\n\nLoad The Data\n\nraw_income = read_csv(\"openml_1590.csv\", na = c(\"?\")) %>%\n  mutate(income_above_50k = ifelse(class == \">50K\",1,0))\n\nincome = read_csv(\"openml_1590.csv\", na = c(\"?\")) %>%\n  drop_na() %>%\n  mutate(income_above_50K = ifelse(class == \">50K\",1,0)) %>%\n  select(-class) %>%\n  dummy_cols(remove_selected_columns = T)"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#run-random-forest-obtain-importance-features",
    "href": "posts/Predicting_Income_Bracket/index.html#run-random-forest-obtain-importance-features",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Run Random Forest & Obtain Importance Features",
    "text": "Run Random Forest & Obtain Importance Features\n\nset.seed(504)\nraw_index <- createDataPartition(income$income_above_50K, p = 0.8, list = FALSE)\ntrain <- income[raw_index,]\ntest  <- income[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 3)\n\nfit <- train(income_above_50K ~ .,\n            data = train, \n            method = \"rf\",\n            ntree = 50,\n            tuneLength = 3,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n\nRandom Forest \n\n36178 samples\n  104 predictor\n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 24118, 24119, 24119 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n    2   0.3476024  0.4097953  0.2772094\n   53   0.3179101  0.4613186  0.1913666\n  104   0.3202068  0.4549034  0.1912386\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 53.\n\nprint(varImp(fit), 10)\n\nrf variable importance\n\n  only 10 most important variables shown (out of 104)\n\n                                    Overall\n`marital-status_Married-civ-spouse` 100.000\nfnlwgt                               88.391\n`capital-gain`                       72.132\nage                                  65.934\n`education-num`                      65.048\n`hours-per-week`                     38.148\nrelationship_Husband                 23.865\n`capital-loss`                       22.797\n`occupation_Exec-managerial`          8.631\n`occupation_Prof-specialty`           5.868"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#pca",
    "href": "posts/Predicting_Income_Bracket/index.html#pca",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "PCA",
    "text": "PCA\n\nChose Top 8 Features\n\ninc <- income %>%\n  select(-c(fnlwgt,\n            `marital-status_Married-civ-spouse`,\n            age,\n            `capital-gain`,\n            `education-num`,\n            `hours-per-week`,\n            relationship_Husband,\n            `capital-loss`))\n\n#Remained unchanged\npr_income = prcomp(x = inc, scale=T, center = T)\nscreeplot(pr_income, type=\"lines\")\n\n\n\nrownames_to_column(as.data.frame(pr_income$rotation)) %>%\n  select(1:11) %>%\n  filter(abs(PC1) >= 0.35 | abs(PC2) >= 0.35 | abs(PC3) >= 0.35 | abs(PC4) >= 0.35 | abs(PC5) >= 0.35 | abs(PC6) >= 0.35 | abs(PC7) >= 0.35 | abs(PC8) >= 0.35 | abs(PC9) >= 0.35 | abs(PC10) >= 0.35)\n\n                       rowname         PC1         PC2          PC3         PC4\n1            workclass_Private -0.17526595 -0.13547253  0.304797799  0.02401763\n2   workclass_Self-emp-not-inc  0.14801244  0.01168218 -0.067006960  0.07757365\n3       education_Some-college -0.06530044  0.06007649  0.046053650 -0.12180499\n4   relationship_Not-in-family -0.11626278  0.09345537 -0.009681895  0.02508551\n5            relationship_Wife -0.07643452  0.10243694 -0.155839801  0.10495583\n6                   race_Black -0.20749199 -0.06347990 -0.071867670 -0.26136856\n7                   sex_Female -0.43938132  0.23499209 -0.149918556  0.12969011\n8                     sex_Male  0.43938132 -0.23499209  0.149918556 -0.12969011\n9 native-country_United-States  0.08714546  0.46368406  0.199639060 -0.22938060\n          PC5          PC6         PC7         PC8         PC9        PC10\n1 -0.20886710  0.441798469 -0.22645965 -0.03914163 -0.04681505  0.00291428\n2  0.14138682 -0.254839748  0.35058106  0.05445338 -0.03103841 -0.33625026\n3 -0.07546707 -0.063088752  0.11632314  0.19569918  0.43973405  0.15884684\n4 -0.12317719 -0.049422098 -0.03404059 -0.57428375  0.20605726 -0.14712496\n5 -0.06292606  0.110184232  0.03508095  0.39257178 -0.25791740  0.12888672\n6  0.37153500 -0.037152844 -0.32702635  0.10727161  0.06843460 -0.15897839\n7 -0.10696874  0.005407197  0.07370032  0.08521373 -0.09415713 -0.03364547\n8  0.10696874 -0.005407197 -0.07370032 -0.08521373  0.09415713  0.03364547\n9  0.15037231  0.067618017 -0.02636173 -0.01036183 -0.06064057 -0.09505529\n\n\n\n\nChose First 10 PCA Features\n\n# IMPORTANT: Since I used 8 features, I updated the prc dataframe to include\n# the features + PCA 1-10\nprc <- \n  bind_cols(select(income, \n                   c(fnlwgt, \n                    `marital-status_Married-civ-spouse`, \n                    age, \n                    `capital-gain`, \n                    age, \n                    `hours-per-week`, \n                    relationship_Husband,\n                    `capital-loss`,\n                    income_above_50K)\n                   ), \n            as.data.frame(pr_income$x)\n            ) %>%\n  select(1:18) %>%\n  ungroup() %>%\n  rename(\"NonBlack_Men\" = PC1,\n         \"US_Women\" = PC2,\n         \"PrivateSec_Men\" = PC3,\n         \"NonUS_NonBlack\" = PC4,\n         \"NonPrivateSec_Black\" = PC5,\n         \"PrivateSec\" = PC6,\n         \"NonBlack_SelfEmploy\" = PC7,\n         \"Wives\" = PC8,\n         \"NonFamily_SomeCollege\" = PC9,\n         \"NotSelfEmployes_NonBlack\" = PC10)\n\nhead(prc)\n\n# A tibble: 6 × 18\n  fnlwgt marital-status_Married-civ-spou…¹   age `capital-gain` `hours-per-week`\n   <dbl>                             <int> <dbl>          <dbl>            <dbl>\n1 226802                                 0    25              0               40\n2  89814                                 1    38              0               50\n3 336951                                 1    28              0               40\n4 160323                                 1    44           7688               40\n5 198693                                 0    34              0               30\n6 104626                                 1    63           3103               32\n# ℹ abbreviated name: ¹​`marital-status_Married-civ-spouse`\n# ℹ 13 more variables: relationship_Husband <int>, `capital-loss` <dbl>,\n#   income_above_50K <dbl>, NonBlack_Men <dbl>, US_Women <dbl>,\n#   PrivateSec_Men <dbl>, NonUS_NonBlack <dbl>, NonPrivateSec_Black <dbl>,\n#   PrivateSec <dbl>, NonBlack_SelfEmploy <dbl>, Wives <dbl>,\n#   NonFamily_SomeCollege <dbl>, NotSelfEmployes_NonBlack <dbl>"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#gradient-boosting-machine",
    "href": "posts/Predicting_Income_Bracket/index.html#gradient-boosting-machine",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Gradient Boosting Machine",
    "text": "Gradient Boosting Machine\n\n#IMPORTANT: I took a while and messed around with the hyperparameters\n# Went From 0.2 Kappa to 0.6 Kappa BEFORE updating the features.\n# After updating to the top 8 features + PCA 1-5, it jumped to \n# 0.88 Kappa. Then I added PCA 1-10 and it jumped to 0.95 for the Kappa!\nset.seed(504)\nraw_index <- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain <- prc[raw_index,]\ntest  <- prc[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 5)\nweights <- ifelse(income$income_above_50K == 1, 75, 25)\n\nhyperparameters <- expand.grid(interaction.depth = 9, \n                    n.trees = 300, \n                    shrinkage = 0.1, \n                    n.minobsinnode = 4)\nfit <- train(factor(income_above_50K) ~ .,\n            data = train, \n            method = \"gbm\",\n            verbose = FALSE,\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n\nStochastic Gradient Boosting \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9827796  0.9533806\n\nTuning parameter 'n.trees' was held constant at a value of 300\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 4\n\n\n\nConfusion Matrix For GBM\n\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6798  111\n         1   42 2093\n                                          \n               Accuracy : 0.9831          \n                 95% CI : (0.9802, 0.9856)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9536          \n                                          \n Mcnemar's Test P-Value : 3.853e-08       \n                                          \n            Sensitivity : 0.9939          \n            Specificity : 0.9496          \n         Pos Pred Value : 0.9839          \n         Neg Pred Value : 0.9803          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7517          \n   Detection Prevalence : 0.7639          \n      Balanced Accuracy : 0.9717          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#logistical-model",
    "href": "posts/Predicting_Income_Bracket/index.html#logistical-model",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Logistical Model",
    "text": "Logistical Model\n\n#I messed around with using a logistical model\n#It turns out that it's pretty good too! Not as great as the GBM\n#But a great and easy model to explain!\n\nset.seed(504)\nraw_index <- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain <- prc[raw_index,]\ntest  <- prc[-raw_index, ]\nctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 3, verboseIter = FALSE)\nhyperparameters <- expand.grid(alpha = 1, \n                               lambda = 0.001)\n\nfit <- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"glmnet\",\n            family = \"binomial\",\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\",\n            importance = TRUE)\nfit\n\nglmnet \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 32560, 32561, 32561, 32561, 32559, 32559, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.9610166  0.895033\n\nTuning parameter 'alpha' was held constant at a value of 1\nTuning\n parameter 'lambda' was held constant at a value of 0.001\n\n\n\nConfusion Matrix For Logistical Regression\n\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6679  197\n         1  161 2007\n                                          \n               Accuracy : 0.9604          \n                 95% CI : (0.9562, 0.9643)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.892           \n                                          \n Mcnemar's Test P-Value : 0.06434         \n                                          \n            Sensitivity : 0.9765          \n            Specificity : 0.9106          \n         Pos Pred Value : 0.9713          \n         Neg Pred Value : 0.9257          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7385          \n   Detection Prevalence : 0.7603          \n      Balanced Accuracy : 0.9435          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#kmeans-clustering",
    "href": "posts/Predicting_Income_Bracket/index.html#kmeans-clustering",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\n\nkclust <- kmeans(na.omit(prc), centers = 4)\nkclust$centers\n\n    fnlwgt marital-status_Married-civ-spouse      age capital-gain\n1  86931.3                         0.4798800 39.36223     1178.652\n2 307822.2                         0.4480979 37.29197      993.978\n3 486698.8                         0.4317060 35.22655     1093.921\n4 188848.4                         0.4654970 38.77250     1090.494\n  hours-per-week relationship_Husband capital-loss income_above_50K\n1       41.21239            0.4228618     85.99175        0.2453963\n2       40.99760            0.4015361     87.95284        0.2414497\n3       40.18760            0.3812397     69.68404        0.2144816\n4       40.78357            0.4129092     92.41845        0.2551951\n  NonBlack_Men    US_Women PrivateSec_Men NonUS_NonBlack NonPrivateSec_Black\n1   0.06956303  0.10628238  -0.0890912042    -0.03258891        -0.008490584\n2  -0.04531689 -0.15055893   0.0006150531    -0.02235042         0.014458490\n3  -0.20329023 -0.44646489   0.1964744251     0.04160186         0.115358306\n4  -0.01331648  0.02500494   0.0462143267     0.02882888        -0.010110363\n   PrivateSec NonBlack_SelfEmploy        Wives NonFamily_SomeCollege\n1  0.03384552          0.17173996 -0.029902238           -0.02188199\n2 -0.07527701         -0.12068976 -0.008023897            0.09248884\n3 -0.26626813         -0.36982627  0.123605508            0.13942943\n4  0.03021208         -0.04107775  0.013720687           -0.03450577\n  NotSelfEmployes_NonBlack\n1              -0.03597294\n2               0.01302787\n3              -0.02812971\n4               0.02304227\n\nkclusts <- tibble(k = 1:9) %>%\n  mutate(\n    kclust = map(k, ~kmeans(prc, .x)),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, prc)\n  )\n\nclusterings <- kclusts %>%\n  unnest(glanced, .drop = TRUE)\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line()"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#augumenting-the-gbm-model-with-kmeans-clustering",
    "href": "posts/Predicting_Income_Bracket/index.html#augumenting-the-gbm-model-with-kmeans-clustering",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Augumenting The GBM Model with KMeans Clustering",
    "text": "Augumenting The GBM Model with KMeans Clustering\n\nprc2 <- augment(kclust, prc)\n\nset.seed(504)\nraw_index <- createDataPartition(prc2$income_above_50K, p = 0.8, list = FALSE)\n\ntrain <- prc2[raw_index,]\ntest  <- prc2[-raw_index, ]\nctrl <- trainControl(method = \"cv\", number = 5)\n\nhyperparameters <- expand.grid(\n  n.trees = 500,\n  interaction.depth = 5,\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n\n\nfit <- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"gbm\",\n            trControl = ctrl,\n            tuneGrid = hyperparameters,\n            verbose = FALSE)\nfit\n\nStochastic Gradient Boosting \n\n36178 samples\n   18 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9836365  0.9557243\n\nTuning parameter 'n.trees' was held constant at a value of 500\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\n\n\n\nConfusion Matrix For KMeans + GBM\n\n#We should be getting a Kappa of 0.9543!\n#Sensitivity = 0.9930, Specificity = 0.9533\n#Excellent Numbers!\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6792  103\n         1   48 2101\n                                          \n               Accuracy : 0.9833          \n                 95% CI : (0.9804, 0.9858)\n    No Information Rate : 0.7563          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9543          \n                                          \n Mcnemar's Test P-Value : 1.11e-05        \n                                          \n            Sensitivity : 0.9930          \n            Specificity : 0.9533          \n         Pos Pred Value : 0.9851          \n         Neg Pred Value : 0.9777          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7510          \n   Detection Prevalence : 0.7624          \n      Balanced Accuracy : 0.9731          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#results",
    "href": "posts/Predicting_Income_Bracket/index.html#results",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Results",
    "text": "Results\nWe used a random forest model to identify the most important variables and selected the top 8 features, then utilized principal component analysis (PCA) to further analyze the data. Our final model incorporated a gradient boosting machine (GBM) algorithm with optimized hyperparameters and achieved an accuracy rate of 0.9829731 and a Kappa score of 0.9538928. To ensure the model was not overfitting, we benchmarked it with a logistic regression model that achieved a Kappa score of 0.895033 and an accuracy rate of 0.9610166. We further improved the model’s accuracy by incorporating unsupervised machine learning with Kmeans clustering, achieving a Kappa score of 0.9543 and an accuracy rate of 0.9833. Overall, our approach of feature selection and PCA was effective and could be applied to future data analysis projects with some additional fine-tuning."
  },
  {
    "objectID": "posts/Predicting_Income_Bracket/index.html#data-references",
    "href": "posts/Predicting_Income_Bracket/index.html#data-references",
    "title": "Predicting Individuals Who Earn More Than 50K",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#abstract",
    "href": "posts/US_HealthCare_Spending/index.html#abstract",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "Abstract",
    "text": "Abstract\nSince 1980, healthcare costs in the United States have been consistently increasing across all categories. Various factors contribute to this rise, such as population growth and higher wages for doctors. This report examines expenditure trends from 1980 to 2005, extending up to 2014. The findings reveal an unprecedented surge in healthcare costs across every sector in the United States. Consequently, this report sheds light on the reasons behind the country’s reputation as one of the world’s most expensive nations in terms of healthcare."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#introduction",
    "href": "posts/US_HealthCare_Spending/index.html#introduction",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "Introduction",
    "text": "Introduction\nHealthcare plays a crucial role in our lives, providing essential support for our well-being and longevity. However, healthcare spending continues to soar annually. This report uncovers the alarming reality of escalating healthcare expenditure, presenting a visual representation of each component. It explores overall national spending and delves into individual categories, demonstrating the persistent upward trend in healthcare costs."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#background",
    "href": "posts/US_HealthCare_Spending/index.html#background",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "Background",
    "text": "Background\nThe dataset utilized for this report is titled “US Healthcare Spending Per Capita” and was obtained from Kaggle. The dataset’s format posed a challenge, as it followed a wide format with numerous columns and few rows. Notably, the years were presented in the format “Y####,” initially impeding analysis. However, by employing pivoting techniques and manipulating the strings, the dataset was transformed, enabling comprehensive analysis. The subsequent section outlines the complete step-by-step process."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#methodology",
    "href": "posts/US_HealthCare_Spending/index.html#methodology",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "Methodology",
    "text": "Methodology\nTo begin, it is essential to assess whether the data is in a “wide” or “long” format. This involves examining the number of rows and columns to facilitate necessary data wrangling.\n\n\n\n\n\n\nR Version\n\n\n\n\n\n\n\n\n# A tibble: 5 × 42\n   Code Item     Group Region_Number Region_Name State_Name  Y1980  Y1981  Y1982\n  <dbl> <chr>    <chr>         <dbl> <chr>       <chr>       <dbl>  <dbl>  <dbl>\n1     1 Persona… Unit…             0 United Sta… <NA>       216977 251789 283073\n2     1 Persona… Regi…             1 New England <NA>        12960  14845  16759\n3     1 Persona… Regi…             2 Mideast     <NA>        43479  49604  55406\n4     1 Persona… Regi…             3 Great Lakes <NA>        40658  46668  51440\n5     1 Persona… Regi…             4 Plains      <NA>        16980  19682  21919\n# ℹ 33 more variables: Y1983 <dbl>, Y1984 <dbl>, Y1985 <dbl>, Y1986 <dbl>,\n#   Y1987 <dbl>, Y1988 <dbl>, Y1989 <dbl>, Y1990 <dbl>, Y1991 <dbl>,\n#   Y1992 <dbl>, Y1993 <dbl>, Y1994 <dbl>, Y1995 <dbl>, Y1996 <dbl>,\n#   Y1997 <dbl>, Y1998 <dbl>, Y1999 <dbl>, Y2000 <dbl>, Y2001 <dbl>,\n#   Y2002 <dbl>, Y2003 <dbl>, Y2004 <dbl>, Y2005 <dbl>, Y2006 <dbl>,\n#   Y2007 <dbl>, Y2008 <dbl>, Y2009 <dbl>, Y2010 <dbl>, Y2011 <dbl>,\n#   Y2012 <dbl>, Y2013 <dbl>, Y2014 <dbl>, …\n\n\n [1] \"Code\"                          \"Item\"                         \n [3] \"Group\"                         \"Region_Number\"                \n [5] \"Region_Name\"                   \"State_Name\"                   \n [7] \"Y1980\"                         \"Y1981\"                        \n [9] \"Y1982\"                         \"Y1983\"                        \n[11] \"Y1984\"                         \"Y1985\"                        \n[13] \"Y1986\"                         \"Y1987\"                        \n[15] \"Y1988\"                         \"Y1989\"                        \n[17] \"Y1990\"                         \"Y1991\"                        \n[19] \"Y1992\"                         \"Y1993\"                        \n[21] \"Y1994\"                         \"Y1995\"                        \n[23] \"Y1996\"                         \"Y1997\"                        \n[25] \"Y1998\"                         \"Y1999\"                        \n[27] \"Y2000\"                         \"Y2001\"                        \n[29] \"Y2002\"                         \"Y2003\"                        \n[31] \"Y2004\"                         \"Y2005\"                        \n[33] \"Y2006\"                         \"Y2007\"                        \n[35] \"Y2008\"                         \"Y2009\"                        \n[37] \"Y2010\"                         \"Y2011\"                        \n[39] \"Y2012\"                         \"Y2013\"                        \n[41] \"Y2014\"                         \"Average_Annual_Percent_Growth\"\n\n\nEarlier, we noticed that the dataset had a wide format, which means the years were in separate columns. To make it easier to analyze, we rearranged the data using a special technique. We combined the year columns into a single “Year” column and placed their corresponding values in a new column called “Cost.”\nWe also made some adjustments to the “Year” column by removing a specific symbol and converting it to numbers. This way, we can work with the years as numeric values instead of text.\nAdditionally, we transformed certain columns into categories, which help us group and analyze the data more effectively. These categories include “Item,” “Region_Name,” “Group,” and “State_Name.”\nFinally, we selected specific columns, including “Item,” “Region_Name,” “State_Name,” “Year,” and “Cost,” to focus on for further analysis. This will provide us with a clearer understanding of the data.\n\n\n# A tibble: 6 × 5\n  Item                 Region_Name   State_Name  Year   Cost\n  <fct>                <fct>         <fct>      <dbl>  <dbl>\n1 Personal Health Care United States <NA>        1980 216977\n2 Personal Health Care United States <NA>        1981 251789\n3 Personal Health Care United States <NA>        1982 283073\n4 Personal Health Care United States <NA>        1983 311677\n5 Personal Health Care United States <NA>        1984 341645\n6 Personal Health Care United States <NA>        1985 376376"
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#rising-health-care-costs",
    "href": "posts/US_HealthCare_Spending/index.html#rising-health-care-costs",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "Rising Health Care Costs",
    "text": "Rising Health Care Costs\nLet’s jump right into the first visualization. It’s evident that healthcare spending has been consistently increasing and shows no signs of slowing down. This graph focuses on the years 1980 to 2005, highlighting the era of escalating healthcare costs.\n\n\n\n\n\n\nDominant Spending Categories: Personal, Hospital, and Physician & Clinical Care\nWow! Personal health care expenses skyrocketed from around $10K to nearly $80K within a relatively short period. Both Hospital and Clinical Care play significant roles in healthcare spending. Surprisingly, all three categories follow a similar upward trend, which reveals some unsettling information.\n\n\n\n\n\n\n\nConsistent Trends Across Regions\nIt’s disheartening to report that every region has been experiencing relentless growth in healthcare spending. The trend lines for each region are strikingly similar and proportionate to one another. Notably, the Mideast stands out as the most expensive region, while the Rocky Mountains region appears to have comparatively lower healthcare costs. It’s important to note that this analysis considers spending up to 2005, and we can hope for potential changes by 2014.\n\n\n\n\n\n\n\nPersistent Trends: Rising Healthcare Spending Across U.S. Regions\nThe bar chart vividly demonstrates the ongoing trends in healthcare spending across different regions. Notably, the Plains, New England, and the Rocky Mountains regions emerge as some of the lowest in terms of medical funding. Surprisingly, their costs can be as low as one-third compared to the most expensive regions. This stark contrast highlights the significant disparities in healthcare expenditure throughout the United States.\n\n\n\n\n\n\n\nFar West: A Surprising 3rd Place in Healthcare Spending\nIn 2014, the Far West region experienced a significant surge in healthcare spending, landing them in the 3rd position. This unexpected leap challenges the assumption that states within this region are heavy spenders. However, the subsequent graphic reveals an intriguing revelation that contradicts this perception.\n\n\n\n\n\n[1] \"$21.81M\"\n[1] \"$21.81M\"\n\n\n\n\nOregon: 3rd Place, but Don’t Be Deceived!\nSurprisingly, Oregon ranks 3rd in healthcare spending. However, let’s not overlook the undeniable fact that California claims the top spot. The massive population size of California is a significant contributing factor to its high expenditure. Although this report doesn’t delve into the specific reasons, it’s plausible that further analysis would align the Far West region more closely with the spending patterns observed in the Plains or New England regions.\n\n\n\n\n\n[1] \"$1.41M\"  \"$53.64M\" \"$1.9M\"   \"$3.41M\"  \"$5.81M\"  \"$10.16M\"\n[1] \"$1.41M\"  \"$53.64M\" \"$1.9M\"   \"$3.41M\"  \"$5.81M\"  \"$10.16M\"\n[1] \"$5.81M\"\n[1] \"$5.81M\"\n\n\n\n\nInappropriate Model: Linear Fit Inadequate for the Data\nAt first glance, the model may seem impressive with an adjusted R-squared value of 0.8572. However, this is deceptive. It’s crucial to note that this model is highly inaccurate and strongly discouraged. The analysis reveals no correlation between Cost and Region_Name per Year, a finding consistent with the filtered dataset covering the years 1980 to 2014.\nThe residual plots provide clear evidence against a linear fit. The Residuals vs Fitted plot indicates a clear quadratic relationship rather than a linear one. The Q-Q plot deviates from linearity, exhibiting multiple curves along the fitted line. Additionally, the scale-location plot highlights that this model is fundamentally unsuitable for the data.\nIt is evident that a linear fit is not the appropriate choice for accurately modeling this dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      1 \n11072.4 \n\n\n\nCall:\nlm(formula = Cost ~ Region_Name + Year, data = regionHealthCareSince2005)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2683.8  -782.9  -279.8   597.7  4139.3 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -680650.41   24479.29 -27.805  < 2e-16 ***\nRegion_NameGreat Lakes        1438.65     368.55   3.904 0.000130 ***\nRegion_NameMideast            1657.50     368.55   4.497 1.17e-05 ***\nRegion_NameNew England       -3909.68     368.55 -10.608  < 2e-16 ***\nRegion_NamePlains            -3830.75     368.55 -10.394  < 2e-16 ***\nRegion_NameRocky Mountains   -5106.26     368.55 -13.855  < 2e-16 ***\nRegion_NameSoutheast         -1231.18     368.55  -3.341 0.000998 ***\nRegion_NameSouthwest          -849.93     368.55  -2.306 0.022133 *  \nYear                           344.83      12.29  28.069  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1329 on 199 degrees of freedom\nMultiple R-squared:   0.88, Adjusted R-squared:  0.8752 \nF-statistic: 182.4 on 8 and 199 DF,  p-value: < 2.2e-16\n\n\n             Df    Sum Sq   Mean Sq F value Pr(>F)    \nRegion_Name   7 1.185e+09 1.693e+08    95.9 <2e-16 ***\nYear          1 1.391e+09 1.391e+09   787.9 <2e-16 ***\nResiduals   199 3.514e+08 1.766e+06                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nSignificant Differences in Means among Regions\nMy objective was to investigate whether there were significant differences in the means of each region’s spending over the years. To start, I utilized the LeveneTest to examine the importance of variance (i.e., the spread of spending) across regions. Both tests yielded remarkably small p-values, indicating that three regions had substantially different variances compared to the others.\nBuilding on this, I employed the TukeyHsd test to confirm if these differing variances were reflected in the means. As anticipated from the LeveneTest results, the means of these regions indeed exhibited significant differences. Notably, New England, Plains, and Rocky Mountains had considerably lower average spending. However, despite their comparatively lower spending, these regions still followed the overall growth trend, with an increase of 18% since 2005.\n\n\n[1] \"The Average Spending In The Expensive Regions since 2005 = $5333.29\"\n\n\n[1] \"The Average Spending In The Expensive Regions since 2014= $7724.45\"\n\n\n[1] \"Difference: +$2391.16 | Percentage Increase: +18.31%\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2005 = $2173.22\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2014= $3142.21\"\n\n\n[1] \"Difference: +$968.99 | Percentage Increase: 18.23%\"\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(>F)    \ngroup   7   12.03 8.727e-13 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   7  11.462 3.292e-12 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(>F)    \ngroup   7  19.546 < 2.2e-16 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   7  12.828 3.075e-14 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n             Df    Sum Sq   Mean Sq F value Pr(>F)    \nRegion_Name   7 1.185e+09 169337440   19.43 <2e-16 ***\nResiduals   200 1.743e+09   8712933                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Cost ~ Region_Name, data = regionHealthCareSince2005)\n\n$Region_Name\n                                   diff         lwr           upr     p adj\nGreat Lakes-Far West         1438.64777 -1069.19906  3946.4945990 0.6495584\nMideast-Far West             1657.50100  -850.34583  4165.3478291 0.4679930\nNew England-Far West        -3909.68132 -6417.52815 -1401.8344885 0.0000930\nPlains-Far West             -3830.75287 -6338.59970 -1322.9060420 0.0001417\nRocky Mountains-Far West    -5106.25783 -7614.10466 -2598.4109954 0.0000001\nSoutheast-Far West          -1231.18113 -3739.02796  1276.6657036 0.8046614\nSouthwest-Far West           -849.92577 -3357.77260  1657.9210559 0.9679983\nMideast-Great Lakes           218.85323 -2288.99360  2726.7000602 0.9999950\nNew England-Great Lakes     -5348.32909 -7856.17592 -2840.4822574 0.0000000\nPlains-Great Lakes          -5269.40064 -7777.24747 -2761.5538109 0.0000000\nRocky Mountains-Great Lakes -6544.90559 -9052.75242 -4037.0587643 0.0000000\nSoutheast-Great Lakes       -2669.82890 -5177.67573  -161.9820653 0.0279871\nSouthwest-Great Lakes       -2288.57354 -4796.42037   219.2732870 0.1019616\nNew England-Mideast         -5567.18232 -8075.02915 -3059.3354875 0.0000000\nPlains-Mideast              -5488.25387 -7996.10070 -2980.4070410 0.0000000\nRocky Mountains-Mideast     -6763.75882 -9271.60565 -4255.9119944 0.0000000\nSoutheast-Mideast           -2888.68213 -5396.52896  -380.8352954 0.0119419\nSouthwest-Mideast           -2507.42677 -5015.27360     0.4200569 0.0500724\nPlains-New England             78.92845 -2428.91838  2586.7752767 1.0000000\nRocky Mountains-New England -1196.57651 -3704.42334  1311.2703233 0.8267302\nSoutheast-New England        2678.50019   170.65336  5186.3470223 0.0270977\nSouthwest-New England        3059.75554   551.90871  5567.6023746 0.0058329\nRocky Mountains-Plains      -1275.50495 -3783.35178  1232.3418768 0.7745369\nSoutheast-Plains             2599.57175    91.72492  5107.4185757 0.0361922\nSouthwest-Plains             2980.82710   472.98027  5488.6739280 0.0081616\nSoutheast-Rocky Mountains    3875.07670  1367.22987  6382.9235291 0.0001119\nSouthwest-Rocky Mountains    4256.33205  1748.48522  6764.1788814 0.0000135\nSouthwest-Southeast           381.25535 -2126.59148  2889.1021825 0.9997829"
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#results",
    "href": "posts/US_HealthCare_Spending/index.html#results",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "Results",
    "text": "Results\nThe cost of healthcare in the United States has increased more than fivefold between 1980 and 2014. Across regions, there is a consistent upward trend in healthcare spending with no clear indications of a decrease. Although some regions are less expensive than others, their growth rates align with the national average. Personal health care spending, which averaged around $10,000 in 1980, has significantly risen to nearly $80,000 in 2014.\nAs of 2014, the Mideast, Great Lakes, and Far West regions rank as the top three most expensive regions, while the Rocky Mountains, New England, and Plains regions are the least expensive.\nWithin the Far West region, Oregon stands out as the third most expensive state."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#conclusion",
    "href": "posts/US_HealthCare_Spending/index.html#conclusion",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "Conclusion",
    "text": "Conclusion\nThe United States continues to experience escalating healthcare expenditures, raising concerns about the affordability of personal health care. The substantial increase of approximately $70,000 over a span of 35 years far exceeds inflation expectations. It would have been beneficial to have inflation-adjusted values in the dataset, allowing for a more comprehensive analysis and deeper insights.\nFurther exploration can be done to investigate potential statistical significance between individual states and their spending patterns. This avenue remains open for future researchers to delve into for a more in-depth understanding of healthcare expenditure variations."
  },
  {
    "objectID": "posts/US_HealthCare_Spending/index.html#data-references",
    "href": "posts/US_HealthCare_Spending/index.html#data-references",
    "title": "Healthcare Spending Is Only Getting Worse",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#purpose",
    "href": "posts/Customer Return ML/index.html#purpose",
    "title": "Predicting Customer Returns",
    "section": "Purpose",
    "text": "Purpose\nIn this evaluation, I scrutinized a partially randomly generated customer returns dataset with the objective of forecasting whether a customer would opt to return their purchased item. By employing progressive techniques that advanced from Logistical to Random Forest, I achieved a robust prediction of customer behavior with respect to product returns."
  },
  {
    "objectID": "posts/Customer Return ML/index.html#approach-and-methodology",
    "href": "posts/Customer Return ML/index.html#approach-and-methodology",
    "title": "Predicting Customer Returns",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\nTo ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as “Season”, “CustomerAge”, “MSRP”, and “PriceRange”, that may play a significant role in predicting a customer’s probability of returning their product. I ran a Random Forest model and achieved an AUC of 0.625. This indicates moderate predictive power for the model, and suggests that further feature engineering or model tuning may be necessary for better performance.\nInitially, I utilized a logistic regression model, however, it required further refinement and tuning. Due to a three-hour time constraint, I focused on building a draft model. While not suitable for production, it is a step towards the right direction."
  },
  {
    "objectID": "posts/Customer Return ML/index.html#load-the-required-packages",
    "href": "posts/Customer Return ML/index.html#load-the-required-packages",
    "title": "Predicting Customer Returns",
    "section": "Load the required packages",
    "text": "Load the required packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#load-the-training-and-test-data",
    "href": "posts/Customer Return ML/index.html#load-the-training-and-test-data",
    "title": "Predicting Customer Returns",
    "section": "Load the training and test data",
    "text": "Load the training and test data\n\ntrain <- read_csv(\"train.csv\") \ntest <- read_csv(\"test.csv\") \n\nglimpse(train)\n\nRows: 64,912\nColumns: 12\n$ ID                <chr> \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           <chr> \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        <chr> \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     <chr> \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate <date> 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         <date> 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment <chr> \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       <chr> \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#data-exploration",
    "href": "posts/Customer Return ML/index.html#data-exploration",
    "title": "Predicting Customer Returns",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Look at Product Department\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductDepartment)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Department\") + \n  theme_minimal()\n\n\n\n# Look at Product Size\ntrain %>% \n  filter(Returned == 1) %>%\n  ggplot(aes(x = ProductSize)) +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per Product Size\") + \n  theme_minimal()\n\n\n\n#This won't be that valuable\ntrain %>% \n  mutate(CustomerState = factor(CustomerState)) %>%\n  filter(Returned == 1) %>%\n  ggplot(aes(x = CustomerState)) +\n  coord_flip() +\n  geom_bar(fill = \"#e67838\") +\n  labs(title = \"Number of Returns Per State\") + \n  theme_minimal()"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#feature-engineering",
    "href": "posts/Customer Return ML/index.html#feature-engineering",
    "title": "Predicting Customer Returns",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Creates the features\nbuildFeatures <- function(ds){\n  CurrentDate <- Sys.Date()\n  ds %>%\n  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\")),\n         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ \"Winter\",\n                            months(OrderDate) %in% month.name[4:6] ~ \"Spring\",\n                            months(OrderDate) %in% month.name[7:9] ~ \"Summer\",\n                            months(OrderDate) %in% month.name[10:12] ~ \"Fall\")), \n         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),\n         MSRP = round(PurchasePrice / (1 - DiscountPct)),\n         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ \"$13-$30\",\n                             MSRP > 30 & MSRP <= 60 ~ \"$31-$60\",\n                             MSRP > 60 & MSRP <= 100 ~ \"$61-$100\",\n                             MSRP > 100 ~ \">$100\")),\n         ProductDepartment = as.factor(ProductDepartment),\n         ProductSize = as.factor(ProductSize),\n         CustomerState = as.factor(CustomerState)\n  ) %>%\n  select(-OrderDate, \n         -CustomerBirthDate, \n         -ID, \n         -OrderID, \n         -CustomerID)\n}\n\nIDCols <- test$ID\n\n#Removes and adds columns for train and test sets\ntrain <- buildFeatures(train)\ntest <- buildFeatures(test)\n\n\n#Inspect the dataset before training the model\nglimpse(train)\n\nRows: 64,912\nColumns: 11\n$ CustomerState     <fct> Kentucky, Kentucky, Kentucky, Kentucky, Indiana, Ind…\n$ ProductDepartment <fct> Youth, Mens, Mens, Mens, Womens, Womens, Youth, Yout…\n$ ProductSize       <fct> M, L, XL, L, XS, M, XS, M, M, M, L, L, XL, M, XXL, M…\n$ ProductCost       <dbl> 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       <dbl> 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     <dbl> 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          <fct> No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ Season            <fct> Winter, Winter, Winter, Winter, Winter, Winter, Wint…\n$ CustomerAge       <dbl> 56, 56, 56, 56, 44, 44, 44, 56, 56, 56, 58, 58, 39, …\n$ MSRP              <dbl> 30, 51, 59, 64, 122, 128, 45, 17, 55, 82, 91, 108, 9…\n$ PriceRange        <fct> $13-$30, $31-$60, $31-$60, $61-$100, >$100, >$100, $…\n\nsummary(train)\n\n      CustomerState     ProductDepartment ProductSize  ProductCost   \n California  : 7612   Accessories: 3952   ~  : 3952   Min.   : 3.00  \n Texas       : 5455   Mens       :27695   L  :16588   1st Qu.:18.00  \n New York    : 4002   Womens     :26922   M  :16223   Median :25.00  \n Florida     : 3875   Youth      : 6343   S  : 9674   Mean   :26.01  \n Illinois    : 2686                       XL : 8874   3rd Qu.:33.00  \n Pennsylvania: 2547                       XS : 5235   Max.   :59.00  \n (Other)     :38735                       XXL: 4366                  \n  DiscountPct     PurchasePrice    Returned       Season       CustomerAge   \n Min.   :0.0019   Min.   :  8.73   No :42035   Fall  :23194   Min.   :26.00  \n 1st Qu.:0.0882   1st Qu.: 44.79   Yes:22877   Spring:12576   1st Qu.:36.00  \n Median :0.1717   Median : 62.54               Summer:16620   Median :47.00  \n Mean   :0.1689   Mean   : 65.42               Winter:12522   Mean   :48.56  \n 3rd Qu.:0.2541   3rd Qu.: 84.84                              3rd Qu.:60.00  \n Max.   :0.3329   Max.   :132.72                              Max.   :77.00  \n                                                                             \n      MSRP           PriceRange   \n Min.   : 13.00   >$100   :17087  \n 1st Qu.: 55.00   $13-$30 : 2720  \n Median : 77.00   $31-$60 :18193  \n Mean   : 78.51   $61-$100:26912  \n 3rd Qu.:102.00                   \n Max.   :133.00                   \n                                  \n\ntable(train$Returned)\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#fit-a-random-forest-model",
    "href": "posts/Customer Return ML/index.html#fit-a-random-forest-model",
    "title": "Predicting Customer Returns",
    "section": "Fit a Random Forest Model",
    "text": "Fit a Random Forest Model\n\nset.seed(345)\n\n#Model using Random Forest \nctrl <- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)\nfit <- train(Returned ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 50,\n             tuneLength = 3,\n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n\nRandom Forest \n\n64912 samples\n   10 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 43274, 43275, 43275 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5554701  1.0000000  0.0000000\n  36    0.6249949  0.8332817  0.3606679\n  70    0.6245694  0.8249790  0.3712897\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 36."
  },
  {
    "objectID": "posts/Customer Return ML/index.html#make-prediction-on-the-test-data",
    "href": "posts/Customer Return ML/index.html#make-prediction-on-the-test-data",
    "title": "Predicting Customer Returns",
    "section": "Make prediction on the test data",
    "text": "Make prediction on the test data\n\ntestPredictions <- predict(fit, newdata = test, type = \"prob\")[,2]"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#writing-the-submission-file",
    "href": "posts/Customer Return ML/index.html#writing-the-submission-file",
    "title": "Predicting Customer Returns",
    "section": "Writing the Submission File",
    "text": "Writing the Submission File\n\nsubmission <- data.frame(ID = IDCols, Prediction = testPredictions)\nwrite.csv(submission, \"submission.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#leftout-features-that-were-considered",
    "href": "posts/Customer Return ML/index.html#leftout-features-that-were-considered",
    "title": "Predicting Customer Returns",
    "section": "Leftout Features that were considered",
    "text": "Leftout Features that were considered\n\n#odds_ratio <- exp(coef(fit$finalModel))\n#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  \n#  arrange(desc(odds_ratio)) %>% \n#  head()\n\n#Sampling\n#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)\n#train <- returns_train[train_index, ]\n#test <- returns_train[-train_index, ]\n\n\n#Returned = factor(Returned, levels = c(0, 1), labels = c(\"No\",\"Yes\"))\n#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ \"18-30\",\n#                              CustomerAge > 30 & CustomerAge <= 45 ~ \"31-45\",\n#                              CustomerAge > 45 & CustomerAge <= 60 ~ \"46-60\",\n#                              CustomerAge > 60 ~ \">61\"),\n#                           levels = c(\"18-30\", \"31-45\", \"46-60\", \">61\"))\n\n\n#select(-OrderDate, \n#         -CustomerBirthDate, \n#         -ID, \n#         -OrderID, \n#         -CustomerID,\n#         -PurchasePrice,\n#         -DiscountPct,\n#         -MSRP,\n#         -ProductCost,\n#         -CustomerState)"
  },
  {
    "objectID": "posts/Customer Return ML/index.html#data-references",
    "href": "posts/Customer Return ML/index.html#data-references",
    "title": "Predicting Customer Returns",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#abstract",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#abstract",
    "title": "Resources for Prospective College Students",
    "section": "Abstract",
    "text": "Abstract\nIn collaboration with Corey Cassell, this study aims to provide prospective college students with crucial information about potential income and student debt in their chosen career paths. To achieve this, we have developed an interactive tool using R Shiny to create engaging visualizations."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#introduction",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#introduction",
    "title": "Resources for Prospective College Students",
    "section": "Introduction",
    "text": "Introduction\nAs students make important decisions about their higher education, it becomes crucial to equip them with insights into the financial aspects of their chosen career paths. To address this need, we have created an interactive tool consisting of four components: salary estimator, tuition estimator, debt estimator, and debt calculator. These tools enable prospective students to explore potential salaries, estimate tuition costs, calculate degree debt, and visualize projected student debts based on their chosen majors."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#shiny-interactive-tool",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#shiny-interactive-tool",
    "title": "Resources for Prospective College Students",
    "section": "Shiny Interactive Tool",
    "text": "Shiny Interactive Tool"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#setup",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#setup",
    "title": "Resources for Prospective College Students",
    "section": "Setup",
    "text": "Setup\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(rsconnect)\nlibrary(colorspace)\nlibrary(plotly)\nlibrary(shinyWidgets)\nlibrary(scales)\nlibrary(ggplot2)\n\n# read the files in from github\nallAgesDf <- read_csv(\"all-ages.csv\")\ntuition_cost <- read_csv(\"tuition_income.csv\")\ntuition <- read_csv(\"tuition_cost.csv\")\nds4 <- read_csv(\"salary_and_stats.csv\")\n\n# Wrangling Salary Potential\nsalary <- allAgesDf %>%\n    dplyr::select(Major, P25th, Median, P75th) %>%\n    pivot_longer(c(P25th, Median, P75th),\n        names_to = \"Percentile_Range\", values_to = \"Salary\"\n    ) %>%\n    arrange(Major) %>%\n    mutate(\n        Percentile_Range = as.factor(Percentile_Range),\n        Major = as.factor(Major)\n    )\n\n# Wrangling Potential Tuition Burden\n\n\ntuition_cost <- tuition_cost %>%\n    filter(year == 2018 & net_cost > 0) %>%\n    arrange(name) %>%\n    mutate(\n        income_lvl = as.factor(income_lvl),\n        name = as.factor(name)\n    )\n\n\ntuition_cost$income_lvl <- recode(tuition_cost$income_lvl,\n    \"0 to 30,000\" = \"$0 to $30,000\",\n    \"30,001 to 48,000\" = \"$30,001 to $48,000\",\n    \"48_001 to 75,000\" = \"$48,001 to $75,000\",\n    \"75,001 to 110,000\" = \"$75,001 to $110,000\",\n    \"Over 110,000\" = \"Over $110,000\"\n)\nsalary$Percentile_Range <- factor(salary$Percentile_Range, levels = c(\"P25th\", \"Median\", \"P75th\"))\nsalary$Percentile_Range <- recode(salary$Percentile_Range,\n    \"P25th\" = \"Early Career\",\n    \"Median\" = \"Middle Career\",\n    \"P75th\" = \"Late Career\"\n)\nsalary$Major <- str_to_title(salary$Major)\nsalary$Major <- gsub(\"And\", \"and\", salary$Major)\n\n\ndf <- tuition %>%\n    group_by(state, degree_length, type) %>%\n    filter(!is.na(state) & degree_length != \"Other\") %>%\n    summarise(\n        room_expenses = mean(room_and_board, na.rm = TRUE),\n        inStateTotal = mean(in_state_total, na.rm = TRUE),\n        outOfStateTotal = mean(out_of_state_total, na.rm = TRUE)\n    )\n\ndf$degree_length <- as.factor(df$degree_length)\ndf$type <- as.factor(df$type)\n\ndf <- df %>% rename(\n    \"Room and Board\" = room_expenses,\n    \"In State Tuition\" = inStateTotal,\n    \"Out of State Tuition\" = outOfStateTotal\n)\n\n\n# vars\ntitle <- 25\nsubtitle <- 20\nfacet_title <- 25\naxis_title <- 18\ntick_numbers <- 13\ntitle_color <- \"black\"\nbackground <- \"gainsboro\"\nplot_background <- \"gainsboro\"\nfacet_header_background <- \"gainsboro\"\nline_type <- \"solid\"\n\nCoreyPlotTheme <- theme(\n    text = element_text(family = \"Futura\"),\n    # background color of page\n    plot.background = element_rect(fill = background),\n\n    # graph background and grid\n    panel.background = element_blank(),\n    panel.grid.major = element_line(size = .1, linetype = line_type, colour = \"gainsboro\"),\n    panel.grid.minor = element_line(size = .1, linetype = line_type, colour = \"black\"),\n\n    # title/font/labels\n    plot.title = element_text(color = title_color, size = title, family = \"Futura\", hjust = 0.5),\n    plot.subtitle = element_text(color = title_color, size = subtitle, family = \"Futura\", hjust = 0.5),\n    # plot.caption = element_textbox_simple(halign = 0, size = tick_numbers, maxwidth = 30,family = \"Futura\"),\n    plot.caption = element_text(color = title_color, face = \"bold\", size = tick_numbers, family = \"Futura\", hjust = 0),\n    strip.text = element_text(color = title_color, size = facet_title, family = \"Futura\"),\n    strip.background = element_rect(fill = facet_header_background),\n\n    # tick marks\n    axis.text = element_text(color = title_color, size = tick_numbers, family = \"Futura\"),\n    axis.title = element_text(color = title_color, size = axis_title, family = \"Futura\"),\n    axis.ticks.x = element_blank(),\n\n    # legend\n    legend.title = element_text(color = title_color, size = subtitle, family = \"Futura\"),\n    legend.background = element_rect(fill = plot_background),\n    legend.text = element_text(size = tick_numbers, family = \"Futura\")\n)"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#plot-sidebar-inputs",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#plot-sidebar-inputs",
    "title": "Resources for Prospective College Students",
    "section": "Plot Sidebar Inputs",
    "text": "Plot Sidebar Inputs\n\n# INPUT FOR PLOT 1\n\ninput1 <- inputPanel(\n    selectInput(\"selectInput1\",\n        label = \"Choose your major:\",\n        choices = unique(salary$Major),\n        selected = \"ART HISTORY AND CRITICISM\"\n    ),\n    checkboxGroupInput(\"percentile_choice\",\n        label = \"Pick your career level:\",\n        choices = list(\n            \"Early Career \" = \"Early Career\",\n            \"Middle Career \" = \"Middle Career\",\n            \"Late Career \" = \"Late Career\"\n        ),\n        selected = c(\"Early Career\", \"Middle Career\", \"Late Career\")\n    ),\n)\n\n\n# INPUT FOR PLOT 2\n\ninput2 <- inputPanel(\n    selectInput(\"money\",\n        label = \"Select the type of expense:\",\n        choices = c(\n            \"Room and Board\" = \"Room and Board\",\n            \"In State Tuition\" = \"In State Tuition\",\n            \"Out of State Tuition\" = \"Out of State Tuition\"\n        ),\n        selected = \"In State Tuition\"\n    ),\n    selectInput(\"state\",\n        label = \"Pick your State:\",\n        choices = unique(df$state),\n        selected = \"Oregon\"\n    ),\n)\n\n\n# INPUT FOR PLOT 3\n\ninput3 <- inputPanel(\n    selectInput(\"selectInput2\",\n        label = \"Select your university:\",\n        choices = unique(tuition_cost$name),\n        selected = \"Willamette University\"\n    ),\n    checkboxGroupInput(\"checkGroup\",\n        label = \"Select your household income bracket:\",\n        choices = list(\n            \"$0 to $30,000\" = \"$0 to $30,000\",\n            \"$30,001 to $48,000\" = \"$30,001 to $48,000\",\n            \"$48,001 to $75,000\" = \"$48,001 to $75,000\",\n            \"$75,001 to $110,000\" = \"$75,001 to $110,000\",\n            \"Over $110,000\" = \"Over $110,000\"\n        ),\n        selected = c(\n            \"$0 to $30,000\",\n            \"$30,001 to $48,000\",\n            \"$48,001 to $75,000\",\n            \"$75,001 to $110,000\",\n            \"Over $110,000\"\n        )\n    ),\n)\n\n\n# INPUT FOR PLOT 4\n\ninput4 <- inputPanel(\n    selectInput(\"major_category\",\n        label = \"Pick a major category:\",\n        choices = unique(ds4$major_category),\n        selected = \"Computers & Mathematics\"\n    ),\n)"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#plots",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#plots",
    "title": "Resources for Prospective College Students",
    "section": "Plots",
    "text": "Plots\n\n# PLOT1\nplot1 <- renderPlot({\n    salary %>%\n        filter((Major %in% input$selectInput1) & (Percentile_Range %in% input$percentile_choice)) %>%\n        ggplot(aes(x = Percentile_Range, y = Salary, fill = Percentile_Range)) +\n        geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n        geom_label(\n            aes(\n                y = Salary,\n                label = print(paste0(\"$\", round(Salary / 1000, 2), \"K\"))\n            ),\n            show.legend = FALSE,\n            size = 7,\n            family = \"Futura\",\n            fill = \"white\"\n        ) +\n        scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Estimated Salary for \", input$selectInput1),\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n# PLOT2\nplot2 <- renderPlot({\n    df %>%\n        filter(state == input$state) %>%\n        ggplot(aes(x = degree_length, y = .data[[input$money]], fill = degree_length)) +\n        geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n        facet_wrap(~type) +\n        geom_label(\n            aes(\n                y = .data[[input$money]],\n                label = print(paste0(\"$\", round(.data[[input$money]] / 1000, 2), \"K\"))\n            ),\n            family = \"Oswald\",\n            size = 7,\n            show.legend = FALSE,\n            fill = \"white\"\n        ) +\n        scale_y_continuous(\n            labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3),\n            limits = c(0, 55000)\n        ) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Average \", input$money, \" for \", input$state, \" Universities\"),\n            subtitle = \"For Undergraduate Degrees\",\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n# PLOT3\nplot3 <- renderPlot({\n    tuition_cost %>%\n        filter((income_lvl %in% input$checkGroup) & (name %in% input$selectInput2)) %>%\n        ggplot(aes(x = income_lvl, y = net_cost, fill = income_lvl)) +\n        geom_col(color = \"black\", width = 0.4, position = \"dodge\", show.legend = FALSE) +\n        geom_label(\n            aes(\n                y = net_cost,\n                label = print(paste0(\"$\", round(net_cost / 1000, 2), \"K\"))\n            ),\n            family = \"Oswald\",\n            size = 7,\n            show.legend = FALSE,\n            fill = \"white\"\n        ) +\n        scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Median Student Loan Debt for \", input$selectInput2),\n            subtitle = \"After Completing Their Undergraduate Degree\",\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n# PlOT4\nplot4 <- renderPlot({\n    ds4 %>%\n        filter(major_category == input$major_category) %>%\n        ggplot(aes(perfect_payback_period, reorder(major, perfect_payback_period), fill = perfect_payback_period)) +\n        geom_col(show.legend = FALSE) +\n        geom_label(aes(label = paste(round(perfect_payback_period, 2), \" yrs.\")),\n            show.legend = FALSE,\n            fill = \"white\",\n            hjust = 1.1\n        ) +\n        theme(\n            axis.title.y = element_blank(),\n            axis.text.x = element_blank()\n        ) +\n        labs(\n            title = \"How Long Will You Be In Debt?\",\n            subtitle = \"Based on Your Major\",\n            x = \"Time to pay off loans\"\n        ) +\n        CoreyPlotTheme +\n        theme(plot.title = element_text(hjust = 0.5)) +\n        scale_fill_continuous_sequential(\"PuBuGn\")\n})"
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#conclusion",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#conclusion",
    "title": "Resources for Prospective College Students",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the collaboration with Corey Cassell, our interactive tool provides valuable resources for high school students considering higher education. By offering comprehensive tools for estimating salaries, tuition costs, and student debt accumulation, we empower students to make informed decisions about their future. This project showcases the power of interactive visualizations in providing crucial information to prospective college students, guiding them towards successful career paths and financial planning."
  },
  {
    "objectID": "posts/CollegeStudent_Debt_Tool/index.html#data-references",
    "href": "posts/CollegeStudent_Debt_Tool/index.html#data-references",
    "title": "Resources for Prospective College Students",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#unveiling-natures-heatwave-the-hidden-impact-of-temperature-on-pesticide-toxicity",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#unveiling-natures-heatwave-the-hidden-impact-of-temperature-on-pesticide-toxicity",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Unveiling Nature’s Heatwave: The Hidden Impact of Temperature on Pesticide Toxicity",
    "text": "Unveiling Nature’s Heatwave: The Hidden Impact of Temperature on Pesticide Toxicity\nIn this captivating study, we embark on a quest to unravel the influence of temperature on the toxicity of pesticides, driven by growing concerns about the effects of climate change. Our mission extends beyond conventional pesticide testing, as we strive to shed light on the potential impacts of varying temperatures. We seek to answer a pivotal question: Does temperature significantly shape pesticide toxicity? To uncover the truth, we employ survival analysis, delving into the realm of time-to-death for the exposed subjects. By deciphering the intricate interplay between temperature and pesticide toxicity, we aim to reveal the magnitude of this effect and its implications for our changing world."
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#parasitic-marvels-unveiled-delving-into-the-intricate-world-of-echinostoma-trivolvis",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#parasitic-marvels-unveiled-delving-into-the-intricate-world-of-echinostoma-trivolvis",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Parasitic Marvels Unveiled: Delving into the Intricate World of Echinostoma trivolvis",
    "text": "Parasitic Marvels Unveiled: Delving into the Intricate World of Echinostoma trivolvis\nPrepare to be amazed as we take you on a captivating journey into the hidden intricacies of Echinostoma trivolvis, a remarkable species of trematode parasite. From its widespread infection of birds, mammals, and reptiles to its complex life cycle involving snails, birds, and mammals, this parasite holds many secrets waiting to be unlocked. Join us as we explore its adult worms’ intriguing residence in the small intestine, where they feed on blood and nutrients, causing a range of symptoms from abdominal pain to malnutrition. Found abundantly in North American wetlands and aquatic environments, Echinostoma trivolvis is a true marvel of nature that continues to fascinate and astonish researchers and nature enthusiasts alike."
  },
  {
    "objectID": "posts/SurvAnalysis_Parasite_RevealJS/index.html#presentation",
    "href": "posts/SurvAnalysis_Parasite_RevealJS/index.html#presentation",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#purpose",
    "href": "posts/Pokemon_Database/index.html#purpose",
    "title": "Pokédex Database",
    "section": "Purpose",
    "text": "Purpose\nThe main objective of this project was to construct a fully operational Postgresql database in a time frame of fewer than two weeks by employing the Extract, Transform, Load (ETL) methodology. The purpose of this approach was to extract data from various sources, transform it into a format that could be easily integrated into the database, and finally load the transformed data into the database.\nThe process involved several intricate steps, including identifying the relevant data sources, cleansing the extracted data to remove inconsistencies, standardizing the data to a uniform format, and applying data validation and verification techniques to ensure accuracy and completeness. Furthermore, it required careful consideration of the database schema, including the design of tables, relationships between tables, and the use of appropriate data types.\nThe successful implementation of this project was dependent on the utilization of cutting-edge technologies and tools, such as data integration software, data profiling tools, and scripting languages. The result was a functional database that can efficiently store and manage data, making it readily available for analysis, decision-making, and reporting purposes."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#summary",
    "href": "posts/Pokemon_Database/index.html#summary",
    "title": "Pokédex Database",
    "section": "Summary",
    "text": "Summary\nThe inquiry of identifying the optimal base stat Pokemon type sparked my interest, prompting me to delve into the realm of data engineering. In order to craft a well-informed response to this question, I began by utilizing the expansive and multifaceted “Pokémon of Kanto, Johto, and Hoenn Region” dataset to establish a structured and organized database."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#unleashing-the-power-of-postgresql-building-a-database",
    "href": "posts/Pokemon_Database/index.html#unleashing-the-power-of-postgresql-building-a-database",
    "title": "Pokédex Database",
    "section": "Unleashing the Power of PostgreSQL: Building a Database",
    "text": "Unleashing the Power of PostgreSQL: Building a Database\nThis SQL code creates several tables for storing Pokémon data. The tables include information about Pokémon, their types, abilities, generations, and moves. The code establishes primary keys, foreign key constraints, and defines the data types for each column. These tables form the foundation for a comprehensive Pokémon database, enabling efficient storage and retrieval of Pokémon-related information."
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#data-transformation-and-csv-preparation-in-sql-a-step-by-step-guide",
    "href": "posts/Pokemon_Database/index.html#data-transformation-and-csv-preparation-in-sql-a-step-by-step-guide",
    "title": "Pokédex Database",
    "section": "Data Transformation and CSV Preparation in SQL: A Step-by-Step Guide",
    "text": "Data Transformation and CSV Preparation in SQL: A Step-by-Step Guide\nThis section of the SQL file focuses on transforming and preparing a CSV file for analysis. It involves multiple SELECT statements that extract relevant data from different tables and join them together. The extracted data is then inserted into temporary tables, including ‘temp1’, ‘temp2’, and ‘temp3’, with each step refining the data further. Finally, the transformed data in ‘temp3’ is selected and filtered based on specific conditions, ordered, and then exported to a CSV file named ‘scuffed_pokedex.csv’. This process prepares the data for further analysis and exploration in external tools or applications.\n\n\nSELECT \n    identifier AS pokemon_name, \n    pokemon_types.type_id,\n    pokemon_abilities.ability_id\nINTO temp1\nFROM pokemon\nLEFT JOIN pokemon_types\nON pokemon.id = pokemon_types.pokemon_id\nLEFT JOIN pokemon_abilities\nON pokemon.id  = pokemon_abilities.pokemon_id;\n\n\nSELECT \n    pokemon_name,\n    types.identifier AS pokemon_type,\n    abilities.identifier AS pokemon_ability,\n    types.generation_id AS gen_id,\n    types.id AS type_id\nINTO temp2\nFROM temp1\nLEFT JOIN types\nON temp1.type_id = types.id\nLEFT JOIN abilities\nON temp1.ability_id = abilities.id;\n\n\nDROP TABLE temp3;\nSELECT \n    pokemon_name,\n    pokemon_type,\n    pokemon_ability,\n    generations.identifier AS pokemon_generation,\n    moves.identifier AS pokemon_move,\n    moves.power AS pokemon_power,\n    moves.accuracy AS pokemon_accuracy,\n    moves.pp AS pokemon_pp\nINTO temp3\nFROM temp2\nLEFT JOIN generations\nON temp2.gen_id = generations.main_region_id\nLEFT JOIN moves\nON temp2.type_id = moves.type_id;\n\nSELECT *\nFROM temp3\nWHERE pokemon_power IS NOT NULL \n    AND pokemon_accuracy IS NOT NULL\nORDER BY pokemon_accuracy, pokemon_power;\n\n\nCOPY temp3\nTO '/Users/Shared/Data_503/Datasets/scuffed_pokedex.csv'\nWITH (FORMAT CSV, HEADER);"
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#unveiling-pokémon-insights-analyzing-damage-output-and-accuracy",
    "href": "posts/Pokemon_Database/index.html#unveiling-pokémon-insights-analyzing-damage-output-and-accuracy",
    "title": "Pokédex Database",
    "section": "Unveiling Pokémon Insights: Analyzing Damage Output and Accuracy",
    "text": "Unveiling Pokémon Insights: Analyzing Damage Output and Accuracy\nThrough the power of data analysis and visualization, we have delved into the world of Pokémon to uncover insights about types, damage output, accuracy, and their relationships. Our exploration has shed light on the best Pokémon type for damage output, the most accurate contenders, and the interplay between power and accuracy. By combining the captivating nature of Pokémon with the analytical capabilities of R, we have gained valuable knowledge and set the stage for further investigations in the vast Pokémon universe.\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\n\npokemon <- read_csv(\"scuffed_pokedex.csv\")\n\nnames(pokemon)\n\nnb.cols <- 18\nmycolors <- colorRampPalette(brewer.pal(8, \"YlOrRd\"))(nb.cols)\n\npokemon %>% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %>%\n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_power, y = reorder(pokemon_type, avg_power), fill = reorder(pokemon_type, avg_power)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average power\",\n       y = \"Pokemon type\",\n       title = \"FIRE! The Best Pokemon Type For Damage Output Is...?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\nnb.cols <- 18\nmycolors <- colorRampPalette(brewer.pal(8, \"Blues\"))(nb.cols)\n\npokemon %>% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %>%\n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_accuracy, y = reorder(pokemon_type, avg_accuracy), fill = reorder(pokemon_type, avg_accuracy)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average accuracy\",\n       y = \"Pokemon type\",\n       title = \"Ouch! Who wins the bullseye competition?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\npokemon %>% \n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE)) %>%\n  ggplot(aes(x = avg_power, y = avg_accuarcy, color = pokemon_type)) +\n  geom_point()\n\n\nstats <- pokemon %>% \n  group_by(pokemon_type) %>%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE))\n            \nmodel <- lm(data = stats, avg_accuracy ~ avg_power)\nplot(model)"
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#presentation",
    "href": "posts/Pokemon_Database/index.html#presentation",
    "title": "Pokédex Database",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "posts/Pokemon_Database/index.html#data-references",
    "href": "posts/Pokemon_Database/index.html#data-references",
    "title": "Pokédex Database",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#abstract",
    "href": "posts/AmazonProductsEN/index.html#abstract",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Abstract",
    "text": "Abstract\nThis project focuses on the development of a Shiny application to analyze Amazon products in India. Using a dataset sourced from Kaggle, the application aims to create an informative dashboard showcasing categories with the highest average ratings and reviews. Despite encountering challenges with product categorization, the project prioritizes efficiency and design improvements."
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#introduction",
    "href": "posts/AmazonProductsEN/index.html#introduction",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Introduction",
    "text": "Introduction\nThe objective of this project is to create a Shiny application that explores the Amazon products dataset from India. The dataset, obtained from Kaggle, presented challenges during development, particularly regarding product categories. However, the project’s primary focus was on creating a concise and informative dashboard displaying categories with the highest average ratings and reviews.\nCompared to previous Shiny app development experiences, this project demonstrated improved efficiency, taking significantly less time to complete. Furthermore, advanced CSS theming was implemented to enhance the overall design of the application. While additional features and visualizations could be incorporated, the decision was made to leave them for future exploration."
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#questions",
    "href": "posts/AmazonProductsEN/index.html#questions",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average rating for each category and subcategory?\nWhat is the average review for each category and subcategory?\n\n\n\nFullscreen"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#load-libraries",
    "href": "posts/AmazonProductsEN/index.html#load-libraries",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\nlibrary(plotly)\nlibrary(DT)\noptions(scipen = 999)"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#part-1-data-wrangling-multiple-datasets",
    "href": "posts/AmazonProductsEN/index.html#part-1-data-wrangling-multiple-datasets",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 1: Data Wrangling Multiple Datasets",
    "text": "Part 1: Data Wrangling Multiple Datasets\n\n#ORIGINAL DATA WRANGLING \n\ndatasets <- as.data.frame(list.files(path = \"~/Documents/ShinyApps/AmazonProducts/AmazonProductApp\", pattern = \"csv\"))\ncolnames(datasets) <- \"Datasets\"\n\n#Combine all the datasets\nfor (i in length(nrow(datasets))){\n combinedDs <- read_csv(datasets[[i]])\n}\n\namazonProducts <- combinedDs %>%\n mutate(Name = name,\n       MainCategory = factor(str_to_title((sort(main_category)))),\n        SubCategory = factor(sort(sub_category)),\n        ProductImage = image,\n        ProductRating = as.numeric(ratings),\n        NumberOfRatings = as.numeric(gsub(\"\\\\,\",\"\",no_of_ratings)),\n        DiscountPrice = round(as.numeric(gsub(\"[\\\\₹,]\", \"\", discount_price)) / 81.85, 2), #convert from Rupee to USD\n        Price = round(as.numeric(gsub(\"[\\\\₹,]\", \"\", actual_price)) / 81.85, 2),           #convert from Rupee to USD\n        ProductLink = link) %>%\n select(-c(name, \n           main_category, \n           sub_category, \n           image, \n           ratings, \n           no_of_ratings, \n           discount_price, \n           actual_price,\n           link)) %>% \n drop_na() %>%\n filter(!str_detect(SubCategory, \"^All \"))\n\namazonProducts %>%\n write_csv(\"amazonProducts.csv\")\n\n\nReload Data\n\nproducts <- read_csv(\"amazonProducts.csv\")"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#part-2-data-wrangling-for-visualization",
    "href": "posts/AmazonProductsEN/index.html#part-2-data-wrangling-for-visualization",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 2: Data Wrangling For Visualization",
    "text": "Part 2: Data Wrangling For Visualization\n\nproducts <- products %>% mutate(DiscountPrice = round(DiscountPrice82.04,2), Price = round(Price82.04,2))\n\n#For Plot 1\nratings <- products %>% select(-c(ProductImage, Name, ProductLink)) %>% group_by(MainCategory,SubCategory) %>% summarise(AverageRating = mean(ProductRating)) %>% ungroup()\n\nreviews <- products %>% select(-c(ProductImage, Name, ProductLink)) %>% group_by(MainCategory,SubCategory) %>% summarise(AverageReview = mean(NumberOfRatings)) %>% ungroup()\n\n#For Plot 2 \ntop10Products <- products %>% filter(ProductRating > 4.5, NumberOfRatings > 50) %>% group_by(SubCategory) %>% arrange(desc(ProductRating)) %>% slice(1:10) %>% select(-c(ProductImage, Name, ProductLink))\n\n#unique(products$MainCategory)\n\n\nColor Theming\n\n#Plot 1 \nnum_colors <- 21 \ncolors <- c(\"#f2f2f2\", \"#ff9900\") \npal1 <- colorRampPalette(colors)(num_colors)\n\nprint(pal1)\n\n#Plot 2 \nnum_colors <- 21 \ncolors <- colors <- c(\"#f2f2f2\",\"#00a8e1\") \npal2 <- colorRampPalette(colors)(num_colors)"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#part-1-shiny-ui",
    "href": "posts/AmazonProductsEN/index.html#part-1-shiny-ui",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 1: Shiny UI",
    "text": "Part 1: Shiny UI\n\n# Define UI for application \nui <- fluidPage(\n  \n  #Background CSS\n  tags$head(tags$style(HTML('\n  @import url(https://fonts.googleapis.com/css?family=Montserrat&display=swap);\n    body {\n      font-family: Montserrat, sans-serif;\n      background-color: #FF9900;\n    }\n    .dataTables_wrapper {\n      background-color: #fff;\n    }\n    .sidebar {\n      background-color: #fff;\n      width: 3/12;\n      height: 2/12;\n    }\n    .nav-tabs > li > a {\n      color: black;\n      background-color: #00a8e1;\n      border-color: #00a8e1;\n    }'))),\n  \n  # Application title\n  titlePanel(\"Amazon Inc. Product Dashboard (EN.)\"),\n  \n  # Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"MainCategoryChoice\", \n                  label = h3(\"Select Category:\"), \n                  choices = unique(products$MainCategory), \n                  selected = \"Accessories\"),\n      \n      uiOutput(\"SubCategoryChoice\")\n    ),\n    \n    # Tabs\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Plot\", \n                 plotlyOutput(\"RatingsPlot\"),\n                 plotlyOutput(\"ReviewsPlot\")),\n        tabPanel(\"Data\", dataTableOutput(\"myDataTable\"))\n      )\n    )\n  )\n)"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#part-2-shiny-server",
    "href": "posts/AmazonProductsEN/index.html#part-2-shiny-server",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 2: Shiny Server",
    "text": "Part 2: Shiny Server\n\n# Define server logic required to draw Dashboard\nserver <- function(input, output) {\n  \n  # Subcategory choices\n  output$SubCategoryChoice <- renderUI({\n    subcategories <- unique(products$SubCategory[products$MainCategory == input$MainCategoryChoice])\n    checkboxGroupInput(\"SubCategoryChoice\", \n                       label = h3(\"Select Subcategories:\"), \n                       choices = subcategories, \n                       selected = subcategories)\n  })\n  \n  # Plot 1: Product Rating\n  output$RatingsPlot <- renderPlotly({\n    ratings %>%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %>% \n      mutate(SubCategory_ordered = factor(SubCategory, \n                                          levels = unique(SubCategory[order(AverageRating)]))) %>%\n      plot_ly(x = ~SubCategory_ordered, \n              y = ~round(AverageRating,2),\n              type = 'bar',\n              marker = list(color = ~pal1[SubCategory_ordered])) %>%\n      add_annotations(x = ~SubCategory_ordered,\n                      y = ~round(AverageRating,2),\n                      text = ~paste0(round(AverageRating,2)),\n                      font = list(color = 'black', \n                                  size = 10),\n                      showarrow = FALSE,\n                      yshift = 5) %>%\n      layout(title = paste0(\"Average Product Rating For \",input$MainCategoryChoice),\n             xaxis = list(title = \"\", tickangle = 45),\n             yaxis = list(title = \"\"),\n             showlegend = FALSE,\n             margin = list(t = 50,\n                           l = 50,\n                           r = 50,\n                           b = 50)) \n  })\n  \n  # Plot 2: Product Reviews\n  output$ReviewsPlot <- renderPlotly({\n    reviews %>%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %>% \n      mutate(SubCategory_ordered = factor(SubCategory, \n                                          levels = unique(SubCategory[order(AverageReview)]))) %>%\n      plot_ly(x = ~SubCategory_ordered, \n              y = ~round(AverageReview),\n              type = 'bar',\n              marker = list(color = ~pal2[SubCategory_ordered])) %>%\n      add_annotations(x = ~SubCategory_ordered,\n                      y = ~round(AverageReview),\n                      text = ~paste0(round(AverageReview)),\n                      font = list(color = 'black', \n                                  size = 10),\n                      showarrow = FALSE,\n                      yshift = 5) %>%\n      layout(title = paste0(\"Average Number of Reviews For \",input$MainCategoryChoice),\n             xaxis = list(title = \"\", tickangle = 45),\n             yaxis = list(title = \"\"),\n             showlegend = FALSE,\n             margin = list(t = 50,\n                           l = 50,\n                           r =50,\n                           b = 50)) \n  })\n  \n  output$myDataTable <- DT::renderDataTable({\n    products %>%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %>%\n      mutate(ProductImage = sprintf('<img src=\"%s\" width=\"75px\"/>', ProductImage)) %>%\n      DT::datatable(., escape = FALSE, options = list(\n        pageLength = 10,\n        lengthMenu = c(5, 10, 25),\n        scrollY = \"600px\",\n        scrollX = TRUE\n      )) %>%\n      DT::formatStyle(columns = colnames(products), \n                      backgroundColor = styleEqual(c(\"green\", \"white\"), c(\"rgb(51, 102, 0)\", \"rgb(255, 255, 255)\")))\n  })\n  \n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#further-improvement-questions",
    "href": "posts/AmazonProductsEN/index.html#further-improvement-questions",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Further Improvement Questions",
    "text": "Further Improvement Questions\n\nIs there a correlation between the number of ratings and the product rating?\nWhat is the average discount percentage for each main category and subcategory?\nWhat is the price range for each main category and subcategory?\nWhich products have the highest ratings and how do they compare in terms of price and number of ratings?"
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#conclusion",
    "href": "posts/AmazonProductsEN/index.html#conclusion",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Conclusion",
    "text": "Conclusion\nThis project served as a rewarding experience, providing opportunities to enhance data analysis and Shiny app development skills. The Shiny application developed enables users to analyze Amazon products in India and offers insights into categories with the highest average ratings and reviews. Despite encountering challenges related to product categorization, the project prioritized efficiency and introduced improvements in design. Future exploration of additional features and visualizations remains open for further development and enhancement."
  },
  {
    "objectID": "posts/AmazonProductsEN/index.html#data-references",
    "href": "posts/AmazonProductsEN/index.html#data-references",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#abstract",
    "href": "posts/AppleInc_IncomeStatement/index.html#abstract",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Abstract",
    "text": "Abstract\nThis project aims to utilize R’s plotly package to create dynamic and interactive visualizations of Time Series Graphs. The focus of the analysis is on the Apple stock prices from 1981 to the present, with a specific emphasis on identifying notable trends that have emerged over time. The study reveals a significant spike in the company’s stock prices following 2010, attributed to the success of Apple’s iPhone and related products. The research also highlights the impact of Apple’s marketing strategy in maintaining its market value and relevance to changing consumer needs. Overall, the project demonstrates the insightful use of R’s plotly for visualizing Time Series Graphs and providing meaningful explanations of stock market trends."
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#introduction",
    "href": "posts/AppleInc_IncomeStatement/index.html#introduction",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Introduction",
    "text": "Introduction\nVisualizing Time Series Graphs using R’s plotly package provides a powerful means to analyze and interpret complex data. In this project, the focus is on exploring the Apple stock prices over several decades and identifying key trends that have shaped the company’s market performance. The analysis uncovers a significant upsurge in Apple’s stock prices after 2010, which can be attributed to the tremendous success of the iPhone and Apple’s ability to stay in tune with evolving consumer demands.\nFurthermore, the research recognizes the role of Apple’s marketing strategy in maintaining the company’s market value and driving its growth. Apple’s innovative technology and ability to revolutionize the tech industry have contributed significantly to its economic impact. By employing R’s plotly package, this project aims to provide a more meaningful and interactive representation of the Time Series Graphs, enabling a comprehensive understanding of the trends in the stock market."
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#setup",
    "href": "posts/AppleInc_IncomeStatement/index.html#setup",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\n\n\nds <- read_csv(\"AppleInc_Stocks.csv\")\n#head(ds)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#apply-custom-theme",
    "href": "posts/AppleInc_IncomeStatement/index.html#apply-custom-theme",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Apply Custom Theme",
    "text": "Apply Custom Theme\n\nmyTheme <- function(){ \n    font <- \"SF Mono\"   #assign font family up front\n    \n    theme_minimal() %+replace%    #replace elements we want to change\n    \n    theme(\n      \n      #grid elements\n      panel.grid.major.x = element_blank(),    #strip major gridlines\n      panel.grid.minor = element_blank(),    #strip minor gridlines\n      axis.ticks = element_blank(),          #strip axis ticks\n      \n      #since theme_minimal() already strips axis lines, \n      #we don't need to do that again\n      \n      #text elements\n      plot.title = element_text(             #title\n                   family = font,            #set font family\n                   size = 16,                #set font size\n                   face = 'bold',            #bold typeface\n                   hjust = 0,                #left align\n                   vjust = 2),               #raise slightly\n      \n      plot.subtitle = element_text(          #subtitle\n                   family = font,            #font family\n                   size = 12),               #font size\n      \n      plot.caption = element_text(           #caption\n                   family = font,            #font family\n                   size = 9,                 #font size\n                   hjust = 1),               #right align\n      \n      axis.title = element_text(             #axis titles\n                   family = font,            #font family\n                   size = 10),               #font size\n      \n      axis.text = element_text(              #axis text\n                   family = font,            #axis famuly\n                   size = 9),                #font size\n      \n      axis.text.x = element_text(            #margin for axis text\n                    margin=margin(5, b = 10))\n      \n      #since the legend often requires manual tweaking \n      #based on plot content, don't define it here\n    )\n}"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#data-wrangling",
    "href": "posts/AppleInc_IncomeStatement/index.html#data-wrangling",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nyearlyDs <- ds %>% \n  drop_na() %>%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %>%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %>%\n  summarize(Open = mean(Open),\n            High = mean(High),\n            Low = mean(Low),\n            Close = mean(Close),\n            AdjClose = mean(`Adj Close`),\n            Volume = mean(Volume))\n\n\nlog_yearlyDs <- ds %>% \n  drop_na() %>%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %>%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %>%\n  summarize(Open = log(mean(Open)),\n            High = log(mean(High)),\n            Low = log(mean(Low)),\n            Close = log(mean(Close)),\n            AdjClose = log(mean(`Adj Close`)),\n            Volume = log(mean(Volume)))"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "href": "posts/AppleInc_IncomeStatement/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Times Series Plots of Apple Inc. Stock Prices",
    "text": "Times Series Plots of Apple Inc. Stock Prices\n\np <- yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price Since 1981\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  myTheme()\n\nggplotly(p) %>%\n  layout(hovermode = \"x unified\") %>% \n  style(hovertext = paste0(\" High: $\", round(yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: $\", round(yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: $\", round(yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#applying-log-norm",
    "href": "posts/AppleInc_IncomeStatement/index.html#applying-log-norm",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Applying Log-norm",
    "text": "Applying Log-norm\n\np2 <- log_yearlyDs %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price (Log-Normalized)\") +\n  myTheme()\n\nggplotly(p2, tooltip = \"text\") %>%\n  layout(hovermode = \"x unified\", \n         hovertext = paste0(\" Year: \", log_yearlyDs$Year)) %>% \n  style(hovertext = paste0(\" High: \", round(log_yearlyDs$High,2)),\n        traces = 1) %>%\n  style(hovertext = paste0(\" Low: \", round(log_yearlyDs$Low,2)),\n        traces = 2) %>%\n  style(hovertext = paste0(\" AdjClose: \", round(log_yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#conclusion",
    "href": "posts/AppleInc_IncomeStatement/index.html#conclusion",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Conclusion",
    "text": "Conclusion\nThe utilization of R’s plotly package for visualizing Time Series Graphs in the context of Apple’s stock prices has yielded valuable insights. The analysis revealed a substantial increase in Apple’s stock prices following 2010, fueled by the success of the iPhone and the company’s adept marketing strategy. The study emphasizes the transformative impact of Apple’s technology and its contributions to the tech industry and economic growth.\nBy employing dynamic and interactive visualizations, this project has enhanced the interpretation of Time Series Graphs, enabling a deeper understanding of the trends in the stock market. The use of R’s plotly package has proven to be a valuable tool in visual data exploration and storytelling. This undertaking serves as a testament to the power of R’s plotly in uncovering meaningful patterns and explaining the dynamics of stock market trends in a more engaging and informative manner."
  },
  {
    "objectID": "posts/AppleInc_IncomeStatement/index.html#data-references",
    "href": "posts/AppleInc_IncomeStatement/index.html#data-references",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#abstract",
    "href": "posts/GymRat_Plotly/index.html#abstract",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Abstract",
    "text": "Abstract\nThis study examines the role of stretching in muscle building and its implications for fitness enthusiasts. While stretching offers benefits in terms of flexibility and injury prevention, it falls short as a primary method for muscle growth. The focus of stretching on range of motion rather than muscle mass and strength limits its effectiveness. In isolation, stretching lacks the necessary resistance to stimulate muscle growth. Moreover, my analysis using R reveals that stretching ranks the lowest in terms of workout ratings. The findings highlight the importance of incorporating weight and resistance training as the primary approaches for muscle development."
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#introduction",
    "href": "posts/GymRat_Plotly/index.html#introduction",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Introduction",
    "text": "Introduction\nStretching enhances flexibility and prevents injuries, but its effectiveness in building muscle mass and strength is debated. This study explores the role of stretching in muscle building compared to other exercises. Stretching prioritizes flexibility over muscle development due to inadequate resistance and tension. It may not yield significant gains in muscle mass or strength. Excessive stretching can even reduce muscle activation and power output. Analysis of workout ratings using R shows stretching ranks lowest for muscle building. Weight and resistance training are emphasized as primary methods, with stretching as a supplementary activity for flexibility."
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#general-setup",
    "href": "posts/GymRat_Plotly/index.html#general-setup",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "General Setup",
    "text": "General Setup\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(extrafont)\nlibrary(plotly)\nlibrary(htmlwidgets)\n# font_import()\nloadfonts()"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#data-wrangling",
    "href": "posts/GymRat_Plotly/index.html#data-wrangling",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ngymDs <- read_csv(\"megaGymDataset.csv\")\n\n# head(gymDs, 5)\n# names(gymDs)\n\nds <- gymDs %>%\n    mutate(\n        ID = ...1,\n        Level = factor(Level, levels = c(\"Beginner\", \"Intermediate\", \"Expert\")),\n        Type = as.factor(Type)\n    ) %>%\n    select(-...1) %>%\n    drop_na()"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#unleashing-the-power-of-plotly",
    "href": "posts/GymRat_Plotly/index.html#unleashing-the-power-of-plotly",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Unleashing the Power of Plotly",
    "text": "Unleashing the Power of Plotly\n\nplotDs <- ds %>%\n    group_by(Type, Level) %>%\n    summarize(meanRating = mean(Rating)) %>%\n    arrange(Type, meanRating) %>%\n    ungroup()\n\np <- plotDs %>%\n    highlight_key(., ~ reorder_within(Type, meanRating, Level)) %>%\n    ggplot(aes(\n        x = meanRating,\n        y = reorder_within(Type, meanRating, Level),\n        fill = fct_reorder(Type, meanRating),\n        text = paste0(\n            \"Rating: \", round(meanRating, 2),\n            \"<br>Type: \", Type\n        )\n    )) +\n    geom_col(color = \"black\") +\n    facet_grid(\n        rows = vars(Level),\n        scales = \"free_y\",\n        switch = \"y\",\n        space = \"free_y\"\n    ) +\n    scale_y_reordered() +\n    scale_fill_brewer(palette = \"PuBuGn\") +\n    labs(\n        title = \"Ranking Exercise Type According to Experience Level of Individuals\",\n        x = \"Average Rating of Each Exercise Type\",\n        fill = \"Workout Types\"\n    ) +\n    theme_minimal() +\n    myTheme\n\n\nggplotly(p, tooltip = \"text\") %>%\n    config(displayModeBar = FALSE) %>%\n    highlight(on = \"plotly_hover\", off = \"plotly_doubleclick\") %>%\n    layout(\n        uniformtext = list(minsize = 8, mode = \"hide\"),\n        margin = list(b = 70, l = 140, r = 140)\n    )"
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#conclusion",
    "href": "posts/GymRat_Plotly/index.html#conclusion",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Conclusion",
    "text": "Conclusion\nWhile stretching remains valuable for improving flexibility and reducing the risk of injuries, it should be viewed as a supplementary activity rather than the main strategy for building muscle mass and strength. The incorporation of weight and resistance training, supported by the utilization of interactive plots using R, offers a more effective and comprehensive approach to achieving desired muscle development goals."
  },
  {
    "objectID": "posts/GymRat_Plotly/index.html#data-references",
    "href": "posts/GymRat_Plotly/index.html#data-references",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Data References",
    "text": "Data References"
  },
  {
    "objectID": "posts/PredictSeverityLevels/index.html#abstract",
    "href": "posts/PredictSeverityLevels/index.html#abstract",
    "title": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis On A Rare Disease",
    "section": "Abstract",
    "text": "Abstract\nThis paper presents a machine learning study on predicting severity levels in patients using a random forest approach based on symptom analysis. The objective was to develop accurate rules for classifying patients into three severity levels: Mild, Moderate, and Severe. A dataset containing patient symptoms was analyzed, and a random forest model was employed for classification. The top features were explored to derive meaningful rules. The results revealed that patients with 1 to 3 symptoms, including depression, were classified as Mild, while those with at least 3 symptoms, including depression and cramps (at least 2 symptoms), were classified as Moderate. Patients with 4 symptoms, including cramps and spasms (at least 2 symptoms), were classified as Severe. The random forest model achieved promising results, as confirmed by the confusion matrix. This study provides valuable insights into predicting severity levels in patients, facilitating personalized healthcare and treatment planning."
  },
  {
    "objectID": "posts/PredictSeverityLevels/index.html#introduction",
    "href": "posts/PredictSeverityLevels/index.html#introduction",
    "title": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis On A Rare Disease",
    "section": "Introduction",
    "text": "Introduction\nIn the field of rare diseases, physicians often observe variations in the severity levels of patients; however, there is a lack of established guidelines to map individual patients directly to specific severity levels. Our client, who is developing a product for this rare disease, aims to target patients with moderate to severe conditions. To address this challenge, the client possesses a comprehensive database containing clinically relevant information about the disease, including symptom flags and a symptom count variable. Additionally, each patient in the database has been rated by a physician as mild, moderate, or severe. The client has approached [REDACTED] with the objective of extracting the mental heuristics employed by physicians when assigning severity labels to patients. The task at hand involves deriving simple rules (1-3 per severity level) from the database that can effectively classify patients. For instance, a set of rules might involve identifying patients as severe if they exhibit fatigue or have exactly two symptoms. By extracting these rules, our aim is to provide the client with a clearer understanding of the factors influencing severity levels in order to enhance their product development and enable more personalized patient care."
  },
  {
    "objectID": "posts/PredictSeverityLevels/index.html#methodology",
    "href": "posts/PredictSeverityLevels/index.html#methodology",
    "title": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis On A Rare Disease",
    "section": "Methodology",
    "text": "Methodology\n\n# Import the required libraries\nimport pandas as pd                 # Library for data manipulation and analysis\nimport numpy as np                  # Library for numerical computations\nimport matplotlib.pyplot as plt     # Library for data visualization\nimport seaborn as sns\n\n# Import Machine Learning libraries\nfrom sklearn import tree            # Library for decision tree models\nfrom sklearn.ensemble import RandomForestClassifier  # Library for random forest models\nfrom sklearn.model_selection import (\n  cross_val_score,                   # Library for cross-validation\n  train_test_split,\n  GridSearchCV\n\n)  \nfrom sklearn.preprocessing import LabelEncoder      # Library for label encoding\nfrom sklearn.metrics import (                        # Library for model evaluation metrics\n    confusion_matrix,                                # Confusion matrix\n    roc_auc_score,                                   # ROC AUC score\n    roc_curve,                                       # ROC Curve plot\n    recall_score,                                    # Recall score\n    precision_score,                                 # Precision score\n    precision_recall_curve,                          # Recall vs. Precision Curve\n    cohen_kappa_score                                # Cohen's kappa score\n)\n\n# Load the dataset\nds = pd.read_csv(\"severityLevels.csv\")  \nds.head(10)  # Display the first 10 rows of the dataset\n\n\n\n\n\n  \n    \n      \n      Patient\n      Final Category\n      Fatigue\n      Weakness\n      Depression\n      Anxiety\n      Dry Skin\n      Spasms\n      Tingling\n      Headaches\n      Cramps\n      Number of Symptoms\n    \n  \n  \n    \n      0\n      1\n      Mild\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      1\n      2\n      Mild\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      2\n    \n    \n      2\n      3\n      Mild\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      4\n    \n    \n      3\n      4\n      Mild\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      5\n      Mild\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      5\n      6\n      Mild\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      6\n      7\n      Mild\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      7\n      8\n      Mild\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      2\n    \n    \n      8\n      9\n      Mild\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      2\n    \n    \n      9\n      10\n      Mild\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\nExploratory Data Analysis\n\n# Display basic statistics of the dataset\nds.describe()\n\n\n\n\n\n  \n    \n      \n      Patient\n      Fatigue\n      Weakness\n      Depression\n      Anxiety\n      Dry Skin\n      Spasms\n      Tingling\n      Headaches\n      Cramps\n      Number of Symptoms\n    \n  \n  \n    \n      count\n      300.000000\n      300.000000\n      300.000000\n      300.000000\n      300.000000\n      300.000000\n      300.000000\n      300.000000\n      300.000000\n      300.000000\n      300.000000\n    \n    \n      mean\n      150.500000\n      0.466667\n      0.396667\n      0.356667\n      0.363333\n      0.366667\n      0.200000\n      0.323333\n      0.333333\n      0.213333\n      3.020000\n    \n    \n      std\n      86.746758\n      0.499721\n      0.490023\n      0.479816\n      0.481763\n      0.482700\n      0.400668\n      0.468530\n      0.472192\n      0.410346\n      1.703704\n    \n    \n      min\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      75.750000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      2.000000\n    \n    \n      50%\n      150.500000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      3.000000\n    \n    \n      75%\n      225.250000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      0.000000\n      1.000000\n      1.000000\n      0.000000\n      4.000000\n    \n    \n      max\n      300.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      8.000000\n    \n  \n\n\n\n\n\n# Visualize the distribution of the target variable 'Final Category'\nplt.figure(figsize=(14, 8))\nds['Final Category'].value_counts().plot(kind='bar')\nplt.xlabel('Symptom Category')\nplt.ylabel('Number of Cases')\nplt.title('Distribution of Final Category')\nplt.xticks(rotation=0)  # Set rotation to 0 degrees for horizontal x-axis labels\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n# Calculate the correlation matrix\ncorr_matrix = ds.corr()\n\n# Visualize the correlation matrix\nplt.figure(figsize=(14, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.xticks(rotation=45)  # Set rotation to 0 degrees for horizontal x-axis labels\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\n\n\n\nFeature Engineering\nThe code performs some data preprocessing and feature selection to prepare the data for a Random Forest model. Let’s go through each step:\n\nDrop the ID column: The code removes a column named “Patient” from the dataset. This column likely contains unique identifiers for each patient, and since it’s not relevant for our analysis, we can safely remove it.\nCreate dummy columns for “Number of Symptoms”: The code converts a column called “Number of Symptoms” into multiple binary columns, known as dummy variables. Each dummy variable represents a unique value in the original column. This transformation helps us to use categorical data in our machine learning model.\nConcatenate the dummy columns with the original dataframe: The code combines the newly created dummy columns with the original dataset. This ensures that we retain all the existing information while incorporating the transformed categorical data.\nDrop the original “Number of Symptoms” column: Since we have created the dummy columns, we no longer need the original “Number of Symptoms” column. Therefore, the code removes this column from the dataset.\nSeparate the features (X) and the target variable (y): The code splits the dataset into two parts. The features, represented by the variable X, contain all the columns except the “Final Category” column, which is the target variable we want to predict. The target variable, represented by the variable y, contains only the “Final Category” column. This is the severity cases of ‘Mild’, ‘Moderate’ and ‘Severe’\nPerform feature selection using Random Forest: The code utilizes a machine learning algorithm called Random Forest to identify the most important features for each category in the target variable. It trains a separate Random Forest model for each category and determines the top three features that contribute the most to predicting that category.\nStore the top features for each category: The code stores the top three features for each category in a dictionary called “top_features.” Each category is represented by a label, and the corresponding top features are stored as a list.\nPrint the top 3 features for each label: The code then prints the top three features for each category in the target variable. This information helps us understand which features are most influential in determining the predicted category.\n\nOverall, this code prepares the data by transforming categorical data into a suitable format and identifies the top features that contribute to predicting different categories. This sets the stage for further analysis and building the final machine learning model based on these selected features.\n\n# Drop the ID column\nds.drop('Patient', axis=1, inplace=True)\n\n# Create dummy columns from the \"Number of Symptoms\" column\ndummy_cols = pd.get_dummies(ds['Number of Symptoms'], prefix='Symptom')\n\n# Concatenate the dummy columns with the original dataframe\nds = pd.concat([ds, dummy_cols], axis=1)\n\n# Drop the original \"Number of Symptoms\" column\nds.drop('Number of Symptoms', axis=1, inplace=True)\n\nds.head(10)\n\n\n\n\n\n  \n    \n      \n      Final Category\n      Fatigue\n      Weakness\n      Depression\n      Anxiety\n      Dry Skin\n      Spasms\n      Tingling\n      Headaches\n      Cramps\n      Symptom_0\n      Symptom_1\n      Symptom_2\n      Symptom_3\n      Symptom_4\n      Symptom_5\n      Symptom_6\n      Symptom_7\n      Symptom_8\n    \n  \n  \n    \n      0\n      Mild\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      Mild\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      Mild\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      Mild\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      Mild\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      5\n      Mild\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6\n      Mild\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      7\n      Mild\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      Mild\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      9\n      Mild\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n# Separate the features (X) and the target variable (y)\nX = ds.drop('Final Category', axis=1)\ny = ds['Final Category']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store top features per label\ntop_features = {}\n\n# Perform feature selection for each label\nfor label in y_train.unique():\n    # Encode the target variable for the current label\n    y_label_train = (y_train == label).astype(int)\n    y_label_test = (y_test == label).astype(int)\n    \n    # Create a Random Forest model\n    rf_model = RandomForestClassifier(random_state=60)\n    \n    # Train the model\n    rf_model.fit(X_train, y_label_train)\n    \n    # Get feature importances\n    feature_importances = rf_model.feature_importances_\n    \n    # Sort features by importance in descending order\n    sorted_features = sorted(zip(X_train.columns, feature_importances), key=lambda x: x[1], reverse=True)\n    \n    # Get the top 3 features for the current label\n    selected_features = [feature for feature, _ in sorted_features[:3]]\n    \n    # Store the top features for the current label\n    top_features[label] = selected_features\n\n# Print the top 3 features for each label\nfor label, features in top_features.items():\n    print(f\"Top 3 features for {label}:\")\n    print(features)\n    print()\n\nTop 3 features for Severe:\n['Cramps', 'Spasms', 'Symptom_4']\n\nTop 3 features for Mild:\n['Depression ', 'Symptom_1', 'Symptom_3']\n\nTop 3 features for Moderate:\n['Symptom_3', 'Depression ', 'Cramps']\n\n\n\n\nFeatures Explained\nThe feature selection process aims to identify the most important factors (features) that contribute to determining the severity of a patient’s condition. In this case, the severity levels are categorized as “Mild,” “Moderate,” and “Severe.” The top three features that were found to be most indicative for each severity level are as follows:\nFor patients rated as “Mild”:\n\nHas 1 symptom:\n\nThis feature indicates the presence of a specific symptom (let’s say, symptom X) that is associated with a mild severity rating. If a patient has symptom X, it suggests a higher likelihood of being classified as “Mild.”\n\nDepression:\n\nThis feature refers to the presence or absence of depression symptoms in the patient. The presence of depression symptoms is considered important in determining a mild severity rating.\n\nHas 3 symptoms:\n\nSimilar to the previous feature, this feature represents the presence of another symptom (let’s call it symptom Y) that is associated with a mild severity rating. If a patient has symptom Y, it suggests a higher likelihood of being classified as “Mild.”\nFor patients rated as “Moderate”:\n\nHas 3 symptoms:\n\nThis feature indicates the presence of symptom Y, which is associated with a moderate severity rating. If a patient has symptom Y, it suggests a higher likelihood of being classified as “Moderate.”\n\nDepression:\n\nThe presence or absence of depression symptoms also plays a role in determining a moderate severity rating.\n\nCramps:\n\nThis feature represents the presence or absence of cramps in the patient. The presence of cramps is considered important in predicting a moderate severity rating.\nFor patients rated as “Severe”:\n\nCramps:\n\nThis feature indicates the presence or absence of cramps, which is associated with a severe severity rating. If a patient has cramps, it suggests a higher likelihood of being classified as “Severe.”\n\nSpasms:\n\nThis feature refers to the presence or absence of muscle spasms in the patient. The presence of spasms is considered important in predicting a severe severity rating.\n\nHas 4 symptoms:\n\nThis feature represents the presence of another symptom (let’s call it symptom Z) that is associated with a severe severity rating. If a patient has symptom Z, it suggests a higher likelihood of being classified as “Severe.”\nIn summary, the top features identified for each severity level provide insights into the specific symptoms and factors that contribute to determining the severity of a patient’s condition. By considering the presence or absence of these features, the model can make predictions about the severity rating of a patient’s condition, helping healthcare professionals assess the level of severity and provide appropriate care and treatment.\n\n\n\nRandom Forest Model\nThe code performs machine learning tasks using a Random Forest model with the selected features from the earlier model. Let’s go through each step:\n\nSeparate the features (X) and the target variable (y) using only the top features: The code selects specific features from the dataset based on their importance in predicting the target variable. These features are obtained from the “top_features” dictionary, which contains the top three features for each category (Mild, Moderate, and Severe) in the target variable.\nEncode the target variable using label encoding: The target variable “Final Category” is a categorical variable. To use it in the machine learning model, we need to convert it into numeric form. The code uses label encoding, which assigns a unique numeric value to each category in the target variable.\nCreate a random forest model using the top features: The code initializes a Random Forest model with a specific random state. Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions.\nPerform 10-fold cross-validation: The code evaluates the performance of the Random Forest model using a technique called cross-validation. Cross-validation helps estimate how well the model will generalize to new, unseen data. In this case, 10-fold cross-validation is performed, which means the dataset is divided into 10 equal parts (folds). The model is trained and tested 10 times, with each fold serving as the test set once.\nPrint the cross-validation scores: The code prints the cross-validation scores obtained from each fold. These scores indicate how well the model performed on each fold. Additionally, the mean cross-validation score is calculated, which provides an overall measure of the model’s performance.\nTrain the model: The code trains the Random Forest model using all the available data, as specified by the features (X) and target variable (y).\nMake predictions on the training set: The code uses the trained model to make predictions on the same dataset that was used for training. This helps evaluate how well the model can predict the target variable for the given features.\nPrint the confusion matrix: The code prints a confusion matrix, which is a table that shows the number of correct and incorrect predictions made by the model. It provides insights into the model’s performance for each category in the target variable.\nCalculate and print other evaluation metrics: The code calculates additional evaluation metrics such as ROC AUC score, recall, precision, and Kappa metric. These metrics help assess the model’s performance in terms of classification accuracy, sensitivity, precision, and agreement beyond chance.\n\nOverall, this code builds a Random Forest model using selected features, evaluates its performance through cross-validation, and provides insights into the model’s predictive capabilities using various evaluation metrics. The goal is to understand how well the model can predict the categories in the target variable based on the selected features.\n\n# Separate the features (X) and the target variable (y) using only the top features\nX = ds[top_features['Mild'] + top_features['Moderate'] + top_features['Severe']]\ny = ds['Final Category']\n\n# Encode the target variable using label encoding\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Create a random forest model\nrf_model = RandomForestClassifier(random_state=60)\n\n# Define the parameter grid for hyperparameter optimization\nparam_grid = {\n    'max_depth': [None],\n    'min_samples_leaf': [4],\n    'min_samples_split': [2],\n    'n_estimators': [100]\n}\n\n# Perform hyperparameter optimization using grid search and 10-fold cross-validation\ngrid_search = GridSearchCV(rf_model, param_grid, cv=10)\ngrid_search.fit(X, y)\n\n# Get the best random forest model with optimized hyperparameters\nrf_model = grid_search.best_estimator_\n\n# Perform 10-fold cross-validation with the optimized model\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Print the cross-validation scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())\nprint()\n\nCross-Validation Scores: [0.86666667 0.9        1.         0.9        0.96666667 0.9\n 0.9        0.83333333 0.86666667 0.8       ]\nMean CV Score: 0.8933333333333333\n\n\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=87)\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\n\n# Calculate and print ROC AUC score\nroc_auc = roc_auc_score(y_test, rf_model.predict_proba(X_test), multi_class='ovr')\nprint(\"ROC AUC:\", roc_auc)\nprint()\n\n# Calculate and print recall score\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint(\"Recall:\", recall)\nprint()\n\n# Calculate and print precision score\nprecision = precision_score(y_test, y_pred, average='weighted')\nprint(\"Precision:\", precision)\nprint()\n\n# Calculate and print Kappa metric\nkappa = cohen_kappa_score(y_test, y_pred)\nprint(\"Kappa:\", kappa)\nprint()\n\nConfusion Matrix:\n[[18  4  0]\n [ 0 20  1]\n [ 0  1 16]]\n\nROC AUC: 0.9480560359313329\n\nRecall: 0.9\n\nPrecision: 0.9133333333333333\n\nKappa: 0.8493723849372385\n\n\n\n\n\nMetrics Explained\nLet’s break down the provided metrics based on the given confusion matrix:\n\nConfusion Matrix\nThe confusion matrix is a table that helps us understand the performance of a classification model. It shows the predicted labels versus the actual labels for each class. In this case, the confusion matrix has three rows and three columns, representing the three severity rating categories. In this case, since I set the test set to be 20% of the sample of 300, there is 60 total patients that were randomly tested.\n\n\\begin{array}{c|ccc}\n& {\\text{Actual}} \\\\\n\\text{Predicted} & \\text{Mild} & \\text{Moderate} & \\text{Severe} \\\\\n\\hline\n\\text{Mild} & 18 & 4 & 0 \\\\\n\\text{Moderate} & 0 & 20 & 1 \\\\\n\\text{Severe} & 0 & 1 & 16 \\\\\n\\end{array}\n\nFirst, the number 18 in the first row and column indicates that the model correctly predicted 18 patients as “mild” which coincides with the actual “mild” severity label. Next, the number 4 in the first row and second column indicates that the model incorrectly predicted 4 patients as “moderate” when their actual severity rating was “mild.” Lastly, the number 0 in the first row and third column indicates that the model correctly predicted 0 patients as being “severe”, meaning that those patients were labeled correctly as “mild”.\nSimilarly, the other numbers in the confusion matrix represent the model’s predictions for the other severity rating categories. Given that this was a dataset with only 300 patients, it is very intriguing that it can label each patient with high accuarcy.\n\n\nROC Curve\nNow, the ROC AUC (Receiver Operating Characteristic Area Under the Curve) is a measure of the model’s ability to distinguish between different severity ratings. It represents the overall performance of the model across all severity levels. The value of 0.948 indicates a high level of performance, close to 1, suggesting that the model has good predictive capability for distinguishing between severity ratings.\n\n# Calculate predicted probabilities for each class\ny_pred_proba = rf_model.predict_proba(X_test)\n\n# Compute the ROC curve for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor class_index in range(len(label_encoder.classes_)):\n    fpr[class_index], tpr[class_index], _ = roc_curve(\n        (y_test == class_index).astype(int), y_pred_proba[:, class_index]\n    )\n    roc_auc[class_index] = roc_auc_score(\n        (y_test == class_index).astype(int), y_pred_proba[:, class_index]\n    )\n\n# Plot the ROC curve for each class\nplt.figure(figsize=(8, 6))\nfor class_index in range(len(label_encoder.classes_)):\n    plt.plot(\n        fpr[class_index],\n        tpr[class_index],\n        label=f\"Class {label_encoder.classes_[class_index]} (AUC = {roc_auc[class_index]:.2f})\",\n    )\n\n# Plot random guessing line\nplt.plot([0, 1], [0, 1], \"k--\")\n\n# Set plot properties\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\n\n# Show the plot\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\nRecall vs. Precision Curve\nRecall, also known as sensitivity or true positive rate, measures the proportion of actual positive instances (patients with a particular severity rating) that the model correctly identified. A recall value of 0.9 means that the model identified nearly 90.0% of the patients with their correct severity rating.\nPrecision measures the proportion of instances that the model predicted correctly as positive (patients with a particular severity rating) out of all instances it predicted as positive. A precision value of 0.913 indicates that out of all the patients the model identified as having a specific severity rating, 91.3% of them were correct.\n\n# Calculate precision and recall values for each class\nprecision, recall, thresholds = precision_recall_curve(y_test, rf_model.predict_proba(X_test)[:, 1], pos_label=1)\n\n# Plot the recall vs. precision curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Recall vs. Precision Curve')\nplt.grid(True)\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\nKappa\nThe Kappa statistic is a measure of agreement between the model’s predictions and the actual severity ratings, taking into account the possibility of agreement occurring by chance. A Kappa value of 0.849 indicates a substantial level of agreement between the model’s predictions and the actual severity ratings, suggesting a reliable performance of the model.\nOverall, these metrics indicate that the model has performed well in predicting the severity ratings of the patients, with high accuracy, good distinction between severity levels, and substantial agreement with the actual severity ratings provided by physicians.\n\n\n\nVisualizing Random Forest’s Best Decision Tree\n\n# Perform 10-fold cross-validation\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Find the index of the best decision tree\nbest_tree_index = np.argmax(cv_scores)\n\n# Get the best decision tree from the random forest\nbest_tree = rf_model.estimators_[best_tree_index]\n\n# Visualize the best decision tree using matplotlib\nplt.figure(figsize=(15, 14))\ntree.plot_tree(best_tree, feature_names=X.columns, class_names=label_encoder.classes_, filled=True)\n\nplt.show()"
  },
  {
    "objectID": "posts/PredictSeverityLevels/index.html#results",
    "href": "posts/PredictSeverityLevels/index.html#results",
    "title": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis On A Rare Disease",
    "section": "Results",
    "text": "Results\n\nIf a patient has between 1 to 3 symptoms, and one of those symptoms includes depression, they are classified as ‘Mild’.\nIf a patient has at least 3 symptoms, and two of those symptoms are depression and cramps, they are classified as ‘Moderate’.\nLastly, if a patient has 4 symptoms, and two of those symptoms are cramps and spasms, they are classified as ‘Severe’."
  },
  {
    "objectID": "posts/PredictSeverityLevels/index.html#conclusion",
    "href": "posts/PredictSeverityLevels/index.html#conclusion",
    "title": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis On A Rare Disease",
    "section": "Conclusion",
    "text": "Conclusion\nThis study focused on predicting severity levels in patients with a rare disease using a random forest approach based on symptom analysis. The client’s goal was to map individual patients to appropriate severity levels, given the absence of established guidelines. By leveraging a comprehensive database containing clinically relevant information and severity ratings provided by physicians, we extracted simple rules to classify patients.\nThe study revealed that patients with 1 to 3 symptoms, including depression, were classified as ‘Mild’. For patients with at least 3 symptoms, the presence of depression and cramps (at least 2 symptoms) indicated a classification of ‘Moderate’. Lastly, patients presenting with 4 symptoms, including cramps and spasms (at least 2 symptoms), were categorized as ‘Severe’.\nThe derived rules provide valuable insights into the mental heuristics employed by physicians when assessing severity levels in patients. By incorporating these rules into the client’s product development, personalized healthcare targeting patients with moderate to severe disease can be enhanced. Furthermore, these findings contribute to filling the existing gap in severity level guidelines for this rare disease."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Brian Cervantes Alvarez",
    "section": "",
    "text": "© Copyright Brian Adolfo Cervantes Alvarez\nThis website represents my personal views and opinions and does not reflect the endorsement of my employer or any affiliated organizations. The content available on this site is licensed under the [Creative Commons (CC-BY) 4.0 license]. You are welcome to reuse the content, attributing it to me as the author and providing a link to the original material. The source code of this website is released under the [MIT license] and can be reused without any restrictions."
  },
  {
    "objectID": "posts/PredictSeverityLevels/index.html#data-exploration",
    "href": "posts/PredictSeverityLevels/index.html#data-exploration",
    "title": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n# Perform exploratory data analysis (EDA)\n# Display basic statistics of the dataset\nprint(ds.describe())\n\n# Check the number of unique values in each column\nprint(ds.nunique())\n\n# Check the distribution of the target variable 'Final Category'\nprint(ds['Final Category'].value_counts())\n\n# Visualize the distribution of the target variable 'Final Category'\nplt.figure(figsize=(22, 8))\nds['Final Category'].value_counts().plot(kind='bar')\nplt.xlabel('Final Category')\nplt.ylabel('Count')\nplt.title('Distribution of Final Category')\nplt.show()\n\n# Calculate the correlation matrix\ncorr_matrix = ds.corr()\n\n# Visualize the correlation matrix\nplt.figure(figsize=(24, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n          Fatigue    Weakness  Depression      Anxiety    Dry Skin  \\\ncount  300.000000  300.000000   300.000000  300.000000  300.000000   \nmean     0.466667    0.396667     0.356667    0.363333    0.366667   \nstd      0.499721    0.490023     0.479816    0.481763    0.482700   \nmin      0.000000    0.000000     0.000000    0.000000    0.000000   \n25%      0.000000    0.000000     0.000000    0.000000    0.000000   \n50%      0.000000    0.000000     0.000000    0.000000    0.000000   \n75%      1.000000    1.000000     1.000000    1.000000    1.000000   \nmax      1.000000    1.000000     1.000000    1.000000    1.000000   \n\n           Spasms    Tingling   Headaches      Cramps   Symptom_0   Symptom_1  \\\ncount  300.000000  300.000000  300.000000  300.000000  300.000000  300.000000   \nmean     0.200000    0.323333    0.333333    0.213333    0.066667    0.150000   \nstd      0.400668    0.468530    0.472192    0.410346    0.249861    0.357668   \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n75%      0.000000    1.000000    1.000000    0.000000    0.000000    0.000000   \nmax      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n\n        Symptom_2   Symptom_3   Symptom_4   Symptom_5   Symptom_6   Symptom_7  \\\ncount  300.000000  300.000000  300.000000  300.000000  300.000000  300.000000   \nmean     0.126667    0.306667    0.173333    0.093333    0.053333    0.023333   \nstd      0.333155    0.461880    0.379168    0.291385    0.225073    0.151212   \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n75%      0.000000    1.000000    0.000000    0.000000    0.000000    0.000000   \nmax      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n\n        Symptom_8  \ncount  300.000000  \nmean     0.006667  \nstd      0.081513  \nmin      0.000000  \n25%      0.000000  \n50%      0.000000  \n75%      0.000000  \nmax      1.000000  \nFinal Category    3\nFatigue           2\nWeakness          2\nDepression        2\nAnxiety           2\nDry Skin          2\nSpasms            2\nTingling          2\nHeadaches         2\nCramps            2\nSymptom_0         2\nSymptom_1         2\nSymptom_2         2\nSymptom_3         2\nSymptom_4         2\nSymptom_5         2\nSymptom_6         2\nSymptom_7         2\nSymptom_8         2\ndtype: int64\nMild        100\nModerate    100\nSevere      100\nName: Final Category, dtype: int64\n\n\n\n\n\n\n\n\n\n# Separate the features (X) and the target variable (y)\nX = ds.drop('Final Category', axis=1)\ny = ds['Final Category']\n\n# Initialize a dictionary to store top features per label\ntop_features = {}\n\n# Perform feature selection for each label\nfor label in y.unique():\n    # Encode the target variable for the current label\n    y_label = (y == label).astype(int)\n    \n    # Create a Random Forest model\n    rf_model = RandomForestClassifier(random_state=60)\n    \n    # Train the model\n    rf_model.fit(X, y_label)\n    \n    # Get feature importances\n    feature_importances = rf_model.feature_importances_\n    \n    # Sort features by importance in descending order\n    sorted_features = sorted(zip(X.columns, feature_importances), key=lambda x: x[1], reverse=True)\n    \n    # Get the top 3 features for the current label\n    selected_features = [feature for feature, _ in sorted_features[:3]]\n    \n    # Store the top features for the current label\n    top_features[label] = selected_features\n\n# Print the top 3 features for each label\nfor label, features in top_features.items():\n    print(f\"Top 3 features for {label}:\")\n    print(features)\n    print()\n\nTop 3 features for Mild:\n['Symptom_1', 'Depression ', 'Symptom_3']\n\nTop 3 features for Moderate:\n['Symptom_3', 'Depression ', 'Cramps']\n\nTop 3 features for Severe:\n['Cramps', 'Spasms', 'Symptom_4']\n\n\n\n\nFeatures Explained\nThe feature selection process aims to identify the most important factors (features) that contribute to determining the severity of a patient’s condition. In this case, the severity levels are categorized as “Mild,” “Moderate,” and “Severe.” The top three features that were found to be most indicative for each severity level are as follows:\nFor patients rated as “Mild”: 1. Has 1 symptom: This feature indicates the presence of a specific symptom (let’s say, symptom X) that is associated with a mild severity rating. If a patient has symptom X, it suggests a higher likelihood of being classified as “Mild.” 2. Depression: This feature refers to the presence or absence of depression symptoms in the patient. The presence of depression symptoms is considered important in determining a mild severity rating. 3. Has 3 symptoms: Similar to the previous feature, this feature represents the presence of another symptom (let’s call it symptom Y) that is associated with a mild severity rating. If a patient has symptom Y, it suggests a higher likelihood of being classified as “Mild.”\nFor patients rated as “Moderate”: 1. Has 3 symptoms: This feature indicates the presence of symptom Y, which is associated with a moderate severity rating. If a patient has symptom Y, it suggests a higher likelihood of being classified as “Moderate.” 2. Depression: The presence or absence of depression symptoms also plays a role in determining a moderate severity rating. 3. Cramps: This feature represents the presence or absence of cramps in the patient. The presence of cramps is considered important in predicting a moderate severity rating.\nFor patients rated as “Severe”: 1. Cramps: This feature indicates the presence or absence of cramps, which is associated with a severe severity rating. If a patient has cramps, it suggests a higher likelihood of being classified as “Severe.” 2. Spasms: This feature refers to the presence or absence of muscle spasms in the patient. The presence of spasms is considered important in predicting a severe severity rating. 3. Has 4 symptoms: This feature represents the presence of another symptom (let’s call it symptom Z) that is associated with a severe severity rating. If a patient has symptom Z, it suggests a higher likelihood of being classified as “Severe.”\nIn summary, the top features identified for each severity level provide insights into the specific symptoms and factors that contribute to determining the severity of a patient’s condition. By considering the presence or absence of these features, the model can make predictions about the severity rating of a patient’s condition, helping healthcare professionals assess the level of severity and provide appropriate care and treatment.\n\n\nRandom Forest Model\nThe code performs machine learning tasks using a Random Forest model with selected features. Let’s go through each step:\n\nSeparate the features (X) and the target variable (y) using only the top features: The code selects specific features from the dataset based on their importance in predicting the target variable. These features are obtained from the “top_features” dictionary, which contains the top three features for each category (Mild, Moderate, and Severe) in the target variable.\nEncode the target variable using label encoding: The target variable “Final Category” is a categorical variable. To use it in the machine learning model, we need to convert it into numeric form. The code uses label encoding, which assigns a unique numeric value to each category in the target variable.\nCreate a random forest model using the top features: The code initializes a Random Forest model with a specific random state. Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions.\nPerform 10-fold cross-validation: The code evaluates the performance of the Random Forest model using a technique called cross-validation. Cross-validation helps estimate how well the model will generalize to new, unseen data. In this case, 10-fold cross-validation is performed, which means the dataset is divided into 10 equal parts (folds). The model is trained and tested 10 times, with each fold serving as the test set once.\nPrint the cross-validation scores: The code prints the cross-validation scores obtained from each fold. These scores indicate how well the model performed on each fold. Additionally, the mean cross-validation score is calculated, which provides an overall measure of the model’s performance.\nTrain the model: The code trains the Random Forest model using all the available data, as specified by the features (X) and target variable (y).\nMake predictions on the training set: The code uses the trained model to make predictions on the same dataset that was used for training. This helps evaluate how well the model can predict the target variable for the given features.\nPrint the confusion matrix: The code prints a confusion matrix, which is a table that shows the number of correct and incorrect predictions made by the model. It provides insights into the model’s performance for each category in the target variable.\nCalculate and print other evaluation metrics: The code calculates additional evaluation metrics such as ROC AUC score, recall, precision, and Kappa metric. These metrics help assess the model’s performance in terms of classification accuracy, sensitivity, precision, and agreement beyond chance.\n\nOverall, this code builds a Random Forest model using selected features, evaluates its performance through cross-validation, and provides insights into the model’s predictive capabilities using various evaluation metrics. The goal is to understand how well the model can predict the categories in the target variable based on the selected features.\n\n# Separate the features (X) and the target variable (y) using only the top features\nX = ds[top_features['Mild'] + top_features['Moderate'] + top_features['Severe']]\ny = ds['Final Category']\n\n# Encode the target variable using label encoding\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Create a random forest model using the top features\nrf_model = RandomForestClassifier(random_state=60)\n\n# Perform 10-fold cross-validation\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Print the cross-validation scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())\nprint()\n\n# Train the model\nrf_model.fit(X, y)\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X)\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y, y_pred))\nprint()\n\n# Calculate and print ROC AUC score\nroc_auc = roc_auc_score(y, rf_model.predict_proba(X), multi_class='ovr')\nprint(\"ROC AUC:\", roc_auc)\nprint()\n\n# Calculate and print recall score\nrecall = recall_score(y, y_pred, average='weighted')\nprint(\"Recall:\", recall)\nprint()\n\n# Calculate and print precision score\nprecision = precision_score(y, y_pred, average='weighted')\nprint(\"Precision:\", precision)\nprint()\n\n# Calculate and print Kappa metric\nkappa = cohen_kappa_score(y, y_pred)\nprint(\"Kappa:\", kappa)\nprint()\n\nCross-Validation Scores: [0.86666667 0.9        1.         0.9        0.96666667 0.9\n 0.86666667 0.8        0.86666667 0.8       ]\nMean CV Score: 0.8866666666666667\n\nConfusion Matrix:\n[[84 13  3]\n [ 1 94  5]\n [ 0  8 92]]\n\nROC AUC: 0.9712833333333334\n\nRecall: 0.9\n\nPrecision: 0.908542199488491\n\nKappa: 0.85\n\n\n\n\nMetrics Explained\nLet’s break down the provided metrics based on the given confusion matrix:\nConfusion Matrix: The confusion matrix is a table that helps us understand the performance of a classification model. It shows the predicted labels versus the actual labels for each class. In this case, the confusion matrix has three rows and three columns, representing the three severity rating categories.\n\n\\begin{array}{c|ccc}\n& {\\text{Actual}} \\\\\n\\text{Predicted} & \\text{Mild} & \\text{Moderate} & \\text{Severe} \\\\\n\\hline\n\\text{Mild} & 84 & 13 & 3 \\\\\n\\text{Moderate} & 1 & 94 & 5 \\\\\n\\text{Severe} & 0 & 8 & 92 \\\\\n\\end{array}\n\nThe number 84 in the first row and first column indicates that the model correctly predicted 84 patients as “mild” when their actual severity rating was also “mild.” The number 13 in the first row and second column indicates that the model incorrectly predicted 13 patients as “moderate” when their actual severity rating was “mild.” The number 3 in the first row and third column indicates that the model incorrectly predicted 3 patients as “severe” when their actual severity rating was “mild.”\nSimilarly, the other numbers in the matrix represent the model’s predictions for the other severity rating categories. ROC AUC (Receiver Operating Characteristic Area Under the Curve): ROC AUC is a measure of the model’s ability to distinguish between different severity ratings. It represents the overall performance of the model across all severity levels. The value of 0.971 indicates a high level of performance, close to 1, suggesting that the model has good predictive capability for distinguishing between severity ratings.\nRecall, also known as sensitivity or true positive rate, measures the proportion of actual positive instances (patients with a particular severity rating) that the model correctly identified. A recall value of 0.9 means that the model identified 90% of the patients with their correct severity rating.\nPrecision measures the proportion of instances that the model predicted correctly as positive (patients with a particular severity rating) out of all instances it predicted as positive. A precision value of 0.909 indicates that out of all the patients the model identified as having a specific severity rating, 90.9% of them were correct.\nKappa: The Kappa statistic is a measure of agreement between the model’s predictions and the actual severity ratings, taking into account the possibility of agreement occurring by chance. A Kappa value of 0.85 indicates a substantial level of agreement between the model’s predictions and the actual severity ratings, suggesting a reliable performance of the model.\nOverall, these metrics indicate that the model has performed well in predicting the severity ratings of the patients, with high accuracy, good distinction between severity levels, and substantial agreement with the actual severity ratings provided by physicians.\n\n\n\nVisualizing Random Forest’s Best Decision Tree\n\n# Perform 10-fold cross-validation\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Find the index of the best decision tree\nbest_tree_index = np.argmax(cv_scores)\n\n# Get the best decision tree from the random forest\nbest_tree = rf_model.estimators_[best_tree_index]\n\n# Visualize the best decision tree using matplotlib\nplt.figure(figsize=(20, 15))\ntree.plot_tree(best_tree, feature_names=X.columns, class_names=label_encoder.classes_, filled=True)\n\nplt.show()"
  },
  {
    "objectID": "posts/PredictSeverityLevels/index.html#further-research",
    "href": "posts/PredictSeverityLevels/index.html#further-research",
    "title": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis On A Rare Disease",
    "section": "Further Research",
    "text": "Further Research\nIt is recommended to validate and refine these rules using larger datasets. By expanding the dataset, researchers can ensure the generalizability of the derived rules and improve the accuracy of the classification. Additionally, exploring additional factors that may influence severity levels, such as demographic information, medical history, or genetic markers, can provide a more comprehensive understanding of the disease and enhance the prediction models.\nBy continuing to refine and validate the rules and incorporating more factors into the analysis, personalized treatment for patients with this rare disease can be further optimized, leading to improved patient outcomes. The combination of symptom analysis and machine learning approaches holds significant potential for facilitating accurate classification and personalized healthcare in various medical domains."
  },
  {
    "objectID": "posts/PredictSeverityLevels/index.html#references",
    "href": "posts/PredictSeverityLevels/index.html#references",
    "title": "Predicting Severity Levels in Patients: A Random Forest Approach Based on Symptom Analysis On A Rare Disease",
    "section": "References",
    "text": "References\nPandas: McKinney, W. (2010). Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference, 445-451. [Link: https://conference.scipy.org/proceedings/scipy2010/mckinney.html]\nNumPy: Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357-362. [Link: https://www.nature.com/articles/s41586-020-2649-2]\nMatplotlib: Hunter, J. D. (2007). Matplotlib: A 2D Graphics Environment. Computing in Science & Engineering, 9(3), 90-95. [Link: https://ieeexplore.ieee.org/document/4160265]\nSeaborn: Waskom, M., Botvinnik, O., Hobson, P., … Halchenko, Y. (2021). mwaskom/seaborn: v0.11.1 (February 2021). Zenodo. [Link: https://doi.org/10.5281/zenodo.4473861]\nScikit-learn: Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., … Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830. [Link: https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html]"
  }
]