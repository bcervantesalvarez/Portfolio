[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n  \n    \n    Brian Cervantes Alvarez\n    Statistician & Data Scientist • R/Shiny • Python • SQL • Educator\n\n    \n      I design explainable ML and decision-ready analytics, ship interactive tools teams actually use,\n      and teach the concepts clearly enough that adoption sticks.\n    \n\n    \n      Hire Me →\n      \n        Download Resume\n      \n    \n\n    \n      Featured Projects\n      •\n      LinkedIn\n      •\n      Skills\n    \n  \n\n\n\n\n  How I Deliver Results\n  Clear problem framing, rigorous validation, and production-minded delivery.\n  \n\n    \n    \n      \n      Decision-Focused Analytics\n      \n        Translate goals into crisp KPIs and hypotheses; design analyses that answer “What should we do next?” not just “What happened?”.\n      \n    \n\n    \n    \n      \n      Reliable Data Plumbing\n      \n        Build auditable R/Python/SQL pipelines with documented assumptions, reproducible runs, and failure-aware checks.\n      \n    \n\n    \n    \n      \n      Explainable Models\n      \n        Train and validate with holdout/CV, emphasize interpretability (GLMs, trees, SHAP-style diagnostics) so stakeholders trust results.\n      \n    \n\n    \n    \n      \n      Executive-Ready Visuals\n      \n        Ship interactive dashboards and reports that are readable at a glance and defensible in detail.\n      \n    \n\n    \n    \n      \n      Production & Iteration\n      \n        Deploy lightweight apps, instrument usage, and iterate based on real-world feedback to sustain impact.\n      \n    \n\n  \n\n\n\n\n  Featured Projects\n  Each case highlights a problem, my approach, and what it shows about how I work.\n\n  \n    \n\n      \n      \n        \n          Teaching & Adoption\n          \n          Interactive Teaching with webR\n          Problem: Static lecture materials slow down comprehension of statistical ideas.\n          Approach: Built browser-native, no-install lessons using webR and Quarto so students can run code inline.\n          Demonstrates: Education UX, reproducibility, R/Quarto integration, and clear pedagogy.\n          See Project →\n        \n      \n\n      \n      \n        \n          Analytics App\n          \n          Shiny App: UK Traffic Accident Trends\n          Problem: Stakeholders need to explore incident patterns without wrangling data.\n          Approach: Delivered an intuitive R/Shiny app with filters, dynamic views, and sensible defaults.\n          Demonstrates: R/Shiny engineering, data modeling, thoughtful IA, and visual best practices.\n          See Project →\n        \n      \n\n      \n      \n        \n          Statistical Modeling\n          \n          Multivariate Analysis: Wine Quality\n          Problem: Identify drivers of perceived wine quality across correlated features.\n          Approach: Applied PCA and supervised models with diagnostics to separate signal from noise.\n          Demonstrates: Sound inference workflow, feature interpretation, and model validation.\n          See Project →\n        \n      \n\n      \n      \n        \n          LLM + RAG\n          \n          DataScienceYapper (Client-side LLM + RAG)\n          Problem: Visitors need fast, private answers about my work without server costs.\n          Approach: Implemented in-browser LLM with retrieval over site content; no data leaves the client.\n          Demonstrates: Modern web ML, RAG design, UX, and performance-minded engineering.\n          Open Chat →\n        \n      \n\n    \n\n    \n    \n      \n    \n    \n      \n    \n  \n\n\n\n\n  Data Science Toolkit\n  Core languages, frameworks, and workflows I use on real projects.\n\n  \n    \n      \n      R / Shiny\n      5+ yrs\n      Production dashboards, tidyverse pipelines, teaching materials, and packages.\n    \n\n    \n      \n      Python\n      4+ yrs\n      ETL, scikit-learn pipelines, automation, data apps.\n    \n\n    \n      \n      Statistical Analysis\n      4+ yrs\n      Experimental design, GLMs, uncertainty communication, and diagnostics.\n    \n\n    \n      \n      SQL\n      2+ yrs\n      Schema design, window functions, performance-minded queries.\n    \n\n    \n      \n      Consulting\n      2+ yrs\n      Scope, plan, and deliver projects with clear tradeoffs and documented assumptions.\n    \n\n    \n      \n      Cloud\n      2+ yrs\n      Deploy lightweight services on GCP/AWS; CI-friendly, cost-aware setups.\n    \n  \n\n\n\n\n  \n    \n    \n\n    Ready to discuss your data problem?\n    \n      Send a note with context and timelines; I’ll reply with a concrete next step.\n    \n\n    \n      \n        \n           fill=\"currentColor\"\n                d=\"M20 18h-2V9.25L12 13 6 9.25V18H4V6h1.2l6.8 4.25L18.8 6H20m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2z\"/&gt;\n        \n        Email Me\n      \n\n      \n        \n        LinkedIn\n      \n    \n\n    \n      \n      Download Full Resume\n    \n  \n\n\n\n\n  Resume\n  Preview inline or download a copy.\n\n  \n    \n      \n        Open in new tab\n      \n      \n        Download PDF\n      \n      Print\n    \n\n    \n    \n\n    \n      Your browser can’t display the PDF.\n         You can download it here."
  },
  {
    "objectID": "content/talks/understanding-quarto/index.html",
    "href": "content/talks/understanding-quarto/index.html",
    "title": "Quarto 101",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o1    \n  \n  I created a dynamic Quarto presentation integrating code examples in R, Python, Julia, and ObservableJS, as well as embedded Shiny applications. I showcased the generation of multiple simulated datasets, multi-language FizzBuzz tabsets, and a serverless Shiny approach. This methodology emphasizes cross-language interoperability, reproducible workflows, and streamlined deployment for advanced data visualization."
  },
  {
    "objectID": "content/talks/missing-data-with-mice-statistical-analysis/index.html",
    "href": "content/talks/missing-data-with-mice-statistical-analysis/index.html",
    "title": "Missing Data & Application of M.I.C.E.",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I explored the impact of missing data patterns (MCAR, MAR, MNAR) on regression analysis using Multiple Imputation by Chained Equations (MICE), a method that leverages relationships among variables to fill gaps. My analysis showed MICE effectively preserved data integrity and provided unbiased parameter estimates, even when missingness reached 70%. Evaluation metrics like Percent Bias, Coverage Rate, and RMSE confirmed MICE’s statistical validity, highlighting its advantage over simpler imputation techniques."
  },
  {
    "objectID": "content/talks/index.html",
    "href": "content/talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I’ve presented a series of talks that span a wide range of data science applications and methodologies. My recent discussions include practical strategies for building professional portfolios with Quarto and GitHub Pages, and in-depth analyses such as logistic regression for loan approval and survival analysis to assess pesticide toxicity under varying temperatures. Other sessions have focused on implementing MICE for handling missing data, leveraging interactive teaching tools like webR, and exploring the fundamentals of reproducible reporting with Quarto 101.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nJan 8, 2025\n\n\nBuilding a Professional Portfolio with Quarto and GitHub Pages\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\nDec 4, 2024\n\n\nLoan Approval Logistic Regression Analysis\n\n\nBrian Cervantes Alvarez, Wylea Walker\n\n\n\n\n\n\nDec 3, 2024\n\n\nMissing Data & Application of M.I.C.E.\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\nAug 6, 2024\n\n\nInteractive Teaching with webR\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\nNov 14, 2023\n\n\nQuarto 101\n\n\nBrian Cervantes Alvarez\n\n\n\n\n\n\nFeb 27, 2023\n\n\nInvestigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach\n\n\nBrian Cervantes Alvarez, Willa Van Liew, Hans Lehndorff\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/resources/index.html",
    "href": "content/resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Resources\n\n    Want to learn more? Here are links to help you improve your data science skills.\n\n\n\n    \n\n        \n\n            \n\n        \n\n        \n\n            Quarto\n\n            Empowers scientific communication with advanced formatting and data display. Explore its support for reproducible workflows across languages like R, Python, and Julia.\n\n        \n\n    \n\n\n\n    \n\n        \n\n            \n\n        \n\n        \n\n            Posit\n\n            Provides integrated tools for data analysis, visualization, and application development. Enhance productivity with its collaborative environment and seamless integration with RStudio.\n\n        \n\n    \n\n\n\n    \n\n        \n\n            \n\n        \n\n        \n\n            Shiny\n\n            Create dynamic R web apps with Shiny. Easily build interactive interfaces for your data projects. Leverage its wide array of built-in features for data visualization and user input.\n\n        \n\n    \n\n\n\n    \n\n        \n\n            \n\n        \n\n        \n\n            GitHub Pages\n\n            Easily host and publish your projects directly from a GitHub repository. Showcase your work professionally with customizable templates and a straightforward setup.\n\n        \n\n    \n\n\n\n    \n\n        \n\n            \n\n        \n\n        \n\n            R4DS Online Community\n\n            Join to improve R skills through collaboration, resources, and discussions. Access a supportive community and numerous guides to accelerate your learning journey.\n\n        \n\n    \n\n\n\n    \n\n        \n\n            \n\n        \n\n        \n\n            R-bloggers\n\n            Aggregates R-related content to enhance your programming knowledge and insight. Stay updated with the latest trends and tutorials from experienced R users worldwide.\n\n        \n\n    \n\n\n\n    \n\n        \n\n            \n\n        \n\n        \n\n            R Weekly\n\n            Weekly updates on R programming, showcasing community contributions and projects. Dive into curated resources to stay connected with the latest in the R ecosystem."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html",
    "title": "Applied Multivariate Analysis",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I performed exploratory data analysis and applied multivariate statistical methods, including MANOVA and Principal Component Analysis (PCA), to distinguish red from white wines and identify key variables predicting wine quality. My Random Forest classification model achieved an excellent recall of approximately 98.96% in distinguishing wine types, and an accuracy of 69.2% in predicting wine quality categories. PCA identified influential components like ‘Wine Body’ and ‘Fermentation Characteristics,’ contributing significantly (68.13% accuracy with 11 principal components) to effective classification performance."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-eda-on-red-white-wine-means",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-eda-on-red-white-wine-means",
    "title": "Applied Multivariate Analysis",
    "section": "Perform EDA on Red & White Wine Means:",
    "text": "Perform EDA on Red & White Wine Means:\nI conducted an exploratory data analysis in comparing means for the 11 chemical attributes in the red and white wines. Notably, ‘totalSulfurDioxide’ showed significant mean differences, followed by ‘freeSulfurDioxide’ and ‘residualSugar.’ These attributes currently stand out showing a clear difference between the means of white and red wines."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-manova-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-red-and-white-wines-with-a-95-confidence-level.",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-manova-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-red-and-white-wines-with-a-95-confidence-level.",
    "title": "Applied Multivariate Analysis",
    "section": "Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.",
    "text": "Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.\n\\begin{align*}\n&H_0: \\bar{\\mu}_{\\text{red}} = \\bar{\\mu}_{\\text{white}} \\\\\n&H_A: \\bar{\\mu}_{\\text{red}} \\neq \\bar{\\mu}_{\\text{white}}\n\\end{align*}\n\nMANOVA Results\nThe MANOVA reveals a significant disparity in the means for specific chemical traits between red and white wines. Notably, Pillai’s Trace (0.86158) indicates a robust effect, accounting for approximately 86.128\\% of the variance. The p-value of 2.2 \\times 10^{-16} signifies significant differences in the mean vectors. Hence, the MANOVA decisively rejects the null hypothesis of no difference in means, and we have exceptionally high confidence in accepting the alternative-that there is a difference in means between each wine."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-classification-modeling-on-red-white-wines",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-classification-modeling-on-red-white-wines",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Classification Modeling on Red & White Wines",
    "text": "Perform Classification Modeling on Red & White Wines\n\nTrain-Test Split & Cross-Validation Set up\nI performed a train-test split to ensure the models can handle new, unseen data. I allocated 70\\% to the training data to have a larger sample for the testing data (30\\%). To enhance reliability, I employed cross-validation, repeatedly splitting the data into different training and testing sets. This approach provides a more comprehensive evaluation of the model’s effectiveness.\n\n\nRandom Forest, SVM and Logistic Models\nFor my models, I’ve selected Random Forest, Support Vector Machine, and Logistic Regression as promising candidates for effective classification. Logistic Regression is particularly beneficial in binary classification scenarios due to its simplicity and interpretability. Meanwhile, Random Forest excels in capturing complex relationships through ensemble learning, and Support Vector Machine demonstrates proficiency in handling both linear and non-linear patterns. As the results will show later, Random Forest performed the best followed by SVM. Note, the models were not tuned to use their best hyperparameters.\n\n\nMetrics & Variable Importance\nThe confusion matrix for the red and white wine classification using the Random Forest model shows strong performance. The model correctly identified 474 red wines and 1464 white wines, with only 5 red wines and 5 white wines misclassified. This indicates high accuracy in both precision and recall. In comparison to the Support Vector Machine (SVM) and Logistic Regression models, the Random Forest performed better by minimizing misclassifications. The SVM model had slightly more misclassified instances (13 in total), while the Logistic Regression model had 17 misclassifications. The Random Forest’s performance makes it a better choice for this classifying red and white wines compared to SVM and Logistic Regression.\nThe variable importance analysis shows that “chlorides,” with a significance of 100%, is the most crucial feature for distinguishing red and white wines. Additionally, “totalSulfurDioxide” (96.92%) and “volatileAcidity” (43.47%) also played key roles, contributing to the model’s clear performance in wine classification.\n\n\nClassifying a New Red Wine Drawn From The Same Population\nTo estimate the probability of correctly classifying a new red wine drawn from the same population, we can use the concept of recall.\nIn our confusion matrix:\n \\text{Recall (for red wine)} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}} \nTo find the probability:\n \\text{Recall (for red wine)} = \\frac{474}{474 + 5} = \\frac{474}{479}  = 0.9896\nSo, the estimated probability of correctly classifying a new red wine, drawn from the same population, is approximately \\frac{474}{479}, or roughly 98.96\\%. This suggests a very high probability of correctly identifying red wines based on the model’s current performance."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#k-means-clustering",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#k-means-clustering",
    "title": "Applied Multivariate Analysis",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nI chose k-means clustering with Euclidean distance for its efficiency with standardized numerical data. While k = 2 visually showed a clear distinction between red and white wines, higher k values (for example, 3 or 4) led to overlapping clusters, affecting the meaningful separation observed with k = 2."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-manovas-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-wines-with-different-qualityquality-groups-with-a-95-confidence-level.",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-manovas-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-wines-with-different-qualityquality-groups-with-a-95-confidence-level.",
    "title": "Applied Multivariate Analysis",
    "section": "Perform MANOVAs To Determine If There Are A Significant Difference In Mean Vectors Between Wines With Different Quality/Quality Groups With A 95% Confidence Level.",
    "text": "Perform MANOVAs To Determine If There Are A Significant Difference In Mean Vectors Between Wines With Different Quality/Quality Groups With A 95% Confidence Level.\n\nMANOVA for Quality Levels 3, 4, 5, 6, 7, 8\n\\begin{align*}\n&H_0: \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 = \\mu_6 = \\mu_7 = \\mu_8 \\\\\n&H_A: \\text{At least one of } \\mu_2, \\mu_3, \\mu_4, \\mu_5, \\mu_6, \\mu_7, \\text{ or } \\mu_8 \\text{ is different}\n\\end{align*}\nIn the first analysis, we looked at the Original Quality Scores, and the results were highly significant. The p-value was super close to zero, less than 2.2 x 10^{-16}. This means there are big differences in the average chemical properties for different quality scores. Therefore, the mean vectors for each quality level varied, providing strong support for rejecting the null hypothesis.\n\n\nMANOVA for Quality Groups [Low, Medium, High]\n\\begin{align*}\n&H_0: \\mu_{\\text{Low}} = \\mu_{\\text{Medium}} = \\mu_{\\text{High}} \\\\\n&H_A: \\text{At least one of } \\mu_{\\text{Low}}, \\mu_{\\text{Medium}}, \\text{ or } \\mu_{\\text{High}} \\text{ is different}\n\\end{align*}\nIn the second analysis, we focused on Quality Groups (Low, Medium, High), and the results were also highly significant. The p-value was very close to zero, less than 2.2 x 10^{-16}. This indicates significant differences in the average chemical properties across different quality groups. We have strong evidence that the mean vectors between each quality group differed. Therefore, we have evidence to reject the null and be in favor of the alternative.\n\n\nOverall MANOVA Test Conclusion\nTo summarize, our MANOVA tests reveal significant differences in average values for both original quality scores and quality groups. For original scores, statistics like Pillai’s trace and Wilks’ lambda had extremely low p-values p &lt; 2.2 x 10^{-16}. Quality groups exhibited similar results."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-classification-on-quality-for-red-wines",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-classification-on-quality-for-red-wines",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Classification on Quality for Red Wines",
    "text": "Perform Classification on Quality for Red Wines\n\nRandom Forest and SVM models\nI performed the same procedure from the previous classification of red and white wines. I’ve selected Random Forest & Support Vector Machine the top models for classification. Logistic Regression is not designed for multiple classes. Interestingly, the Random Forest performed the best again, followed by SVM. It’s important to note that the models were not fine-tuned for hyperparameters at this stage.\n\n\nMetrics & Variable Importance\nRandom Forest emerged as the top-performing model once again, with an accuracy of 69.2\\%. For instance, we can observe that quality level 5 has the highest number of correct predictions (163), while quality levels 4 and 6 have some misclassifications. Among the features, alcohol, total sulfur dioxide, and volatile acidity emerged as the top three influential variables, showcasing their significance in predicting wine quality in red wines."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-principal-component-analysis",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-principal-component-analysis",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Principal Component Analysis",
    "text": "Perform Principal Component Analysis\n\nExplaining PC1 & PC2\nPC1 can be interpreted as representing “Wine Body.” Red wines with higher fixed acidity, citric acid, free sulfur dioxide, and total sulfur dioxide contribute positively to this component, indicating a fuller and more robust body. Hence, higher levels of volatile acidity, residual sugar, and alcohol contribute negatively to this component.\nPC2 can be labeled as “Fermentation Characteristics.” Additionally, red wines with elevated levels of free sulfur dioxide, total sulfur dioxide, and density contribute positively to this component, highlighting aspects related to the fermentation process. On the other end, higher alcohol content and volatile acidity contribute negatively to PC2.\n\n\nRandom Forest Model with 2 PCA\nIn the confusion matrix, it’s evident that the model struggled to accurately predict certain classes, particularly in categories 3, 4, and 7, where the predicted values differ from the actual values. To add, random forest model achieved an accuracy of 58.07% which is quite below the previous models. Despite its limitations, the model demonstrated some success in capturing patterns related to “Wine Body” and “Fermentation Characteristics.” And it’s with just 2 variables with linear combinations.\n\n\nRandom Forest Model with 11 PCAs\nThe random forest model attained an accuracy of 68.13%. Plus, it excelled in predicting class 5 but faced challenges in classes 3, 4, 6, and 7. Principal Component Analysis (PCA) highlights PC2 as the most influential (100%), followed by PC3 (80.49%), PC5 (32.47%), and others. This suggests a need for further analysis to enhance predictions in specific classes and leverage insights from key Principal Components for optimization.\n\n\nComparison between Random Forest Models (normal, 2PCs, 11PCs)\nIn comparing the Random Forest models, both the normal model and the 11 PCs model achieve an accuracy of approximately 68%, surpassing the 2 PCs model, which attains an accuracy of 58.07%. It’s noteworthy that the 2 PCs model demonstrates the potency of PCA, albeit with a trade-off in interpretability. Despite the challenges encountered, each model variant provides valuable insights for optimizing the predictive power. The room for improvement is wide open. Factors such as hyperparameter tuning, other models that were not explored, feature engineering, and delving further into factor analysis are instances that could be used to maximize performance."
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-eda-on-red-white-wine-means-1",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-eda-on-red-white-wine-means-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform EDA on Red & White Wine Means:",
    "text": "Perform EDA on Red & White Wine Means:\n\n# Calculate the mean vectors for red and white wines \n# separately for each of the 11 chemical attributes\nmeanVectors &lt;- wineDs %&gt;% \n  group_by(wineType) %&gt;%\n  summarize_all(mean)\n\n# Display the mean vectors\nhead(meanVectors, 2)\n\n# A tibble: 2 × 12\n  wineType chlorides density    pH sulphates alcohol fixedAcidity\n  &lt;fct&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 red         0.0875   0.997  3.31     0.658    10.4         8.32\n2 white       0.0458   0.994  3.19     0.490    10.5         6.85\n# ℹ 5 more variables: volatileAcidity &lt;dbl&gt;, citricAcid &lt;dbl&gt;,\n#   residualSugar &lt;dbl&gt;, freeSulfurDioxide &lt;dbl&gt;, totalSulfurDioxide &lt;dbl&gt;\n\n# Convert to long format for plotting\nmeanDs &lt;- tidyr::gather(meanVectors,\n                        key = \"attribute\", \n                        value = \"means\", -wineType)\n\n#meanDs\n\n# Plot\np1 &lt;- ggplot(meanDs, aes(x = means, y = attribute, fill = wineType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Mean Values for Red|White Wines\",\n       x = \"Mean Value\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\n\n# Plot\np2 &lt;- ggplot(meanDs, aes(x = log(means), y = attribute, fill = wineType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Log-Transformed Mean Values for Red|White Wines\",\n       x = \"Mean Value\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\nmeanDifDs &lt;- meanDs %&gt;%\n  spread(wineType, means) %&gt;%\n  mutate(meanDifference = red - white)\n\n# Plot the mean differences\np3 &lt;- ggplot(meanDifDs, aes(x = meanDifference, y = attribute)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", \n           fill = \"red\", color = \"black\") +\n  labs(title = \"Mean Differences between Red & White Wines\",\n       x = \"Mean Difference\",\n       y = \"Chemical Attributes\") +\n  theme_minimal()\n\n\n# Show plot 1 for report\np1\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\np3"
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-manova-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-red-and-white-wines-with-a-95-confidence-level.-1",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-manova-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-red-and-white-wines-with-a-95-confidence-level.-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.",
    "text": "Perform MANOVA to determine if there are a significant difference in mean vectors between red and white wines with a 95% confidence level.\n\nwineManova &lt;- manova(cbind(chlorides, density, pH, sulphates, \n                           alcohol, fixedAcidity, volatileAcidity, \n                           citricAcid, residualSugar, freeSulfurDioxide,\n                           totalSulfurDioxide) ~ wineType, data = wineDs) \n\n# Print the summary of the MANOVA\nsummary(wineManova)\n\n            Df  Pillai approx F num Df den Df                Pr(&gt;F)    \nwineType     1 0.86158   3669.6     11   6485 &lt; 0.00000000000000022 ***\nResiduals 6495                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nMANOVA Results"
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-classification-modeling-on-red-white-wines-1",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-classification-modeling-on-red-white-wines-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Classification Modeling on Red & White Wines",
    "text": "Perform Classification Modeling on Red & White Wines\n\nTrain-Test Split & Cross-Validation Set up\n\nsplitIndex &lt;- createDataPartition(wineDs$wineType, p = 0.7, list = FALSE)\ntrainData &lt;- wineDs[splitIndex, ]\ntestData &lt;- wineDs[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n\n\nRandom Forest, SVM and Logistic Models\n\nset.seed(2013)\n\n# Train Random Forest\nrfModel &lt;- train(wineType ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Train Logistic Regression\nlogisticModel &lt;- train(wineType ~ ., \n                       data = trainData,\n                       method = \"glm\",\n                       family = \"binomial\")\n\n# Train Support Vector Machine\nsvmModel &lt;- train(wineType ~ ., \n                  data = trainData,\n                  method = \"svmRadial\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\nlogisticPred &lt;- predict(logisticModel, newdata = testData)\nsvmPred &lt;- predict(svmModel, newdata = testData)\n\n\n\nMetrics & Variable Importance\n\n# Random Forest model metrics\nconfMatrixRF &lt;- confusionMatrix(rfPred, testData$wineType, \n                                dnn = c(\"Prediction\", \"Reference\"))\naccuracyRF &lt;- confMatrixRF$overall[\"Accuracy\"]\n\n# Logistic Regression model metrics\nconfMatrixLogistic &lt;- confusionMatrix(logisticPred, testData$wineType, \n                                      dnn = c(\"Prediction\", \"Reference\"))\naccuracyLogistic &lt;- confMatrixLogistic$overall[\"Accuracy\"]\n\n# SVM model metrics\nconfMatrixSVM &lt;- confusionMatrix(svmPred, testData$wineType, \n                                 dnn = c(\"Prediction\", \"Reference\"))\naccuracySVM &lt;- confMatrixSVM$overall[\"Accuracy\"]\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$wineType)) +\n    scale_y_discrete(labels = levels(testData$wineType))\n}\n\n#confMatrixRF\nconfMatrixLogistic\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  red white\n     red    473     6\n     white    6  1463\n                                             \n               Accuracy : 0.9938             \n                 95% CI : (0.9893, 0.9968)   \n    No Information Rate : 0.7541             \n    P-Value [Acc &gt; NIR] : &lt;0.0000000000000002\n                                             \n                  Kappa : 0.9834             \n                                             \n Mcnemar's Test P-Value : 1                  \n                                             \n            Sensitivity : 0.9875             \n            Specificity : 0.9959             \n         Pos Pred Value : 0.9875             \n         Neg Pred Value : 0.9959             \n             Prevalence : 0.2459             \n         Detection Rate : 0.2428             \n   Detection Prevalence : 0.2459             \n      Balanced Accuracy : 0.9917             \n                                             \n       'Positive' Class : red                \n                                             \n\n#confMatrixSVM\n\n# Plot Confusion Matrices for each model\nplotCM(confMatrixRF, \"Random Forest\")\n\n\n\n\n\n\n\nplotCM(confMatrixLogistic, \"Logistic Regression\")\n\n\n\n\n\n\n\nplotCM(confMatrixSVM, \"Support Vector Machine (SVM)\")\n\n\n\n\n\n\n\n# Print the metrics for each model\nprint(\"Random Forest Model Results\")\n\n[1] \"Random Forest Model Results\"\n\nprint(paste(\"Accuracy:\", round(accuracyRF, 4)))\n\n[1] \"Accuracy: 0.9938\"\n\nprint(\"Logistic Regression Model Results:\")\n\n[1] \"Logistic Regression Model Results:\"\n\nprint(paste(\"Accuracy:\", round(accuracyLogistic, 4)))\n\n[1] \"Accuracy: 0.9938\"\n\nprint(\"Support Vector Machine (SVM) Model Results:\")\n\n[1] \"Support Vector Machine (SVM) Model Results:\"\n\nprint(paste(\"Accuracy:\", round(accuracySVM, 4)))\n\n[1] \"Accuracy: 0.9964\"\n\n# Get variable importance from the best model\nvarImp(rfModel)\n\nrf variable importance\n\n                   Overall\nchlorides          100.000\ntotalSulfurDioxide  80.296\nvolatileAcidity     43.041\nfreeSulfurDioxide   23.194\ndensity             22.347\nresidualSugar       18.246\nsulphates           17.811\nfixedAcidity        12.766\ncitricAcid           4.859\npH                   3.794\nalcohol              0.000\n\n\n\n\nClassifying a New Red Wine Drawn From The Same Population"
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#clustering",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#clustering",
    "title": "Applied Multivariate Analysis",
    "section": "Clustering:",
    "text": "Clustering:\n\nset.seed(123)\n\nds &lt;- wineDs %&gt;%\n  select(-wineType)\n\nds &lt;- scale(ds)\n\nfviz_nbclust(ds, kmeans, method='silhouette')\n\n\n\n\n\n\n\nkm.final &lt;- kmeans(ds, 2, nstart = 30)\n\nfviz_cluster(km.final, data = ds, \n             geom = \"point\",\n             ellipse.type = \"convex\", \n             ggtheme = theme_bw())"
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-manovas-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-wines-with-different-qualityquality-groups-with-a-95-confidence-level",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-manovas-to-determine-if-there-are-a-significant-difference-in-mean-vectors-between-wines-with-different-qualityquality-groups-with-a-95-confidence-level",
    "title": "Applied Multivariate Analysis",
    "section": "Perform MANOVAs to determine if there are a significant difference in mean vectors between wines with different quality/quality groups with a 95% confidence level?",
    "text": "Perform MANOVAs to determine if there are a significant difference in mean vectors between wines with different quality/quality groups with a 95% confidence level?\n\nMANOVA for Quality Levels 3, 4, 5, 6, 7, 8\n\nredWineDs &lt;- wine %&gt;%\n  filter(wineType == \"red\") %&gt;%\n  select(-wineType)\n\n# Extracting Columns\ncolVars &lt;-  cbind(\n  redWineDs$chlorides, redWineDs$density, redWineDs$pH, redWineDs$sulphates, \n  redWineDs$alcohol, redWineDs$fixedAcidity, redWineDs$volatileAcidity, \n  redWineDs$citricAcid, redWineDs$residualSugar, redWineDs$freeSulfurDioxide,\n  redWineDs$totalSulfurDioxide\n)\n\n# MANOVA Analysis - Quality, levels = 3,4,5,6,7,8\nmanaovaTest &lt;- manova(colVars ~ quality, data = redWineDs)\nsummary(manaovaTest)\n\n            Df  Pillai approx F num Df den Df                Pr(&gt;F)    \nquality      1 0.36055   81.348     11   1587 &lt; 0.00000000000000022 ***\nResiduals 1597                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nMANOVA for Quality Groups [Low, Medium, High]\n\n# Adding qualityGroup\nredWineDs &lt;- redWineDs %&gt;% \n  mutate(\n    qualityGroup = case_when(\n      quality %in% 3:4 ~ \"Low\",\n      quality %in% 5:6 ~ \"Medium\",\n      quality %in% 7:8 ~ \"High\"\n    )\n  ) %&gt;%\n  mutate(qualityGroup = factor(qualityGroup, levels = c(\"Low\", \n                                                        \"Medium\", \n                                                        \"High\")))\ncolVarsCategorized &lt;-  cbind(\n  redWineDs$chlorides, redWineDs$density, redWineDs$pH, \n  redWineDs$sulphates, redWineDs$alcohol, redWineDs$fixedAcidity, \n  redWineDs$volatileAcidity, redWineDs$citricAcid, \n  redWineDs$residualSugar, redWineDs$freeSulfurDioxide,\n  redWineDs$totalSulfurDioxide\n)\n\n# MANOVA Analysis - QualityGroup, Levels = \"Low\", \"Medium\", \"High\"\nmanaovaTest &lt;- manova(colVars ~ qualityGroup, data = redWineDs)\nsummary(manaovaTest)\n\n               Df  Pillai approx F num Df den Df                Pr(&gt;F)    \nqualityGroup    2 0.30989   26.453     22   3174 &lt; 0.00000000000000022 ***\nResiduals    1596                                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nOverall MANOVA Test Conclusion"
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-classification-on-quality-for-red-wines-1",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-classification-on-quality-for-red-wines-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Classification on Quality for Red Wines",
    "text": "Perform Classification on Quality for Red Wines\n\nqualityDs &lt;- redWineDs %&gt;%\n  mutate(quality = factor(quality, levels = c(\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"))) %&gt;%\n  select(-qualityGroup)\n\n\n# Split the dataset into training and testing sets\nsplitIndex &lt;- createDataPartition(qualityDs$quality, p = 0.7, list = FALSE)\ntrainData &lt;- qualityDs[splitIndex, ]\ntestData &lt;- qualityDs[-splitIndex, ]\n\n# Create a train control object for repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n\nRandom Forest and SVM models\n\nset.seed(2013)\n# Random Forest model\nrfModel &lt;- train(quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# (SVM) model\nsvmModel &lt;- train(quality ~ ., \n                  data = trainData,\n                  method = \"svmLinear\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\nsvmPred &lt;- predict(svmModel, newdata = testData)\n\n\n\nMetrics & Variable Importance\n\n# Random Forest confusion matrix\nrfConfMatrix &lt;- confusionMatrix(rfPred, testData$quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy &lt;- rfConfMatrix$overall[\"Accuracy\"]\n\n# SVM confusion matrix\nsvmConfMatrix &lt;- confusionMatrix(svmPred, testData$quality, \n                                 dnn = c(\"Prediction\", \"Reference\"))\nsvmAccuracy &lt;- svmConfMatrix$overall[\"Accuracy\"]\n\n# Print the results\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n\n[1] \"Random Forest Accuracy: 0.691823899371069\"\n\nprint(paste(\"SVM Accuracy:\", svmAccuracy))\n\n[1] \"SVM Accuracy: 0.59538784067086\"\n\nrfConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   0   1   0   0\n         5   3  10 163  46   5   0\n         6   0   4  40 137  24   4\n         7   0   1   1   7  30   1\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                               \n               Accuracy : 0.6918               \n                 95% CI : (0.6482, 0.733)      \n    No Information Rate : 0.4277               \n    P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022\n                                               \n                  Kappa : 0.4953               \n                                               \n Mcnemar's Test P-Value : NA                   \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000 0.000000   0.7990   0.7173  0.50847  0.00000\nSpecificity          1.000000 0.997835   0.7656   0.7483  0.97608  1.00000\nPos Pred Value            NaN 0.000000   0.7181   0.6555  0.75000      NaN\nNeg Pred Value       0.993711 0.968487   0.8360   0.7985  0.93364  0.98952\nPrevalence           0.006289 0.031447   0.4277   0.4004  0.12369  0.01048\nDetection Rate       0.000000 0.000000   0.3417   0.2872  0.06289  0.00000\nDetection Prevalence 0.000000 0.002096   0.4759   0.4382  0.08386  0.00000\nBalanced Accuracy    0.500000 0.498918   0.7823   0.7328  0.74228  0.50000\n\nsvmConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   1   0   0   0\n         4   0   0   0   0   0   0\n         5   2  11 159  66   6   0\n         6   1   4  44 125  53   5\n         7   0   0   0   0   0   0\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                            \n               Accuracy : 0.5954            \n                 95% CI : (0.5498, 0.6398)  \n    No Information Rate : 0.4277            \n    P-Value [Acc &gt; NIR] : 0.0000000000001356\n                                            \n                  Kappa : 0.3101            \n                                            \n Mcnemar's Test P-Value : NA                \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000  0.00000   0.7794   0.6545   0.0000  0.00000\nSpecificity          0.997890  1.00000   0.6886   0.6259   1.0000  1.00000\nPos Pred Value       0.000000      NaN   0.6516   0.5388      NaN      NaN\nNeg Pred Value       0.993697  0.96855   0.8069   0.7306   0.8763  0.98952\nPrevalence           0.006289  0.03145   0.4277   0.4004   0.1237  0.01048\nDetection Rate       0.000000  0.00000   0.3333   0.2621   0.0000  0.00000\nDetection Prevalence 0.002096  0.00000   0.5115   0.4864   0.0000  0.00000\nBalanced Accuracy    0.498945  0.50000   0.7340   0.6402   0.5000  0.50000\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$quality)) +\n    scale_y_discrete(labels = levels(testData$quality))\n}\n\nplotCM(rfConfMatrix, \"Random Forest\")\n\n\n\n\n\n\n\nplotCM(svmConfMatrix, \"SVM\")\n\n\n\n\n\n\n\nplot(varImp(rfModel))"
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-principal-component-analysis-1",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#perform-principal-component-analysis-1",
    "title": "Applied Multivariate Analysis",
    "section": "Perform Principal Component Analysis",
    "text": "Perform Principal Component Analysis\n\nPart 1\n\nds &lt;- select(qualityDs, -quality)\n\n# Perform PCA\npcaResults &lt;- prcomp(ds, scale = T, center = T)\n\n# Print PCA summary\nsummary(pcaResults)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     1.7604 1.3878 1.2452 1.1015 0.97943 0.81216 0.76406\nProportion of Variance 0.2817 0.1751 0.1410 0.1103 0.08721 0.05996 0.05307\nCumulative Proportion  0.2817 0.4568 0.5978 0.7081 0.79528 0.85525 0.90832\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.65035 0.58706 0.42583 0.24405\nProportion of Variance 0.03845 0.03133 0.01648 0.00541\nCumulative Proportion  0.94677 0.97810 0.99459 1.00000\n\n# Visualize PCA results\nfviz_eig(pcaResults, addlabels = TRUE, kaiser = TRUE)\n\n\n\n\n\n\n\nfviz_pca_ind(pcaResults, geom = \"point\", col.ind = qualityDs$quality, palette = \"jco\")\n\n\n\n\n\n\n\nfviz_pca_biplot(pcaResults, geom = \"arrow\", col.var = \"contrib\", palette = \"jco\", alpha = 0.7)\n\n\n\n\n\n\n\npcaDs &lt;- rownames_to_column(as.data.frame(pcaResults$rotation))\n\npcaDs %&gt;% \n  select(rowname, PC1, PC2)\n\n              rowname         PC1          PC2\n1           chlorides  0.21224658  0.148051555\n2             density  0.39535301  0.233575490\n3                  pH -0.43851962  0.006710793\n4           sulphates  0.24292133 -0.037553916\n5             alcohol -0.11323207 -0.386180959\n6        fixedAcidity  0.48931422 -0.110502738\n7     volatileAcidity -0.23858436  0.274930480\n8          citricAcid  0.46363166 -0.151791356\n9       residualSugar  0.14610715  0.272080238\n10  freeSulfurDioxide -0.03615752  0.513566812\n11 totalSulfurDioxide  0.02357485  0.569486959\n\n# Adding back the quality with principal components\nprc &lt;- select(qualityDs, quality) %&gt;%\n  bind_cols(as.data.frame(pcaResults$x)) %&gt;%\n  select(quality, PC1, PC2) \n\n# Rename columns\nprc &lt;- prc %&gt;%\n  rename(\n    \"Quality\" = quality,\n    \"Wine Body\" = PC1,\n    \"Fermentation Characteristics\" = PC2\n  )\n\nprc2 &lt;- select(qualityDs, quality) %&gt;%\n  bind_cols(as.data.frame(pcaResults$x)) %&gt;%\n  select(quality, PC1, PC2, PC3, PC4, PC5, PC6, PC7, PC8, PC9, PC10, PC11) \n\nprc2 &lt;- prc2 %&gt;%\n  rename(\"Quality\" = quality)\n\nprc2\n\n# A tibble: 1,599 × 12\n   Quality    PC1    PC2    PC3      PC4     PC5    PC6    PC7    PC8      PC9\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 5       -1.62   0.451 -1.77   0.0437  -0.0670 -0.914  0.161  0.282 -0.00510\n 2 5       -0.799  1.86  -0.911  0.548    0.0184  0.929  1.01  -0.762  0.521  \n 3 5       -0.748  0.882 -1.17   0.411    0.0435  0.401  0.539 -0.598  0.0868 \n 4 6        2.36  -0.270  0.243 -0.928    1.50   -0.131 -0.344  0.455 -0.0915 \n 5 5       -1.62   0.451 -1.77   0.0437  -0.0670 -0.914  0.161  0.282 -0.00510\n 6 5       -1.58   0.569 -1.54   0.0237   0.110  -0.993  0.110  0.314  0.0343 \n 7 5       -1.10   0.608 -1.08  -0.344    1.13    0.175 -0.261 -0.240  0.0273 \n 8 7       -2.25  -0.417 -0.987 -0.00120  0.780   0.286 -0.131 -0.119  0.614  \n 9 7       -1.09  -0.308 -1.52   0.00331  0.227  -0.512 -0.250 -0.439  0.399  \n10 5        0.655  1.66   1.21  -0.824   -1.72   -0.476 -0.230 -0.839 -1.27   \n# ℹ 1,589 more rows\n# ℹ 2 more variables: PC10 &lt;dbl&gt;, PC11 &lt;dbl&gt;\n\n\n\n\nPart 2\n\nset.seed(2013)\n\nsplitIndex &lt;- createDataPartition(prc$Quality, p = 0.7, list = FALSE)\ntrainData &lt;- prc[splitIndex, ]\ntestData &lt;- prc[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n# Train Random Forest\nrfModel &lt;- train(Quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\nnote: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\n\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$Quality)) +\n    scale_y_discrete(labels = levels(testData$Quality))\n}\n\n# Random Forest metrics\nrfConfMatrix &lt;- confusionMatrix(rfPred, testData$Quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy &lt;- rfConfMatrix$overall[\"Accuracy\"]\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n\n[1] \"Random Forest Accuracy: 0.580712788259958\"\n\nplotCM(rfConfMatrix, \"Random Forest\")\n\n\n\n\n\n\n\nrfConfMatrix$table\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   3   2   0   0\n         5   1   7 135  53  11   1\n         6   2   6  56 117  23   4\n         7   0   2  10  19  25   0\n         8   0   0   0   0   0   0\n\n# Variable Importance\nvarImp(rfModel)\n\nrf variable importance\n\n                               Overall\n`Fermentation Characteristics`     100\n`Wine Body`                          0\n\n\n\nset.seed(2013)\nsplitIndex &lt;- createDataPartition(prc2$Quality, p = 0.7, list = FALSE)\ntrainData &lt;- prc2[splitIndex, ]\ntestData &lt;- prc2[-splitIndex, ]\n\n# Create Repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n# Train Random Forest\nrfModel &lt;- train(Quality ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\n\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$Quality)) +\n    scale_y_discrete(labels = levels(testData$Quality))\n}\n\n# Random Forest metrics\nrfConfMatrix &lt;- confusionMatrix(rfPred, testData$Quality, \n                                dnn = c(\"Prediction\", \"Reference\"))\nrfAccuracy &lt;- rfConfMatrix$overall[\"Accuracy\"]\nprint(paste(\"Random Forest Accuracy:\", rfAccuracy))\n\n[1] \"Random Forest Accuracy: 0.681341719077568\"\n\nplotCM(rfConfMatrix, \"Random Forest\")\n\n\n\n\n\n\n\nrfConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   3   4   5   6   7   8\n         3   0   0   0   0   0   0\n         4   0   0   1   0   0   0\n         5   3   9 161  47   4   0\n         6   0   6  39 134  25   3\n         7   0   0   3  10  30   2\n         8   0   0   0   0   0   0\n\nOverall Statistics\n                                               \n               Accuracy : 0.6813               \n                 95% CI : (0.6374, 0.723)      \n    No Information Rate : 0.4277               \n    P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022\n                                               \n                  Kappa : 0.4807               \n                                               \n Mcnemar's Test P-Value : NA                   \n\nStatistics by Class:\n\n                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8\nSensitivity          0.000000 0.000000   0.7892   0.7016  0.50847  0.00000\nSpecificity          1.000000 0.997835   0.7692   0.7448  0.96411  1.00000\nPos Pred Value            NaN 0.000000   0.7188   0.6473  0.66667      NaN\nNeg Pred Value       0.993711 0.968487   0.8300   0.7889  0.93287  0.98952\nPrevalence           0.006289 0.031447   0.4277   0.4004  0.12369  0.01048\nDetection Rate       0.000000 0.000000   0.3375   0.2809  0.06289  0.00000\nDetection Prevalence 0.000000 0.002096   0.4696   0.4340  0.09434  0.00000\nBalanced Accuracy    0.500000 0.498918   0.7792   0.7232  0.73629  0.50000\n\n# Variable Importance\nvarImp(rfModel)\n\nrf variable importance\n\n     Overall\nPC2  100.000\nPC3   80.491\nPC5   32.468\nPC9   30.188\nPC1   16.023\nPC4    7.563\nPC7    6.410\nPC8    5.631\nPC11   3.630\nPC10   3.489\nPC6    0.000"
  },
  {
    "objectID": "content/projects/wine-multivariate-statistical-analysis/index.html#repeat-for-grouped-quality-setting-low-medium-high",
    "href": "content/projects/wine-multivariate-statistical-analysis/index.html#repeat-for-grouped-quality-setting-low-medium-high",
    "title": "Applied Multivariate Analysis",
    "section": "Repeat for Grouped Quality Setting (Low, Medium, High)",
    "text": "Repeat for Grouped Quality Setting (Low, Medium, High)\n\nSetting up data for classification\n\ngroupedQualityDs &lt;- redWineDs %&gt;%\n  select(-quality)\n\n\n\nTrain-Test-Split & Cross-Validation Set up\n\n# Split the dataset into training and testing sets\nsplitIndex &lt;- createDataPartition(groupedQualityDs$qualityGroup, p = 0.7, list = FALSE)\ntrainData &lt;- groupedQualityDs[splitIndex, ]\ntestData &lt;- groupedQualityDs[-splitIndex, ]\n\n# Create a train control object for repeated cross-validation\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 5, \n                     repeats = 3, \n                     verboseIter = FALSE)\n\n\n\nRandom Forest and SVM models\n\nset.seed(2013)\n# Train Random Forest classifier using caret\nrfModel &lt;- train(qualityGroup ~ ., \n                  data = trainData,\n                  method = \"rf\",\n                  control = ctrl,\n                  ntree = 200)\n\n# Train Support Vector Machine (SVM) model using caret\nsvmModel &lt;- train(qualityGroup ~ ., \n                  data = trainData,\n                  method = \"svmRadial\",\n                  trControl = ctrl)\n\n# Make predictions on the test set for each model\nrfPred &lt;- predict(rfModel, newdata = testData)\nsvmPred &lt;- predict(svmModel, newdata = testData)\n\n\n\nMetrics\n\n# Plot Confusion Matrices\nplotCM &lt;- function(confMatrix, modelName) {\n  plt &lt;- as.data.frame(confMatrix$table)\n  plt$Prediction &lt;- factor(plt$Prediction, \n                           levels = rev(levels(plt$Prediction)))\n  \n  ggplot(plt, aes(Prediction, Reference, fill = Freq)) +\n    geom_tile() + geom_text(aes(label = Freq)) +\n    scale_fill_gradient(low = \"white\", high = \"#00859B\") +\n    labs(title = paste(\"Confusion Matrix -\", modelName),\n         x = \"Reference\", y = \"Prediction\") +\n    scale_x_discrete(labels = levels(testData$qualityGroup)) +\n    scale_y_discrete(labels = levels(testData$qualityGroup))\n}\n\n# Random Forest metrics\nrfConfMatrix &lt;- confusionMatrix(rfPred, testData$qualityGroup, \n                                dnn = c(\"Prediction\", \"Reference\"))\nsvmConfMatrix &lt;- confusionMatrix(svmPred, testData$qualityGroup, \n                                 dnn = c(\"Prediction\", \"Reference\"))\n\nrfConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low      0      0    0\n    Medium  18    388   39\n    High     0      7   26\n\nOverall Statistics\n                                          \n               Accuracy : 0.8661          \n                 95% CI : (0.8323, 0.8953)\n    No Information Rate : 0.8264          \n    P-Value [Acc &gt; NIR] : 0.01092         \n                                          \n                  Kappa : 0.395           \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity             0.00000        0.9823     0.40000\nSpecificity             1.00000        0.3133     0.98305\nPos Pred Value              NaN        0.8719     0.78788\nNeg Pred Value          0.96234        0.7879     0.91236\nPrevalence              0.03766        0.8264     0.13598\nDetection Rate          0.00000        0.8117     0.05439\nDetection Prevalence    0.00000        0.9310     0.06904\nBalanced Accuracy       0.50000        0.6478     0.69153\n\nsvmConfMatrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low      0      0    0\n    Medium  18    393   50\n    High     0      2   15\n\nOverall Statistics\n                                         \n               Accuracy : 0.8536         \n                 95% CI : (0.8186, 0.884)\n    No Information Rate : 0.8264         \n    P-Value [Acc &gt; NIR] : 0.06328        \n                                         \n                  Kappa : 0.2611         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity             0.00000        0.9949     0.23077\nSpecificity             1.00000        0.1807     0.99516\nPos Pred Value              NaN        0.8525     0.88235\nNeg Pred Value          0.96234        0.8824     0.89154\nPrevalence              0.03766        0.8264     0.13598\nDetection Rate          0.00000        0.8222     0.03138\nDetection Prevalence    0.00000        0.9644     0.03556\nBalanced Accuracy       0.50000        0.5878     0.61296\n\nplotCM(rfConfMatrix, \"Random Forest\")\n\n\n\n\n\n\n\nplotCM(svmConfMatrix, \"SVM\")\n\n\n\n\n\n\n\n\n\n\nLook at variable importance\n\nplot(varImp(rfModel))"
  },
  {
    "objectID": "content/projects/united-kingdom-car-accident-shiny-app/index.html",
    "href": "content/projects/united-kingdom-car-accident-shiny-app/index.html",
    "title": "UK Accident Time Series",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I developed an interactive Shiny dashboard to analyze time-series data on UK accidents, enabling users to visualize and explore trends, seasonality, and accident severity over custom date ranges. I included the ability to perform advanced statistical analyses including detrending, autocorrelation assessment, and day-of-week frequency evaluations in the shiny application; clearly demonstrating insights such as higher accident occurrences on weekends. Key visual metrics included daily fatality counts and detrended accident trends, enhancing data interpretability and enabling targeted decision-making.\n\n\n\nThis accident time series app allows you to explore and analyze data related accidents in UK. Feel free to download the data and explore other methods of analysis.\nHere is the link to the full page: UKAccidentTimeSeries"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html",
    "href": "content/projects/predicting-salaries-machine-learning/index.html",
    "title": "Predicting 50K+ Salaries",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  We conducted a comprehensive machine learning analysis aiming to predict if individuals earn more than $50K, involving feature engineering, PCA for dimensionality reduction, and building predictive models including Random Forest, Gradient Boosting Machines (GBM), Logistic Regression, and KMeans clustering. We achieved exceptional model performance, with the GBM model yielding a Kappa score of 0.9543 and an accuracy of 98.3%, significantly improved by careful hyperparameter tuning and PCA integration. KPIs clearly presented include Kappa scores, accuracy, sensitivity (99.3%), and specificity (95.3%), reflecting the robustness and precision of the model we developed"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#project-overview",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#project-overview",
    "title": "Predicting 50K+ Salaries",
    "section": "Project Overview",
    "text": "Project Overview\nIn this project, our goal was to predict whether individuals earn more than $50K using machine learning techniques. We engaged in a thorough data science process, from data preprocessing and feature engineering to selecting principal components through PCA and optimizing hyperparameters for our model. Our collaborative efforts led to a high-performing model with a Kappa score of 0.9543, enhancing our data science expertise and setting the stage for future projects."
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#team-contribution",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#team-contribution",
    "title": "Predicting 50K+ Salaries",
    "section": "Team Contribution",
    "text": "Team Contribution\nWilla played a pivotal role in developing the foundational models and pinpointing the key PCA features. Her outstanding efforts laid the groundwork for our high-quality model that accurately predicts incomes over $50,000. Willa’s expertise not only provided a solid base for our project but also enabled us to fine-tune our models for peak performance. I’m thankful for her invaluable input and dedication, which significantly contributed to the success of our work.\nExplore more of Willa’s data science work on her website."
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#predicting-income-50k",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#predicting-income-50k",
    "title": "Predicting 50K+ Salaries",
    "section": "Predicting Income >50K",
    "text": "Predicting Income &gt;50K\n\nLoad Libraries\n\n#knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(caret)\nlibrary(tidymodels)\nlibrary(fastDummies)\nlibrary(randomForest)\n\n\n\nLoad The Data\n\nincome = read_csv(\"../../../assets/datasets/openml_1590.csv\", na = c(\"?\")) %&gt;%\n  drop_na() %&gt;%\n  mutate(income_above_50K = ifelse(class == \"&gt;50K\",1,0)) %&gt;%\n  select(-class) %&gt;%\n  dummy_cols(remove_selected_columns = T)"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#run-random-forest-obtain-importance-features",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#run-random-forest-obtain-importance-features",
    "title": "Predicting 50K+ Salaries",
    "section": "Run Random Forest & Obtain Importance Features",
    "text": "Run Random Forest & Obtain Importance Features\n\nset.seed(504)\nraw_index &lt;- createDataPartition(income$income_above_50K, p = 0.8, list = FALSE)\ntrain &lt;- income[raw_index,]\ntest  &lt;- income[-raw_index, ]\nctrl &lt;- trainControl(method = \"cv\", number = 3)\n\nfit &lt;- train(income_above_50K ~ .,\n            data = train, \n            method = \"rf\",\n            ntree = 50,\n            tuneLength = 3,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n\nRandom Forest \n\n36178 samples\n  104 predictor\n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 24118, 24119, 24119 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n    2   0.3476024  0.4097953  0.2772094\n   53   0.3179101  0.4613186  0.1913666\n  104   0.3202068  0.4549034  0.1912386\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 53.\n\nprint(varImp(fit), 10)\n\nrf variable importance\n\n  only 10 most important variables shown (out of 104)\n\n                                    Overall\n`marital-status_Married-civ-spouse` 100.000\nfnlwgt                               88.391\n`capital-gain`                       72.132\nage                                  65.934\n`education-num`                      65.048\n`hours-per-week`                     38.148\nrelationship_Husband                 23.865\n`capital-loss`                       22.797\n`occupation_Exec-managerial`          8.631\n`occupation_Prof-specialty`           5.868"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#pca",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#pca",
    "title": "Predicting 50K+ Salaries",
    "section": "PCA",
    "text": "PCA\n\nChose Top 8 Features\n\ninc &lt;- income %&gt;%\n  select(-c(fnlwgt,\n            `marital-status_Married-civ-spouse`,\n            age,\n            `capital-gain`,\n            `education-num`,\n            `hours-per-week`,\n            relationship_Husband,\n            `capital-loss`))\n\n#Remained unchanged\npr_income = prcomp(x = inc, scale=T, center = T)\nscreeplot(pr_income, type=\"lines\")\n\n\n\n\n\n\n\nrownames_to_column(as.data.frame(pr_income$rotation)) %&gt;%\n  select(1:11) %&gt;%\n  filter(abs(PC1) &gt;= 0.35 | abs(PC2) &gt;= 0.35 | abs(PC3) &gt;= 0.35 | abs(PC4) &gt;= 0.35 | abs(PC5) &gt;= 0.35 | abs(PC6) &gt;= 0.35 | abs(PC7) &gt;= 0.35 | abs(PC8) &gt;= 0.35 | abs(PC9) &gt;= 0.35 | abs(PC10) &gt;= 0.35)\n\n                       rowname         PC1         PC2          PC3         PC4\n1            workclass_Private -0.17526595 -0.13547253  0.304797799  0.02401763\n2   workclass_Self-emp-not-inc  0.14801244  0.01168218 -0.067006960  0.07757365\n3       education_Some-college -0.06530044  0.06007649  0.046053650 -0.12180499\n4   relationship_Not-in-family -0.11626278  0.09345537 -0.009681895  0.02508551\n5            relationship_Wife -0.07643452  0.10243694 -0.155839801  0.10495583\n6                   race_Black -0.20749199 -0.06347990 -0.071867670 -0.26136856\n7                   sex_Female -0.43938132  0.23499209 -0.149918556  0.12969011\n8                     sex_Male  0.43938132 -0.23499209  0.149918556 -0.12969011\n9 native-country_United-States  0.08714546  0.46368406  0.199639060 -0.22938060\n          PC5          PC6         PC7         PC8         PC9        PC10\n1 -0.20886710  0.441798469 -0.22645965 -0.03914163 -0.04681505  0.00291428\n2  0.14138682 -0.254839748  0.35058106  0.05445338 -0.03103841 -0.33625026\n3 -0.07546707 -0.063088752  0.11632314  0.19569918  0.43973405  0.15884684\n4 -0.12317719 -0.049422098 -0.03404059 -0.57428375  0.20605726 -0.14712496\n5 -0.06292606  0.110184232  0.03508095  0.39257178 -0.25791740  0.12888672\n6  0.37153500 -0.037152844 -0.32702635  0.10727161  0.06843460 -0.15897839\n7 -0.10696874  0.005407197  0.07370032  0.08521373 -0.09415713 -0.03364547\n8  0.10696874 -0.005407197 -0.07370032 -0.08521373  0.09415713  0.03364547\n9  0.15037231  0.067618017 -0.02636173 -0.01036183 -0.06064057 -0.09505529\n\n\n\n\nChose First 10 PCA Features\n\n# IMPORTANT: Since I used 8 features, I updated the prc dataframe to include\n# the features + PCA 1-10\nprc &lt;- \n  bind_cols(select(income, \n                   c(fnlwgt, \n                    `marital-status_Married-civ-spouse`, \n                    age, \n                    `capital-gain`, \n                    age, \n                    `hours-per-week`, \n                    relationship_Husband,\n                    `capital-loss`,\n                    income_above_50K)\n                   ), \n            as.data.frame(pr_income$x)\n            ) %&gt;%\n  select(1:18) %&gt;%\n  ungroup() %&gt;%\n  rename(\"NonBlack_Men\" = PC1,\n         \"US_Women\" = PC2,\n         \"PrivateSec_Men\" = PC3,\n         \"NonUS_NonBlack\" = PC4,\n         \"NonPrivateSec_Black\" = PC5,\n         \"PrivateSec\" = PC6,\n         \"NonBlack_SelfEmploy\" = PC7,\n         \"Wives\" = PC8,\n         \"NonFamily_SomeCollege\" = PC9,\n         \"NotSelfEmployes_NonBlack\" = PC10)\n\nhead(prc)\n\n# A tibble: 6 × 18\n  fnlwgt marital-status_Married-civ-spou…¹   age `capital-gain` `hours-per-week`\n   &lt;dbl&gt;                             &lt;int&gt; &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1 226802                                 0    25              0               40\n2  89814                                 1    38              0               50\n3 336951                                 1    28              0               40\n4 160323                                 1    44           7688               40\n5 198693                                 0    34              0               30\n6 104626                                 1    63           3103               32\n# ℹ abbreviated name: ¹​`marital-status_Married-civ-spouse`\n# ℹ 13 more variables: relationship_Husband &lt;int&gt;, `capital-loss` &lt;dbl&gt;,\n#   income_above_50K &lt;dbl&gt;, NonBlack_Men &lt;dbl&gt;, US_Women &lt;dbl&gt;,\n#   PrivateSec_Men &lt;dbl&gt;, NonUS_NonBlack &lt;dbl&gt;, NonPrivateSec_Black &lt;dbl&gt;,\n#   PrivateSec &lt;dbl&gt;, NonBlack_SelfEmploy &lt;dbl&gt;, Wives &lt;dbl&gt;,\n#   NonFamily_SomeCollege &lt;dbl&gt;, NotSelfEmployes_NonBlack &lt;dbl&gt;"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#gradient-boosting-machine",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#gradient-boosting-machine",
    "title": "Predicting 50K+ Salaries",
    "section": "Gradient Boosting Machine",
    "text": "Gradient Boosting Machine\n\n#IMPORTANT: I took a while and messed around with the hyperparameters\n# Went From 0.2 Kappa to 0.6 Kappa BEFORE updating the features.\n# After updating to the top 8 features + PCA 1-5, it jumped to \n# 0.88 Kappa. Then I added PCA 1-10 and it jumped to 0.95 for the Kappa!\nset.seed(504)\nraw_index &lt;- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain &lt;- prc[raw_index,]\ntest  &lt;- prc[-raw_index, ]\nctrl &lt;- trainControl(method = \"cv\", number = 5)\nweights &lt;- ifelse(income$income_above_50K == 1, 75, 25)\n\nhyperparameters &lt;- expand.grid(interaction.depth = 9, \n                    n.trees = 300, \n                    shrinkage = 0.1, \n                    n.minobsinnode = 4)\nfit &lt;- train(factor(income_above_50K) ~ .,\n            data = train, \n            method = \"gbm\",\n            verbose = FALSE,\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\")\nfit\n\nStochastic Gradient Boosting \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9830284  0.9540703\n\nTuning parameter 'n.trees' was held constant at a value of 300\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 4\n\n\n\nConfusion Matrix For GBM\n\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6797  108\n         1   43 2096\n                                          \n               Accuracy : 0.9833          \n                 95% CI : (0.9804, 0.9858)\n    No Information Rate : 0.7563          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9542          \n                                          \n Mcnemar's Test P-Value : 1.906e-07       \n                                          \n            Sensitivity : 0.9937          \n            Specificity : 0.9510          \n         Pos Pred Value : 0.9844          \n         Neg Pred Value : 0.9799          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7515          \n   Detection Prevalence : 0.7635          \n      Balanced Accuracy : 0.9724          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#logistical-model",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#logistical-model",
    "title": "Predicting 50K+ Salaries",
    "section": "Logistical Model",
    "text": "Logistical Model\n\n#I messed around with using a logistical model\n#It turns out that it's pretty good too! Not as great as the GBM\n#But a great and easy model to explain!\n\nset.seed(504)\nraw_index &lt;- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)\ntrain &lt;- prc[raw_index,]\ntest  &lt;- prc[-raw_index, ]\nctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3, verboseIter = FALSE)\nhyperparameters &lt;- expand.grid(alpha = 1, \n                               lambda = 0.001)\n\nfit &lt;- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"glmnet\",\n            family = \"binomial\",\n            tuneGrid = hyperparameters,\n            trControl = ctrl,\n            metric = \"kappa\",\n            importance = TRUE)\nfit\n\nglmnet \n\n36178 samples\n   17 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 32560, 32561, 32561, 32561, 32559, 32559, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.9610166  0.895033\n\nTuning parameter 'alpha' was held constant at a value of 1\nTuning\n parameter 'lambda' was held constant at a value of 0.001\n\n\n\nConfusion Matrix For Logistical Regression\n\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6679  197\n         1  161 2007\n                                          \n               Accuracy : 0.9604          \n                 95% CI : (0.9562, 0.9643)\n    No Information Rate : 0.7563          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.892           \n                                          \n Mcnemar's Test P-Value : 0.06434         \n                                          \n            Sensitivity : 0.9765          \n            Specificity : 0.9106          \n         Pos Pred Value : 0.9713          \n         Neg Pred Value : 0.9257          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7385          \n   Detection Prevalence : 0.7603          \n      Balanced Accuracy : 0.9435          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#kmeans-clustering",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#kmeans-clustering",
    "title": "Predicting 50K+ Salaries",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\n\nkclust &lt;- kmeans(na.omit(prc), centers = 4)\nkclust$centers\n\n    fnlwgt marital-status_Married-civ-spouse      age capital-gain\n1  86931.3                         0.4798800 39.36223     1178.652\n2 307822.2                         0.4480979 37.29197      993.978\n3 486698.8                         0.4317060 35.22655     1093.921\n4 188848.4                         0.4654970 38.77250     1090.494\n  hours-per-week relationship_Husband capital-loss income_above_50K\n1       41.21239            0.4228618     85.99175        0.2453963\n2       40.99760            0.4015361     87.95284        0.2414497\n3       40.18760            0.3812397     69.68404        0.2144816\n4       40.78357            0.4129092     92.41845        0.2551951\n  NonBlack_Men    US_Women PrivateSec_Men NonUS_NonBlack NonPrivateSec_Black\n1   0.06956303  0.10628238  -0.0890912042    -0.03258891        -0.008490584\n2  -0.04531689 -0.15055893   0.0006150531    -0.02235042         0.014458490\n3  -0.20329023 -0.44646489   0.1964744251     0.04160186         0.115358306\n4  -0.01331648  0.02500494   0.0462143267     0.02882888        -0.010110363\n   PrivateSec NonBlack_SelfEmploy        Wives NonFamily_SomeCollege\n1  0.03384552          0.17173996 -0.029902238           -0.02188199\n2 -0.07527701         -0.12068976 -0.008023897            0.09248884\n3 -0.26626813         -0.36982627  0.123605508            0.13942943\n4  0.03021208         -0.04107775  0.013720687           -0.03450577\n  NotSelfEmployes_NonBlack\n1              -0.03597294\n2               0.01302787\n3              -0.02812971\n4               0.02304227\n\nkclusts &lt;- tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(prc, .x)),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, prc)\n  )\n\nclusterings &lt;- kclusts %&gt;%\n  unnest(glanced, .drop = TRUE)\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line()"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#augumenting-the-gbm-model-with-kmeans-clustering",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#augumenting-the-gbm-model-with-kmeans-clustering",
    "title": "Predicting 50K+ Salaries",
    "section": "Augumenting The GBM Model with KMeans Clustering",
    "text": "Augumenting The GBM Model with KMeans Clustering\n\nprc2 &lt;- augment(kclust, prc)\n\nset.seed(504)\nraw_index &lt;- createDataPartition(prc2$income_above_50K, p = 0.8, list = FALSE)\n\ntrain &lt;- prc2[raw_index,]\ntest  &lt;- prc2[-raw_index, ]\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nhyperparameters &lt;- expand.grid(\n  n.trees = 500,\n  interaction.depth = 5,\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n\n\nfit &lt;- train(factor(income_above_50K)  ~ .,\n            data = train, \n            method = \"gbm\",\n            trControl = ctrl,\n            tuneGrid = hyperparameters,\n            verbose = FALSE)\nfit\n\nStochastic Gradient Boosting \n\n36178 samples\n   18 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 28943, 28943, 28942, 28942, 28942 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9836089  0.9556414\n\nTuning parameter 'n.trees' was held constant at a value of 500\nTuning\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\n\n\n\nConfusion Matrix For KMeans + GBM\n\n#We should be getting a Kappa of 0.9543!\n#Sensitivity = 0.9930, Specificity = 0.9533\n#Excellent Numbers!\nconfusionMatrix(predict(fit, test), factor(test$income_above_50K))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 6798  110\n         1   42 2094\n                                          \n               Accuracy : 0.9832          \n                 95% CI : (0.9803, 0.9857)\n    No Information Rate : 0.7563          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9539          \n                                          \n Mcnemar's Test P-Value : 5.498e-08       \n                                          \n            Sensitivity : 0.9939          \n            Specificity : 0.9501          \n         Pos Pred Value : 0.9841          \n         Neg Pred Value : 0.9803          \n             Prevalence : 0.7563          \n         Detection Rate : 0.7517          \n   Detection Prevalence : 0.7638          \n      Balanced Accuracy : 0.9720          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "content/projects/predicting-salaries-machine-learning/index.html#results",
    "href": "content/projects/predicting-salaries-machine-learning/index.html#results",
    "title": "Predicting 50K+ Salaries",
    "section": "Results",
    "text": "Results\nOur analysis began with a random forest model to pinpoint critical factors, focusing on the top 8 features for in-depth examination. We then applied principal component analysis for further insight. The culmination of our work was a gradient boosting machine model, finely tuned for peak performance, which boasted a remarkable accuracy rate of 98.3% and a Kappa score of 95.4%. To validate our model’s reliability and guard against overfitting, we compared it with a basic logistic regression model, which showed a Kappa score of 89.5% and an accuracy of 96.1%. Enhancements were made by integrating Kmeans clustering, an unsupervised learning technique, pushing the Kappa score slightly higher to 95.4% and accuracy to 98.3%. Our methodology, combining feature selection and PCA, proved highly effective, offering a solid foundation for future projects with potential for even finer adjustments."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html",
    "title": "Enhanced Customer Return Forecasting",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I developed a Random Forest model with repeated 5-fold cross-validation (3 repeats) to accurately predict customer product returns, achieving 97.8% accuracy, a Kappa of 0.95, and an AUC of 0.98—significantly outperforming the baseline (AUC=0.625). He executed comprehensive exploratory data analysis, engineered strategic features like customer demographics, product price sensitivity, and seasonal trends, and created an interactive dashboard for actionable decision-making. This model greatly enhances profitability and customer satisfaction by proactively identifying potential returns."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#project-overview",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#project-overview",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Project Overview",
    "text": "Project Overview\nThis analysis aimed to predict customer product returns, leveraging detailed product characteristics and customer demographics. The objective was to proactively identify returns, enabling strategic interventions to enhance profitability and customer satisfaction."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#key-achievements",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#key-achievements",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Key Achievements",
    "text": "Key Achievements\n\nSignificant improvement in predictive performance:\n\nIncreased model accuracy from a moderate AUC of 0.625 (initial baseline) to a robust AUC of 0.98.\nEnhanced overall accuracy to 97.8% with a Kappa statistic of 0.95."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#exploratory-data-analysis",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#exploratory-data-analysis",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWe then generated the following plots to visualize return behavior in more depth:\n\nReturned Orders by Department\n\nReturned Orders by Product Size\n\nReturned Orders by State and Region\n\nState Contribution to Total Returns (Lollipop Chart)\n\nStandardized Returns by State (Per 100k Population)"
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#strategic-feature-engineering",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#strategic-feature-engineering",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Strategic Feature Engineering",
    "text": "Strategic Feature Engineering\nTo drive predictive accuracy, we enriched our dataset through targeted enhancements:\n\nIntegrated regional demographics and population data, offering context-sensitive insights.\nEngineered impactful features such as:\n\nCustomerAge: Capturing age-driven buying behaviors.\nPriceRange: Classifying products by price sensitivity.\nSeasonality: Reflecting seasonal purchasing patterns.\n\nStandardized returns per 100,000 population, enabling fair comparisons across regions."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#data-driven-insights",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#data-driven-insights",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Data-Driven Insights",
    "text": "Data-Driven Insights\nVisual analyses identified crucial return patterns, directly informing strategic business decisions:\n\nDepartments and Product Sizes:\n\nIdentified product categories with disproportionately high return rates, suggesting areas for targeted product reviews.\n\nGeographic Analysis:\n\nStates and regions contributing most to return volumes were visualized through comprehensive charts, including state-level standardized returns and overall contribution metrics, facilitating focused regional strategies."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#model-development-and-performance",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#model-development-and-performance",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Model Development and Performance",
    "text": "Model Development and Performance\nBuilding upon insights from an initial logistic regression and baseline Random Forest model, we:\n\nImplemented an advanced Random Forest model with repeated 5-fold cross-validation (3 repeats), enhancing stability and predictive consistency.\nAchieved exceptional performance metrics on validation data:\n\n\n\n\nMetric\nPerformance\n\n\n\n\nAccuracy\n97.8%\n\n\nKappa\n0.95\n\n\nAUC\n0.98\n\n\n\nConfusion Matrix:\n               Actual\nPredicted     No     Yes\nNo           8202     80\nYes           205   4495\nThe refined model substantially reduced false predictions, significantly outperforming previous benchmarks."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#actionable-final-predictions",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#actionable-final-predictions",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Actionable Final Predictions",
    "text": "Actionable Final Predictions\nApplied to unseen test data, the refined model produced actionable predictions by:\n\nGenerating clear, binary indicators (Predicted_Return), directly supporting decision-making.\nProviding stakeholders with an interactive dashboard featuring real-time filtering, sorting, and export functionalities to quickly identify and act upon at-risk purchases."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#conclusions-strategic-next-steps",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#conclusions-strategic-next-steps",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Conclusions & Strategic Next Steps",
    "text": "Conclusions & Strategic Next Steps\n\nHistorical context: Initial model (AUC = 0.625) established the importance of detailed feature engineering.\nCurrent approach: Advanced techniques boosted predictive accuracy to 97.8%, demonstrating substantial business value through proactive return management.\nFuture opportunities:\n\nExplore alternative ensemble methods for potential incremental gains.\nInvestigate cost-sensitive learning approaches to optimize financial outcomes related to returns.\nFurther incorporate advanced explainability tools to enhance stakeholder understanding and strategic alignment.\n\n\nThis enhanced predictive framework positions the business to significantly reduce return-related losses, optimize inventory management, and deliver improved customer experiences."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#appendix",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#appendix",
    "title": "Enhanced Customer Return Forecasting",
    "section": "Appendix",
    "text": "Appendix\nBelow is a summary of the key steps and full script to reproduce the results:\n\nLoading and Merging Datasets: Combined region and population data with the original training and test CSVs.\nFeature Engineering: Created CustomerAge, PriceRange, and other derived features, and removed irrelevant columns.\nModel Training:\n\nDefined a Random Forest approach with repeated 5‑fold cross‑validation.\n\nTuned hyperparameters (mtry, number of trees, etc.) to maximize the ROC metric.\n\nEvaluation:\n\nCalculated Accuracy, Kappa, AUC, and confusion matrix results on the validation set.\n\nProduced plots for confusion matrix and ROC curve, verifying high accuracy and minimal misclassification.\n\nTest Predictions:\n\nGenerated final predictions (Yes/No) for the unseen test data.\n\nCreated an interactive DT table for user-friendly inspection."
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#a.-package-loading",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#a.-package-loading",
    "title": "Enhanced Customer Return Forecasting",
    "section": "A. Package Loading",
    "text": "A. Package Loading\n\n# A.1 -- Load Required Packages\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(gbm)\nlibrary(nnet)\nlibrary(lightgbm)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(forcats)\nlibrary(DT)\nlibrary(htmltools)\nlibrary(pROC)"
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#b.-data-loading-and-merging",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#b.-data-loading-and-merging",
    "title": "Enhanced Customer Return Forecasting",
    "section": "B. Data Loading and Merging",
    "text": "B. Data Loading and Merging\n\n# A.2 -- Read CSVs, Merge Region & Population\ntrain &lt;- read_csv(\"../../../assets/datasets/customerReturnTrain.csv\") \ntest  &lt;- read_csv(\"../../../assets/datasets/customerReturnTest.csv\") \n\nstate_regions &lt;- tibble::tribble(\n  ~CustomerState,    ~Region,\n  # Pacific Northwest\n  \"Washington\",      \"Pacific Northwest\",\n  \"Oregon\",          \"Pacific Northwest\",\n  \"Idaho\",           \"Pacific Northwest\",\n  # Southwest\n  \"Arizona\",         \"Southwest\",\n  \"California\",      \"Southwest\",\n  \"Nevada\",          \"Southwest\",\n  \"New Mexico\",      \"Southwest\",\n  \"Utah\",            \"Southwest\",\n  # Rocky Mountain\n  \"Colorado\",        \"Rocky Mountain\",\n  \"Montana\",         \"Rocky Mountain\",\n  \"Wyoming\",         \"Rocky Mountain\",\n  \"Alaska\",          \"Rocky Mountain\",\n  \"Hawaii\",          \"Rocky Mountain\",\n  # Midwest\n  \"Illinois\",        \"Midwest\",\n  \"Indiana\",         \"Midwest\",\n  \"Iowa\",            \"Midwest\",\n  \"Kansas\",          \"Midwest\",\n  \"Michigan\",        \"Midwest\",\n  \"Minnesota\",       \"Midwest\",\n  \"Missouri\",        \"Midwest\",\n  \"Nebraska\",        \"Midwest\",\n  \"North Dakota\",    \"Midwest\",\n  \"Ohio\",            \"Midwest\",\n  \"South Dakota\",    \"Midwest\",\n  \"Wisconsin\",       \"Midwest\",\n  # Southeast\n  \"Alabama\",         \"Southeast\",\n  \"Arkansas\",        \"Southeast\",\n  \"Florida\",         \"Southeast\",\n  \"Georgia\",         \"Southeast\",\n  \"Kentucky\",        \"Southeast\",\n  \"Louisiana\",       \"Southeast\",\n  \"Mississippi\",     \"Southeast\",\n  \"North Carolina\",  \"Southeast\",\n  \"South Carolina\",  \"Southeast\",\n  \"Tennessee\",       \"Southeast\",\n  \"Virginia\",        \"Southeast\",\n  \"West Virginia\",   \"Southeast\",\n  \"Oklahoma\",        \"Southeast\",\n  \"Texas\",           \"Southeast\",\n  # Northeast\n  \"Connecticut\",     \"Northeast\",\n  \"Delaware\",        \"Northeast\",\n  \"Maine\",           \"Northeast\",\n  \"Maryland\",        \"Northeast\",\n  \"Massachusetts\",   \"Northeast\",\n  \"New Hampshire\",   \"Northeast\",\n  \"New Jersey\",      \"Northeast\",\n  \"New York\",        \"Northeast\",\n  \"Pennsylvania\",    \"Northeast\",\n  \"Rhode Island\",    \"Northeast\",\n  \"Vermont\",         \"Northeast\",\n  \"DC\",              \"Northeast\"\n)\nstate_populations &lt;- tribble(\n  ~CustomerState,     ~Population,\n  \"Alabama\",          5024279,\n  \"Alaska\",           733391,\n  \"Arizona\",          7151502,\n  \"Arkansas\",         3011524,\n  \"California\",       39538223,\n  \"Colorado\",         5773714,\n  \"Connecticut\",      3605944,\n  \"Delaware\",         989948,\n  \"DC\",               689545,\n  \"Florida\",          21538187,\n  \"Georgia\",          10711908,\n  \"Hawaii\",           1455271,\n  \"Idaho\",            1839106,\n  \"Illinois\",         12812508,\n  \"Indiana\",          6785528,\n  \"Iowa\",             3190369,\n  \"Kansas\",           2937880,\n  \"Kentucky\",         4505836,\n  \"Louisiana\",        4657757,\n  \"Maine\",            1362359,\n  \"Maryland\",         6177224,\n  \"Massachusetts\",    7029917,\n  \"Michigan\",         10077331,\n  \"Minnesota\",        5706494,\n  \"Mississippi\",      2961279,\n  \"Missouri\",         6154913,\n  \"Montana\",          1084225,\n  \"Nebraska\",         1961504,\n  \"Nevada\",           3104614,\n  \"New Hampshire\",    1377529,\n  \"New Jersey\",       9288994,\n  \"New Mexico\",       2117522,\n  \"New York\",         20201249,\n  \"North Carolina\",   10439388,\n  \"North Dakota\",     779094,\n  \"Ohio\",             11799448,\n  \"Oklahoma\",         3959353,\n  \"Oregon\",           4237256,\n  \"Pennsylvania\",     13002700,\n  \"Rhode Island\",     1097379,\n  \"South Carolina\",   5118425,\n  \"South Dakota\",     886667,\n  \"Tennessee\",        6910840,\n  \"Texas\",            29145505,\n  \"Utah\",             3271616,\n  \"Vermont\",          643077,\n  \"Virginia\",         8631393,\n  \"Washington\",       7705281,\n  \"West Virginia\",    1793716,\n  \"Wisconsin\",        5893718,\n  \"Wyoming\",          576851\n)\n\n# Merge region and population\ntrain &lt;- train %&gt;% \n  left_join(state_regions, by = \"CustomerState\") %&gt;% \n  left_join(state_populations, by = \"CustomerState\")\n\ntest &lt;- test %&gt;% \n  left_join(state_regions, by = \"CustomerState\") %&gt;% \n  left_join(state_populations, by = \"CustomerState\")\n\nglimpse(train)\n\nRows: 64,912\nColumns: 14\n$ ID                &lt;chr&gt; \"58334388-e72d-40d3-afcf-59561c262e86\", \"fb73c186-ca…\n$ OrderID           &lt;chr&gt; \"4fc2f4ea-7098-4e9d-87b1-52b6a9ee21fd\", \"4fc2f4ea-70…\n$ CustomerID        &lt;chr&gt; \"c401d50e-37b7-45ea-801a-d71c13ea6387\", \"c401d50e-37…\n$ CustomerState     &lt;chr&gt; \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Kentucky\", \"Ind…\n$ CustomerBirthDate &lt;date&gt; 1967-01-06, 1967-01-06, 1967-01-06, 1967-01-06, 197…\n$ OrderDate         &lt;date&gt; 2016-01-06, 2016-01-06, 2016-01-06, 2016-01-06, 201…\n$ ProductDepartment &lt;chr&gt; \"Youth\", \"Mens\", \"Mens\", \"Mens\", \"Womens\", \"Womens\",…\n$ ProductSize       &lt;chr&gt; \"M\", \"L\", \"XL\", \"L\", \"XS\", \"M\", \"XS\", \"M\", \"M\", \"M\",…\n$ ProductCost       &lt;dbl&gt; 9, 17, 20, 17, 42, 39, 13, 3, 12, 27, 20, 23, 49, 16…\n$ DiscountPct       &lt;dbl&gt; 0.0356, 0.1192, 0.1698, 0.1973, 0.0663, 0.0501, 0.08…\n$ PurchasePrice     &lt;dbl&gt; 28.93, 44.92, 48.98, 51.37, 113.91, 121.59, 41.40, 1…\n$ Returned          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1…\n$ Region            &lt;chr&gt; \"Southeast\", \"Southeast\", \"Southeast\", \"Southeast\", …\n$ Population        &lt;dbl&gt; 4505836, 4505836, 4505836, 4505836, 6785528, 6785528…\n\nglimpse(test)\n\nRows: 14,809\nColumns: 14\n$ ID                &lt;chr&gt; \"a6f6ecd9-2c08-4363-baf7-b54adc35d486\", \"e819be63-7a…\n$ OrderID           &lt;chr&gt; \"ad944d45-a857-4156-ac31-6d0eeb8a4bb7\", \"cefde178-45…\n$ CustomerID        &lt;chr&gt; \"49d38db8-e5f6-45a6-bbc1-6c6ed1f0214d\", \"65d3e42c-15…\n$ CustomerState     &lt;chr&gt; \"South Carolina\", \"California\", \"Indiana\", \"Indiana\"…\n$ CustomerBirthDate &lt;date&gt; 1976-10-21, 1961-10-31, 1953-11-03, 1953-11-03, 196…\n$ OrderDate         &lt;date&gt; 2021-01-01, 2021-01-01, 2021-01-01, 2021-01-01, 202…\n$ ProductDepartment &lt;chr&gt; \"Accessories\", \"Mens\", \"Womens\", \"Womens\", \"Accessor…\n$ ProductSize       &lt;chr&gt; \"~\", \"M\", \"XXL\", \"XXL\", \"~\", \"L\", \"M\", \"S\", \"L\", \"L\"…\n$ ProductCost       &lt;dbl&gt; 19, 28, 21, 23, 13, 26, 20, 28, 31, 23, 25, 13, 19, …\n$ DiscountPct       &lt;dbl&gt; 0.2414, 0.2771, 0.2665, 0.2305, 0.2621, 0.3081, 0.01…\n$ PurchasePrice     &lt;dbl&gt; 34.14, 73.74, 35.21, 70.79, 30.99, 51.89, 58.36, 73.…\n$ Returned          &lt;chr&gt; \"null\", \"null\", \"null\", \"null\", \"null\", \"null\", \"nul…\n$ Region            &lt;chr&gt; \"Southeast\", \"Southwest\", \"Midwest\", \"Midwest\", \"Pac…\n$ Population        &lt;dbl&gt; 5118425, 39538223, 6785528, 6785528, 4237256, 327161…"
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#c.-exploratory-data-analysis-plot-saving",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#c.-exploratory-data-analysis-plot-saving",
    "title": "Enhanced Customer Return Forecasting",
    "section": "C. Exploratory Data Analysis & Plot Saving",
    "text": "C. Exploratory Data Analysis & Plot Saving\n\n# A.3 -- Exploratory Plots\n# We'll save each plot to a local image file (e.g. .png), which the report can reference.\n\n# 1. Returned Orders by Department\nReturnedOrdersByDept &lt;- ggplot(\n  data = train %&gt;% filter(Returned == 1),\n  aes(x = fct_infreq(ProductDepartment))\n) +\n  geom_bar(fill = \"#1B9E77\") +\n  labs(\n    title = \"Returned Orders by Department\",\n    subtitle = \"Frequency of Returns Across Product Departments\",\n    x = \"Product Department\",\n    y = \"Number of Returns\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(color = \"gray20\")\n  )\nggsave(\"ReturnedOrdersByDept.png\", plot = ReturnedOrdersByDept, width = 10, height = 10)\n\n# 2. Returned Orders by Product Size\nReturnedOrdersBySize &lt;- ggplot(\n  data = train %&gt;% filter(Returned == 1),\n  aes(x = fct_infreq(ProductSize))\n) +\n  geom_bar(fill = \"#EFC000FF\") +\n  labs(\n    title = \"Returned Orders by Product Size\",\n    subtitle = \"Distribution of Returns Across Product Sizes\",\n    x = \"Product Size\",\n    y = \"Number of Returns\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(color = \"gray20\")\n  )\nggsave(\"ReturnedOrdersBySize.png\", plot = ReturnedOrdersBySize, width = 10, height = 10)\n\n# 3. Returned Orders by State & Region\nReturnedOrdersByStateRegion &lt;- ggplot(\n  data = train %&gt;% filter(Returned == 1),\n  aes(x = fct_infreq(CustomerState))\n) +\n  geom_bar(fill = \"#7570B3\") +\n  coord_flip() +\n  facet_wrap(~ Region, scales = \"free_y\") +\n  labs(\n    title = \"Returned Orders by State and Region\",\n    subtitle = \"Count of Returns Faceted by U.S. Region\",\n    x = \"State\",\n    y = \"Number of Returns\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    strip.background = element_rect(fill = \"#D95F02\", color = NA),\n    strip.text = element_text(face = \"bold\", color = \"white\"),\n    plot.title = element_text(face = \"bold\")\n  )\nggsave(\"ReturnedOrdersByStateRegion.png\", plot = ReturnedOrdersByStateRegion, width = 10, height = 10)\n\n# 4. State Contribution (Lollipop)\ntrain_returns_by_state &lt;- train %&gt;%\n  filter(Returned == 1) %&gt;%            \n  group_by(CustomerState) %&gt;%\n  summarise(n_returns = n()) %&gt;%       \n  ungroup() %&gt;%\n  mutate(pct_of_all_returns = 100 * n_returns / sum(n_returns))\n\nStateContribution &lt;- ggplot(train_returns_by_state, \n  aes(x = fct_reorder(CustomerState, pct_of_all_returns), y = pct_of_all_returns)\n) +\n  geom_segment(aes(xend = CustomerState, y = 0, yend = pct_of_all_returns),\n               color = \"#66A61E\", size = 1) +\n  geom_point(color = \"#66A61E\", size = 3) +\n  coord_flip() +\n  labs(\n    title = \"State Contribution to Total Returns\",\n    subtitle = \"Percentage Share of Overall Returned Orders\",\n    x = \"State\",\n    y = \"Share (%)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(color = \"gray30\")\n  )\nggsave(\"StateContribution.png\", plot = StateContribution, width = 10, height = 10)\n\n# 5. Standardized Returns by State\ntrain_returns_by_state_pop &lt;- train %&gt;%\n  filter(Returned == 1) %&gt;%\n  group_by(CustomerState) %&gt;%\n  summarise(n_returns = n(), Population = first(Population)) %&gt;% \n  ungroup() %&gt;%\n  mutate(returns_per_100k = (n_returns / Population) * 100000)\n\nStandardizedReturnsByState &lt;- ggplot(\n  train_returns_by_state_pop,\n  aes(x = fct_reorder(CustomerState, returns_per_100k), y = returns_per_100k)\n) +\n  geom_segment(aes(xend = CustomerState, y = 0, yend = returns_per_100k),\n               color = \"cyan4\", size = 0.4) +\n  geom_text(aes(label = sprintf(\"%.2f\", returns_per_100k)),\n            color = \"black\", hjust = -0.3, size = 3) +\n  coord_flip() +\n  scale_y_continuous(breaks = c(0, 2.5, 5, 7.5, 10)) +\n  labs(\n    title = \"Standardized Returns by State\",\n    subtitle = \"Returns per 100,000 People\",\n    x = \"Customer State\",\n    y = \"Returns per 100k\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(color = \"gray30\")\n  )\nggsave(\"StandardizedReturnsByState.png\", plot = StandardizedReturnsByState, width = 10, height = 10)"
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#d.-feature-engineering-functions",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#d.-feature-engineering-functions",
    "title": "Enhanced Customer Return Forecasting",
    "section": "D. Feature Engineering Functions",
    "text": "D. Feature Engineering Functions\n\n# A.4 -- Feature Engineering and Dummy-Creation\nbuild_features &lt;- function(df) {\n  df &lt;- df %&gt;%\n    mutate(\n      OrderDate = as.Date(OrderDate),\n      CustomerBirthDate = as.Date(CustomerBirthDate),\n      CustomerAge = floor(as.numeric(OrderDate - CustomerBirthDate) / 365),\n      Returned = if(\"Returned\" %in% names(df)) {\n        if(is.numeric(Returned)) {\n          factor(Returned, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n        } else {\n          factor(trimws(Returned) %&gt;% recode(\"0\" = \"No\", \"1\" = \"Yes\", \"no\" = \"No\", \"yes\" = \"Yes\"),\n                 levels = c(\"No\", \"Yes\"))\n        }\n      } else {\n        Returned\n      },\n      ProductDepartment = as.factor(ProductDepartment),\n      ProductSize = as.factor(ProductSize),\n      OrderYear = as.character(year(OrderDate)),\n      OrderMonth = as.character(month(OrderDate)),\n      OrderDayOfWeek = weekdays(OrderDate),\n      Season = case_when(\n        month(OrderDate) %in% c(12, 1, 2) ~ \"Winter\",\n        month(OrderDate) %in% c(3, 4, 5) ~ \"Spring\",\n        month(OrderDate) %in% c(6, 7, 8) ~ \"Summer\",\n        month(OrderDate) %in% c(9, 10, 11) ~ \"Fall\"\n      ),\n      MSRP = round(PurchasePrice / (1 - DiscountPct), 2),\n      PriceRange = case_when(\n        MSRP &gt;= 13 & MSRP &lt;= 30 ~ \"$13-$30\",\n        MSRP &gt; 30 & MSRP &lt;= 60 ~ \"$31-$60\",\n        MSRP &gt; 60 & MSRP &lt;= 100 ~ \"$61-$100\",\n        MSRP &gt; 100 ~ \"&gt;$100\",\n        TRUE ~ \"Other\"\n      ),\n      DiscountAmount = PurchasePrice * DiscountPct,\n      CustomerAgeGroup = cut(CustomerAge,\n        breaks = c(0, 30, 45, 60, Inf),\n        labels = c(\"18-30\", \"31-45\", \"46-60\", \"&gt;60\"),\n        right = FALSE\n      )\n    ) %&gt;%\n    select(-OrderDate, -CustomerBirthDate, -CustomerState)\n  return(df)\n}\n\nremove_ids &lt;- function(df) {\n  drop_cols &lt;- c(\"ID\", \"OrderID\", \"CustomerID\")\n  df &lt;- df %&gt;% select(-one_of(drop_cols))\n  return(df)\n}\n\nmake_dummies &lt;- function(df, outcome = \"Returned\") {\n  outcome_vec &lt;- NULL\n  if(outcome %in% names(df)) {\n    if(all(is.na(df[[outcome]]))) {\n      df &lt;- df %&gt;% select(-all_of(outcome))\n    } else {\n      outcome_vec &lt;- df[[outcome]]\n      df &lt;- df %&gt;% select(-all_of(outcome))\n    }\n  }\n  df &lt;- df[, sapply(df, function(x) length(unique(na.omit(x))) &gt;= 2), drop = FALSE]\n  \n  dmy &lt;- caret::dummyVars(\"~ .\", data = df, fullRank = TRUE)\n  df_dummy &lt;- as.data.frame(predict(dmy, newdata = df))\n  if(!is.null(outcome_vec)) {\n    df_dummy[[outcome]] &lt;- outcome_vec\n  }\n  return(df_dummy)\n}\n\nalign_columns &lt;- function(train_df, test_df, outcome = \"Returned\") {\n  train_predictors &lt;- setdiff(names(train_df), outcome)\n  test_predictors  &lt;- names(test_df)\n  \n  missing_in_test &lt;- setdiff(train_predictors, test_predictors)\n  if(length(missing_in_test) &gt; 0) {\n    for(col in missing_in_test) {\n      test_df[[col]] &lt;- 0\n    }\n  }\n  \n  extra_in_test &lt;- setdiff(test_predictors, train_predictors)\n  if(length(extra_in_test) &gt; 0) {\n    test_df &lt;- test_df %&gt;% select(-one_of(extra_in_test))\n  }\n  \n  test_df &lt;- test_df[, train_predictors, drop = FALSE]\n  return(list(train = train_df, test = test_df))\n}"
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#e.-creating-final-training-testing-sets",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#e.-creating-final-training-testing-sets",
    "title": "Enhanced Customer Return Forecasting",
    "section": "E. Creating Final Training & Testing Sets",
    "text": "E. Creating Final Training & Testing Sets\n\n# A.5 -- Creating the Feature-Engineered Datasets\n\ntrain_path &lt;- file.path(\"../../../assets/datasets\", \"customerReturnTrain.csv\")\ntest_path  &lt;- file.path(\"../../../assets/datasets\", \"customerReturnTest.csv\")\n\ntrain_df &lt;- read.csv(train_path, stringsAsFactors = FALSE)\ntest_df  &lt;- read.csv(test_path, stringsAsFactors = FALSE)\n\ntrain_with_feat &lt;- build_features(train_df) %&gt;% remove_ids()\ntest_with_feat  &lt;- build_features(test_df) %&gt;% remove_ids()\n\ntrain_with_feat_dummies &lt;- make_dummies(train_with_feat, outcome = \"Returned\")\ntest_with_feat_dummies  &lt;- make_dummies(test_with_feat, outcome = \"Returned\")\n\naligned_with_feat  &lt;- align_columns(train_with_feat_dummies, test_with_feat_dummies, outcome = \"Returned\")\ntrain_with_feat_final &lt;- aligned_with_feat$train\ntest_with_feat_final  &lt;- aligned_with_feat$test\n\ncat(\"WITH Features set dimensions:\", dim(train_with_feat_final), dim(test_with_feat_final), \"\\n\")\n\nWITH Features set dimensions: 64912 47 14809 46 \n\ncat(\"Outcome distribution (WITH Features):\\n\")\n\nOutcome distribution (WITH Features):\n\nprint(table(train_with_feat_final$Returned))\n\n\n   No   Yes \n42035 22877"
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#f.-modeling-random-forest-validation-and-metrics",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#f.-modeling-random-forest-validation-and-metrics",
    "title": "Enhanced Customer Return Forecasting",
    "section": "F. Modeling (Random Forest), Validation, and Metrics",
    "text": "F. Modeling (Random Forest), Validation, and Metrics\n\n# A.6 -- Random Forest Training and Validation\n\nset.seed(123)\ntrainIndex_fe &lt;- caret::createDataPartition(train_with_feat_final$Returned, p = 0.8, list = FALSE)\ntrainData_fe  &lt;- train_with_feat_final[trainIndex_fe, ]\nvalData_fe    &lt;- train_with_feat_final[-trainIndex_fe, ]\n\nctrl &lt;- caret::trainControl(\n  method = \"repeatedcv\",\n  number = 5,\n  repeats = 3,\n  sampling = \"up\",\n  classProbs = TRUE,\n  summaryFunction = caret::twoClassSummary\n)\n\nrf_model &lt;- caret::train(\n  Returned ~ .,\n  data = train_with_feat_final,\n  method = \"rf\",\n  ntree = 50,\n  tuneLength = 3,\n  metric = \"ROC\",\n  trControl = ctrl\n)\nprint(rf_model)\n\nRandom Forest \n\n64912 samples\n   46 predictor\n    2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 51930, 51929, 51930, 51930, 51929, 51929, ... \nAddtional sampling using up-sampling\n\nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.5956602  0.4549542  0.6748552\n  24    0.6374951  0.7659649  0.4665528\n  46    0.6369169  0.7613972  0.4673836\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 24.\n\npred_probs_fe &lt;- predict(rf_model, newdata = valData_fe, type = \"prob\")[, \"Yes\"]\npred_class_fe &lt;- ifelse(pred_probs_fe &gt;= 0.5, \"Yes\", \"No\")\n\ncm_fe &lt;- caret::confusionMatrix(\n  factor(pred_class_fe, levels = c(\"No\", \"Yes\")),\n  valData_fe$Returned\n)\nprint(cm_fe)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  8202   87\n       Yes  205 4488\n                                        \n               Accuracy : 0.9775        \n                 95% CI : (0.9748, 0.98)\n    No Information Rate : 0.6476        \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16     \n                                        \n                  Kappa : 0.951         \n                                        \n Mcnemar's Test P-Value : 7.546e-12     \n                                        \n            Sensitivity : 0.9756        \n            Specificity : 0.9810        \n         Pos Pred Value : 0.9895        \n         Neg Pred Value : 0.9563        \n             Prevalence : 0.6476        \n         Detection Rate : 0.6318        \n   Detection Prevalence : 0.6385        \n      Balanced Accuracy : 0.9783        \n                                        \n       'Positive' Class : No            \n                                        \n\n\n\n# A.7 -- Confusion Matrix Plot & ROC\ncm_df &lt;- as.data.frame(cm_fe$table)\n\n# Plot Confusion Matrix\nConfMatPlot &lt;- ggplot(cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 5, color = \"black\") +\n  scale_fill_gradient(low = \"#D6EAF8\", high = \"#154360\") +\n  labs(\n    title = \"Confusion Matrix\",\n    x = \"Predicted Class\",\n    y = \"Actual Class\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\nggsave(\"ConfMatrixRF.png\", plot = ConfMatPlot, width = 8, height = 6)\n\n# ROC Curve\nroc_obj &lt;- pROC::roc(\n  response  = valData_fe$Returned,\n  predictor = pred_probs_fe,\n  levels    = c(\"No\", \"Yes\")\n)\nplot(roc_obj, col = \"#1B9E77\", lwd = 2, main = \"ROC Curve (Random Forest)\")\nauc_val &lt;- pROC::auc(roc_obj)\nlegend(\"bottomright\", legend = sprintf(\"AUC = %.3f\", auc_val), bty = \"n\")\n\n\n\n\n\n\n\n\n(The confusion matrix plot is also saved as ConfMatrixRF.png.)"
  },
  {
    "objectID": "content/projects/predicting-customer-returns-machine-learning/index.html#g.-final-test-predictions-interactive-table",
    "href": "content/projects/predicting-customer-returns-machine-learning/index.html#g.-final-test-predictions-interactive-table",
    "title": "Enhanced Customer Return Forecasting",
    "section": "G. Final Test Predictions & Interactive Table",
    "text": "G. Final Test Predictions & Interactive Table\n\n# A.8 -- Final Test Predictions\npred_probs_test &lt;- predict(rf_model, newdata = test_with_feat_final, type = \"prob\")[, \"Yes\"]\npred_class_test &lt;- ifelse(pred_probs_test &gt;= 0.5, \"Yes\", \"No\")\n\ntest_df$Predicted_Return &lt;- pred_class_test\nhead(test_df)\n\n                                    ID                              OrderID\n1 a6f6ecd9-2c08-4363-baf7-b54adc35d486 ad944d45-a857-4156-ac31-6d0eeb8a4bb7\n2 e819be63-7a98-4e6d-b217-03f9ac8c1d03 cefde178-45a0-406f-becf-31c003430d6f\n3 8936a1c6-f5eb-4c78-9636-e693aae49d9f 24d7df06-80b2-416d-85e7-0ce8bd442b3f\n4 68b74b1d-deab-4d93-bfe8-859d450952ef 24d7df06-80b2-416d-85e7-0ce8bd442b3f\n5 657abc10-0b36-49df-b3ae-a1a6b9d1d145 086b01d2-8ab8-424d-8494-284cea56ae92\n6 233cb816-7565-46db-a82d-b638cfc0a22f 0006d57d-bfdb-4d6c-91e9-5c631dcb0172\n                            CustomerID  CustomerState CustomerBirthDate\n1 49d38db8-e5f6-45a6-bbc1-6c6ed1f0214d South Carolina        1976-10-21\n2 65d3e42c-158f-4104-8dd2-cd8d1379ecf1     California        1961-10-31\n3 5341a19e-27dd-42f9-8f8d-bbb76df99e71        Indiana        1953-11-03\n4 5341a19e-27dd-42f9-8f8d-bbb76df99e71        Indiana        1953-11-03\n5 9efd2c6d-fa30-442a-a99a-d5bb2f284bf6         Oregon        1966-01-10\n6 6e3ce858-0f62-4fd5-a960-91c536aad53f           Utah        1970-07-31\n   OrderDate ProductDepartment ProductSize ProductCost DiscountPct\n1 2021-01-01       Accessories           ~          19      0.2414\n2 2021-01-01              Mens           M          28      0.2771\n3 2021-01-01            Womens         XXL          21      0.2665\n4 2021-01-01            Womens         XXL          23      0.2305\n5 2021-01-01       Accessories           ~          13      0.2621\n6 2021-01-01              Mens           L          26      0.3081\n  PurchasePrice Returned Predicted_Return\n1         34.14     null               No\n2         73.74     null               No\n3         35.21     null               No\n4         70.79     null              Yes\n5         30.99     null               No\n6         51.89     null               No\n\n# Show an interactive DT table with 'Predicted_Return' first\ntest_df &lt;- test_df[, c(\"Predicted_Return\", setdiff(names(test_df), \"Predicted_Return\"))]\n\ncaption_text &lt;- htmltools::tags$caption(\n  style = 'caption-side: top; text-align: center; font-size: 16px; font-weight: bold; color: #2E86C1;',\n  \"Random Forest Model Performance (~97.8% Accuracy, Kappa ~0.95) | Test Dataset\"\n)\n\nDT::datatable(\n  test_df,\n  filter = \"top\",\n  rownames = FALSE,\n  caption = caption_text,\n  extensions = 'Buttons',\n  options = list(\n    dom = 'Blfrtip',\n    buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),\n    pageLength = 10,\n    autoWidth = TRUE,\n    initComplete = JS(\n      \"function(settings, json) {\",\n      \"$(this.api().table().header()).css({'background-color': '#4CAF50','color': '#fff','font-size': '14px'});\",\n      \"}\"\n    )\n  )\n) %&gt;%\n  formatStyle(\n    columns = names(test_df),\n    fontSize = '12px',\n    color = '#333',\n    backgroundColor = styleInterval(0, c('white', '#F8F8F8'))\n  ) %&gt;%\n  formatStyle(\n    \"Predicted_Return\",\n    backgroundColor = styleEqual(c(\"Yes\", \"No\"), c(\"#c6efce\", \"#ffc7ce\")),\n    fontWeight = \"bold\"\n  )"
  },
  {
    "objectID": "content/projects/index.html",
    "href": "content/projects/index.html",
    "title": "Statistical & Data Science Portfolio",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I’ve completed various projects applying advanced statistical and machine learning techniques to solve practical problems. Notably, I leveraged MANOVA and classification methods to uncover significant predictors of wine type and quality, analyzed U.S. healthcare spending trends from 1980-2014 to identify factors behind rising costs, and developed a Random Forest model achieving high accuracy to classify patient symptom severity. Additionally, I’ve created interactive dashboards—including a Quarto dashboard visualizing 100m sprint results and Shiny apps analyzing UK accident data and Yelp spatial reviews—and delivered machine learning models with measurable outcomes, such as a 56% performance increase in forecasting customer returns and accurate classification of wine origins and income brackets exceeding $50K.\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Statistics\n  \n    \n      \n        \n        \n          Read More\n          \n          \n          \n          \n          \n            Applied Multivariate Analysis\n            Bringing modern stats to uncover wine attribute patterns and differences.\n            Exploring wine attributes through EDA, MANOVA, and classification models reveals significant chemical differences and predictors of wine type and quality.\n          \n          \n          \n            Read More &gt;\n          \n        \n        \n      \n    \n      \n    \n      \n        \n        \n          Read More\n          \n          \n          \n          \n          \n            Healthcare Spending Analysis\n            In-depth exploration of rising U.S. healthcare costs from 1980 to 2014.\n            This report examines the surging healthcare costs in the U.S. from 1980 to 2014, revealing the factors behind its status as one of the world's most expensive countries for healthcare.\n          \n          \n          \n            Read More &gt;\n          \n        \n        \n      \n    \n      \n    \n      \n    \n      \n    \n      \n        \n        \n          Read More\n          \n          \n          \n          \n          \n            Symptom Severity Analysis\n            Exploring patient symptom data for improved healthcare insights.\n            Using symptom analysis, this study employs a Random Forest approach to predict severity levels in patients, aiming to enhance healthcare decision-making.\n          \n          \n          \n            Read More &gt;\n          \n        \n        \n      \n    \n      \n    \n      \n    \n  \n\n\n\n\n\n\n\n  Interactive Visualizations\n  \n    \n      \n    \n      \n    \n      \n    \n      \n        \n        \n          Read More\n          \n          \n          \n          \n          \n            Lightning-Fast 100m Dashboard\n            A dynamic Quarto-based dashboard analyzing 100m results.\n            The 100m Dashboard! Dash your way to victory and look beyond! Take your gold medal and observe that this dashboard was built using just quarto.\n          \n          \n          \n            Read More &gt;\n          \n        \n        \n      \n    \n      \n    \n      \n    \n      \n    \n      \n        \n        \n          Read More\n          \n          \n          \n          \n          \n            UK Accident Time Series\n            A Shiny-based approach for analyzing historical accident data across the UK.\n            This accident time series app allows you to explore and analyze data related accidents in UK. Feel free to download the data and explore other methods of analysis.\n          \n          \n          \n            Read More &gt;\n          \n        \n        \n      \n    \n      \n        \n        \n          Read More\n          \n          \n          \n          \n          \n            Yelp Reviews Spatial Analysis\n            Explore Yelp reviews and spatial trends using our interactive dashboard.\n            An interactive Shiny dashboard for exploring Yelp business reviews, spatial patterns, and advanced spatial modeling.\n          \n          \n          \n            Read More &gt;\n          \n        \n        \n      \n    \n  \n\n\n\n\n\n\n\n  Machine Learning\n  \n    \n      \n    \n      \n        \n        \n          Read More\n\n          \n          \n\n          \n            Enhanced Customer Return Forecasting\n            A swift 56% leap in performance in customer return forecasting!\n            Predict customer product returns, leveraging detailed product characteristics and customer demographics.\n          \n\n          \n            Read More &gt;\n          \n        \n\n      \n    \n      \n    \n      \n    \n      \n        \n        \n          Read More\n\n          \n          \n\n          \n            Pinot Province Prediction\n            Classifying wine origin with advanced data analysis and ML.\n            Our project sets a new bar in wine origin identification, transforming how industry professionals use critic data.\n          \n\n          \n            Read More &gt;\n          \n        \n\n      \n    \n      \n        \n        \n          Read More\n\n          \n          \n\n          \n            Predicting 50K+ Salaries\n            Accurately classify individuals by income bracket using advanced ML.\n            Expertly predicting $50K+ incomes through ML, our project highlights the synergy of data science skills and team collaboration.\n          \n\n          \n            Read More &gt;\n          \n        \n\n      \n    \n      \n    \n      \n    \n      \n    \n  \n\nNo matching items"
  },
  {
    "objectID": "content/license/index.html",
    "href": "content/license/index.html",
    "title": "License",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o1    \n  \n  This license outlines that all site content is under the CC-BY 4.0 license for free adaptation and sharing, as long as you credit me. The source code is MIT-licensed, meaning you can use or modify it without major restrictions. Additionally, while some elements were initially generated with AI, every final decision and revision is mine alone, so if you’re curious about the process, feel free to reach out.\n\n\n\n\n  License & Disclaimers\n  \n    © 2024-2025 Brian Adolfo Cervantes Alvarez. This website reflects my\n    personal perspectives, insights, and research, and does not represent\n    the views, opinions, or endorsements of my employer or affiliated\n    organizations.\n  \n  \n    All textual content, graphics, and other materials on this site are\n    licensed under the Creative Commons Attribution 4.0\n    International (CC BY 4.0) license. You are free to share, adapt, and\n    build upon the material provided you credit me appropriately and link back\n    to the original source.\n  \n  \n    While I strive to ensure accuracy in all posted content, I cannot guarantee\n    completeness or timeliness of information. Any reliance on the data,\n    analyses, or opinions provided here is done at your own risk, and I assume\n    no liability for any actions taken based on the information presented.\n  \n\n\n\n\n  Source Code\n  \n    The source code referenced or shared on this site is released under the\n    MIT License.\n    This permissive license allows you to use, modify, merge, publish, and\n    distribute the code with minimal restrictions. If you incorporate any of my\n    code into your own projects, credit is greatly appreciated (though not\n    legally required).\n  \n  \n    I may occasionally showcase code snippets as examples, tutorials, or\n    demonstrations. In such cases, the onus is on you to validate the suitability\n    and security of the code in your specific environment. When in doubt, I\n    recommend thorough testing and compliance with best practices.\n  \n\n\n\n\n  Use of Artificial Intelligence\n  \n    Some content on this site—including text, illustrative examples, or\n    ideation—benefited from AI-based assistance (most notably OpenAI’s\n    language models). This AI support helped accelerate brainstorming,\n    generation of first drafts, and code scaffolding.\n  \n  \n    However, every final edit, conceptual choice, and validation step is\n    exclusively my responsibility. If you notice any inaccuracies or suspect AI\n    artifacts, please let me know. I am committed to refining and correcting\n    content whenever issues arise.\n  \n\n\n\n\n  Ethics\n  \n    I actively strive to ensure that my work aligns with ethical and\n    transparent standards. This includes crediting sources for data,\n    respecting user privacy where applicable, and disclosing AI usage to\n    maintain clarity around how digital tools shape my work.\n  \n  \n    In the spirit of responsible AI, I aim to avoid sharing harmful,\n    confidential, or personally identifiable information. Should any\n    content inadvertently breach ethical or privacy guidelines, I will\n    address it promptly upon being made aware."
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I developed a comprehensive setup guide that walks through installing Git, configuring a GitHub account with a Personal Access Token, and connecting Git with RStudio to enable seamless commit and push operations. I provided clear, step-by-step instructions and troubleshooting tips to ensure a smooth pre-workshop environment for building Quarto-based portfolios on GitHub Pages. This guide ensures participants are well-prepared to create, deploy, and maintain a reproducible, production-quality portfolio before the workshop begins."
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#overview",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#overview",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "Overview",
    "text": "Overview\nIn this short guide, we’ll ensure you have all the pieces in place for our Quarto + GitHub Pages workshop. By the end of this setup, you’ll have:\n\nGit installed and configured,\nA GitHub account (with a Personal Access Token),\nRStudio connected to both Git and GitHub,\nThe ability to push commits so you’re good to go!"
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#install-git",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#install-git",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "1. Install Git",
    "text": "1. Install Git\nWhat is Git?\nGit is a version control system that tracks changes in your files and makes collaboration easier. You’ll need it for publishing your Quarto site to GitHub.\n\nWindows\n\nDownload Git for Windows.\n\nLaunch the installer; the default options generally work fine.\n\nMac\n\nDownload Git for Mac, or use Homebrew:\nbrew install git\n\nLinux\n\nUse your distribution’s package manager. For example, on Ubuntu/Debian:\nsudo apt-get install git\n\n\nVerify Installation\nOpen a terminal or command prompt and type:\ngit --version\nYou should see something like git version 2.42.0 (version numbers may vary).\n\n\n\n\n\n\nNote\n\n\n\nAs of January 29th, 2025, I am running on git version 2.39.5 (Apple Git-154)"
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#create-or-confirm-your-github-account",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#create-or-confirm-your-github-account",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "2. Create or Confirm Your GitHub Account",
    "text": "2. Create or Confirm Your GitHub Account\n\nGo to https://github.com and sign up if you don’t have an account yet.\n\nIf you already have one, just sign in and remember your username and password (we’ll need them)."
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#configure-git-on-your-computer",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#configure-git-on-your-computer",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "3. Configure Git on Your Computer",
    "text": "3. Configure Git on Your Computer\nIn a terminal or command prompt, configure your Git username and email (use the same email as on GitHub):\ngit config --global user.name \"YourGitHubUserName\"\ngit config --global user.email \"YourEmail@example.com\"\nGit uses this info to label your commits."
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#generate-a-personal-access-token-pat",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#generate-a-personal-access-token-pat",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "4. Generate a Personal Access Token (PAT)",
    "text": "4. Generate a Personal Access Token (PAT)\nGitHub no longer uses passwords for Git operations; you’ll need a Personal Access Token.\n\nLog in to GitHub and click your profile pic (top-right corner).\n\nGo to Settings → Developer settings → Personal access tokens → Tokens (classic).\n\nChoose Generate new token → Generate new token (classic) (if prompted).\n\nName your token something like RStudio-GitHub.\n\nSelect the repo scope (so you can push to repositories).\n\nGenerate and copy the token somewhere safe (you won’t see it again after leaving the page).\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not share your Personal Access Token! Don’t put it in your repo or save it on your computer. Treat it like a private API key."
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#connect-git-github-in-rstudio",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#connect-git-github-in-rstudio",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "5. Connect Git & GitHub in RStudio",
    "text": "5. Connect Git & GitHub in RStudio\n\nOpen RStudio.\n\nNavigate to Tools → Global Options → Git/SVN (or Version Control in newer versions).\n\nEnsure the Git executable path is correctly detected (e.g., /usr/bin/git on Mac/Linux or C:/Program Files/Git/bin/git.exe on Windows).\n\n\nOptional: Using usethis to manage credentials\nTo store your PAT in R, you can use the usethis package. In RStudio:\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\ncreate_github_token()  # This opens a browser to create a token.\ngitcreds_set()         # This will prompt you to enter your PAT.\n\nAfter following the prompts, your token will be securely saved."
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#test-your-setup-commit-push",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#test-your-setup-commit-push",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "6. Test Your Setup (Commit & Push)",
    "text": "6. Test Your Setup (Commit & Push)\n\nCreate or open an RStudio Project linked to Git:\n\nFile → New Project → Version Control → Git.\n\nPaste in your GitHub repo URL.\n\nSelect where to clone it locally.\n\nMake a small change to a file (e.g., add “Hello, Quarto!” to a .qmd or README.md).\nSave, then go to RStudio’s Git tab, check the file, and Commit with a message (e.g., “Testing my setup”).\nClick Push. If you’re asked for username and password:\n\nUsername: Your GitHub username.\nPassword: Your Personal Access Token (PAT).\n\n\nIf it succeeds, you’ll see a success message or “Everything up to date.”"
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#troubleshooting-repeated-login-prompts",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#troubleshooting-repeated-login-prompts",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "7. Troubleshooting Repeated Login Prompts",
    "text": "7. Troubleshooting Repeated Login Prompts\nIf RStudio keeps asking for your username/password on every push:\n\nlibrary(usethis)\ngitcreds_set()\n\nEnter your Personal Access Token when prompted. That should do the trick."
  },
  {
    "objectID": "content/blog/quarto-github-pages-preworkshop/index.html#ready-for-the-workshop-quarto-github-pages",
    "href": "content/blog/quarto-github-pages-preworkshop/index.html#ready-for-the-workshop-quarto-github-pages",
    "title": "Pre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages",
    "section": "8. Ready for the Workshop: Quarto + GitHub Pages",
    "text": "8. Ready for the Workshop: Quarto + GitHub Pages\nWith Git and GitHub all set, you’re ready to rock your Quarto-based portfolio! In the workshop, we’ll:\n\nCreate a Quarto project.\nAdd .qmd files for your portfolio pages.\nCommit and push to GitHub.\nActivate GitHub Pages to share your site with the world.\n\n\n\n\n\n\n\nPro Tip\n\n\n\nIf anything goes sideways, screenshot the error and note what step you were on. We’ll troubleshoot together during the workshop.\n\n\n\nThat’s it! Make sure you finish these steps so you can jump right into creating your Quarto-based portfolio during the workshop. See you there!"
  },
  {
    "objectID": "content/blog/plotly-gym-exercises-showcase/index.html",
    "href": "content/blog/plotly-gym-exercises-showcase/index.html",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I imported and cleaned a gym dataset using tidyverse to properly factorize variables, remove missing values, and prepare the data for analysis. I then computed mean workout ratings by exercise type and experience level and created interactive Plotly visualizations with a custom ggplot theme to rank these exercises. My analysis revealed that while stretching improves flexibility and reduces injury risk, it consistently ranks the lowest for muscle building compared to weight training, emphasizing its role as a supplementary activity."
  },
  {
    "objectID": "content/blog/plotly-gym-exercises-showcase/index.html#abstract",
    "href": "content/blog/plotly-gym-exercises-showcase/index.html#abstract",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Abstract",
    "text": "Abstract\nThis study examines the role of stretching in muscle building and its implications for fitness enthusiasts. While stretching offers benefits in terms of flexibility and injury prevention, it falls short as a primary method for muscle growth. The focus of stretching on range of motion rather than muscle mass and strength limits its effectiveness. In isolation, stretching lacks the necessary resistance to stimulate muscle growth. Moreover, my analysis using R reveals that stretching ranks the lowest in terms of workout ratings. The findings highlight the importance of incorporating weight and resistance training as the primary approaches for muscle development."
  },
  {
    "objectID": "content/blog/plotly-gym-exercises-showcase/index.html#introduction",
    "href": "content/blog/plotly-gym-exercises-showcase/index.html#introduction",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Introduction",
    "text": "Introduction\nStretching enhances flexibility and prevents injuries, but its effectiveness in building muscle mass and strength is debated. This study explores the role of stretching in muscle building compared to other exercises. Stretching prioritizes flexibility over muscle development due to inadequate resistance and tension. It may not yield significant gains in muscle mass or strength. Excessive stretching can even reduce muscle activation and power output. Analysis of workout ratings using R shows stretching ranks lowest for muscle building. Weight and resistance training are emphasized as primary methods, with stretching as a supplementary activity for flexibility."
  },
  {
    "objectID": "content/blog/plotly-gym-exercises-showcase/index.html#general-setup",
    "href": "content/blog/plotly-gym-exercises-showcase/index.html#general-setup",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "General Setup",
    "text": "General Setup\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(extrafont)\nlibrary(plotly)\nlibrary(htmlwidgets)\n# font_import()\nloadfonts()"
  },
  {
    "objectID": "content/blog/plotly-gym-exercises-showcase/index.html#data-wrangling",
    "href": "content/blog/plotly-gym-exercises-showcase/index.html#data-wrangling",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ngymDs &lt;- read_csv(\"../../../assets/datasets/megaGymDataset.csv\")\n\n# head(gymDs, 5)\n# names(gymDs)\n\nds &lt;- gymDs %&gt;%\n    mutate(\n        ID = ...1,\n        Level = factor(Level, levels = c(\"Beginner\", \"Intermediate\", \"Expert\")),\n        Type = as.factor(Type)\n    ) %&gt;%\n    select(-...1) %&gt;%\n    drop_na()"
  },
  {
    "objectID": "content/blog/plotly-gym-exercises-showcase/index.html#unleashing-the-power-of-plotly",
    "href": "content/blog/plotly-gym-exercises-showcase/index.html#unleashing-the-power-of-plotly",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Unleashing the Power of Plotly",
    "text": "Unleashing the Power of Plotly\n\nplotDs &lt;- ds %&gt;%\n    group_by(Type, Level) %&gt;%\n    summarize(meanRating = mean(Rating)) %&gt;%\n    arrange(Type, meanRating) %&gt;%\n    ungroup()\n\np &lt;- plotDs %&gt;%\n    highlight_key(., ~ reorder_within(Type, meanRating, Level)) %&gt;%\n    ggplot(aes(\n        x = meanRating,\n        y = reorder_within(Type, meanRating, Level),\n        fill = fct_reorder(Type, meanRating),\n        text = paste0(\n            \"Rating: \", round(meanRating, 2),\n            \"&lt;br&gt;Type: \", Type\n        )\n    )) +\n    geom_col(color = \"black\") +\n    facet_grid(\n        rows = vars(Level),\n        scales = \"free_y\",\n        switch = \"y\",\n        space = \"free_y\"\n    ) +\n    scale_y_reordered() +\n    scale_fill_brewer(palette = \"PuBuGn\") +\n    labs(\n        title = \"Ranking Exercise Type According to Experience Level of Individuals\",\n        x = \"Average Rating of Each Exercise Type\",\n        fill = \"Workout Types\"\n    ) +\n    theme_minimal() +\n    myTheme\n\n\nggplotly(p, tooltip = \"text\") %&gt;%\n    config(displayModeBar = FALSE) %&gt;%\n    highlight(on = \"plotly_hover\", off = \"plotly_doubleclick\") %&gt;%\n    layout(\n        uniformtext = list(minsize = 8, mode = \"hide\"),\n        margin = list(b = 70, l = 140, r = 140)\n    )"
  },
  {
    "objectID": "content/blog/plotly-gym-exercises-showcase/index.html#conclusion",
    "href": "content/blog/plotly-gym-exercises-showcase/index.html#conclusion",
    "title": "Do You Like Stretching? I Would Reconsider!",
    "section": "Conclusion",
    "text": "Conclusion\nWhile stretching remains valuable for improving flexibility and reducing the risk of injuries, it should be viewed as a supplementary activity rather than the main strategy for building muscle mass and strength. The incorporation of weight and resistance training, supported by the utilization of interactive plots using R, offers a more effective and comprehensive approach to achieving desired muscle development goals."
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html",
    "href": "content/blog/interactive-teaching-with-webr/index.html",
    "title": "Interactive Learning with WebR",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I built an interactive lab using webR to run R code directly in the browser, streamlining the setup process and providing students with immediate access to real-time data wrangling and visualization tools. I demonstrated key techniques—filtering, selecting, mutating, summarizing, reshaping data, and handling missing values—integrated with dynamic visualizations using ggplot2 to make complex concepts tangible and accessible. This lab showcases how webR transforms the learning experience by offering instant feedback and hands-on exploration, enabling students to master data manipulation skills efficiently."
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#why-use-webr-in-education",
    "href": "content/blog/interactive-teaching-with-webr/index.html#why-use-webr-in-education",
    "title": "Interactive Learning with WebR",
    "section": "Why use webR in Education?",
    "text": "Why use webR in Education?\nwebR seamlessly brings the power of R to the browser, removing the need for complex local installations. This accessibility is crucial for modern classrooms, where students might be working on different devices and operating systems. By embedding R code directly into educational materials, webR facilitates instant feedback, interactive exercises, and dynamic visualizations, making data science education more intuitive and approachable."
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#key-features-of-webr",
    "href": "content/blog/interactive-teaching-with-webr/index.html#key-features-of-webr",
    "title": "Interactive Learning with WebR",
    "section": "Key Features of webR",
    "text": "Key Features of webR\n\nCross-Platform Compatibility: Students can run R code on any device with a web browser, ensuring a consistent learning experience.\nImmediate Feedback: webR allows for instant execution of code, enabling students to see the results of their actions right away.\nInteractive Visualizations: By integrating with packages like ggplot2, educators can create interactive plots that students can manipulate directly in their browser."
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#implementation-in-data-wrangling-labs",
    "href": "content/blog/interactive-teaching-with-webr/index.html#implementation-in-data-wrangling-labs",
    "title": "Interactive Learning with WebR",
    "section": "Implementation in Data Wrangling Labs",
    "text": "Implementation in Data Wrangling Labs\nFor example, when teaching data wrangling concepts such as filtering, selecting, mutating, and summarizing, webR allows students to experiment with real datasets interactively. They can instantly see the impact of different data manipulation techniques, reinforcing their understanding through practice.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#benefits-for-educators",
    "href": "content/blog/interactive-teaching-with-webr/index.html#benefits-for-educators",
    "title": "Interactive Learning with WebR",
    "section": "Benefits for Educators",
    "text": "Benefits for Educators\nwebR enables educators to:\n\nSimplify Setup: No more lengthy instructions on installing R and its dependencies.\nEnhance Engagement: Interactive content keeps students involved, making complex topics more digestible.\nFacilitate Learning: By allowing students to explore data manipulation and visualization in real time, webR helps solidify their understanding of essential concepts.\n\n\nSummary\nwebR transforms how data science is taught, making R’s powerful features accessible and interactive in the classroom. Whether you’re teaching undergraduates new to R or advanced students refining their skills, webR offers a dynamic platform that enhances learning and encourages exploration."
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#what-is-data-wrangling",
    "href": "content/blog/interactive-teaching-with-webr/index.html#what-is-data-wrangling",
    "title": "Interactive Learning with WebR",
    "section": "What is Data Wrangling?",
    "text": "What is Data Wrangling?\nData wrangling is the process of converting messy, untidy data into a tidy format, making it suitable for data visualization and analysis.\n\nData is often messy: Real-world data is rarely provided in a tidy format.\nIndustry challenges: Many industries have poorly designed data structures, requiring data preparation before visualization.\nRarely tidy datasets: It is uncommon to receive a dataset that is already tidy.\n\n\nWhat is Tidy Data?\nTidy data is a structured format that aligns the organization of a dataset with its underlying meaning. In tidy data:\n\nEach variable has its own column: Every column in the dataset corresponds to a specific variable or attribute.\nEach observation has its own row: Every row captures a single observation or data entry.\nEach cell contains a single value: Each cell holds one distinct piece of information for a particular variable and observation.\n\nIn most cases, data is often imported using SQL to create narrower datasets. While we won’t cover SQL in this course, it’s a valuable skill to learn in the future. For now, we’ll focus on using R to manipulate and create subset datasets from larger datasets for focused analysis.\n\n\nWhat Causes Untidy Data?\n\nIncorrect/Inconsistent Dates: Dates can be tricky because they might be formatted differently across datasets or have errors like typos or missing parts. For example, some data might use “MM/DD/YYYY” while others use “YYYY-MM-DD,” leading to confusion and potential errors when analyzing time-based data.\nWide Format Times: Time data is sometimes presented in a wide format, where each column represents a different time period. This structure can make it difficult to perform certain types of analysis, as many statistical and visualization tools prefer data in a long format, where each row represents a single observation at a specific time.\nVoid or Misspelled Descriptions: Descriptions and labels are often incomplete, missing, or contain typos. These errors can make it challenging to interpret the data correctly, especially when variables are not clearly defined or are inconsistent across different parts of the dataset.\nMissing Values: Missing data is common and can occur in any part of a dataset, leading to gaps that can skew analysis or result in errors. Handling these missing values is crucial for ensuring that any conclusions drawn from the data are accurate.\nCondensed or Incorrect Headers: Column names might be too short, unclear, or incorrectly labeled, leading to confusion about what the data actually represents. For example, a column labeled “Pop” might be ambiguous—does it refer to population, popularity, or something else?\nRow Content Split: Sometimes, a single column contains data that should be divided into multiple columns, such as when a “Location” column includes both city and state. This issue can make it difficult to analyze the data separately or perform operations that rely on more granular details. These common issues contribute to untidy data, which can complicate analysis and lead to inaccurate results.\n\nMastering data wrangling is crucial because you might have to handle datasets with millions of rows and hundreds of columns."
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#getting-started",
    "href": "content/blog/interactive-teaching-with-webr/index.html#getting-started",
    "title": "Interactive Learning with WebR",
    "section": "Getting Started",
    "text": "Getting Started\nFirst, ensure you have the necessary packages installed and loaded. We will use the dplyr, lubridate, readr, ggplot2, and tidyr packages for our examples.\n\n\n\n\n\n\nDownloading R-packages\n\n\n\nUse install.packages('Name of Package') to install an R package. Careful! Package names are case sensitive, so install.packages(‘GGplot2’) will not work, but install.packages('ggplot2') will.\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(lubridate)\n\n\nDownload the Data\n\n\n\n\n\n\nRunning Code Block Shortcuts\n\n\n\nMac users: Use ⌘ + return to run single or highlighted line(s). Use shift + return to run entire code block\nWindows users: Use ctrl + enter to run single or highlighted line(s). Use shift + enter to run entire code block\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVerify Datasets with head()\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nDescription of the Dataset\n\ncountry: Country name from a predefined list of 10 countries.\nyear: Years between 2010 and 2023.\npopulation: Real-world population size for each country and year.\ngdp: Gross Domestic Product (GDP) in USD millions for each country and year.\ngdp_per_capita: GDP per capita, calculated as GDP divided by population.\nlife_expectancy: Life expectancy of citzens\nbirth_rate: Birth rate for each country and year\ntemperature: Average temperature in celsius per country\nregion: Geographical region corresponding to each country.\ncategory: Classification of the country as “First World,” “Second World,” or “Third World.”"
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#tidy-data-wrangling",
    "href": "content/blog/interactive-teaching-with-webr/index.html#tidy-data-wrangling",
    "title": "Interactive Learning with WebR",
    "section": "Tidy Data Wrangling",
    "text": "Tidy Data Wrangling\nImportant Concepts\n\nFiltering: Filter data based on conditions such as year, country, or region.\nSelecting: Select specific columns for focused analysis.\nMutating: Create new columns, such as cases per 100,000 population.\nSummarizing: Aggregate data by country, year, or region to find totals and averages.\n\n\nFiltering\nFiltering is essential for narrowing down datasets to the most relevant information, making patterns easier to identify.\n\nExample 1\nYou are tasked with visualizing trends in life expectancy in Asian countries between 2010 and 2020.\n\nFiltering Process\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat did we do here?\nBy focusing on specific countries and years, filtering allows for more targeted and relevant visualizations, making it easier to analyze trends and patterns specific to the context.\n\n\n\nExample 2\nAnalyze the relationship between GDP per capita and life expectancy in European countries with a GDP per capita above $30,000 for the years 2015-2020.\n\nFiltering Process\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat did we do here?\nFiltering based on economic indicators allows for focused analysis on the relationship between wealth and life expectancy, removing noise from countries with different economic conditions.\n\n\n\nFiltering Challenges\n\nPopulation Size in Africa:\n\nFilter: Only countries from Africa where the population exceeds 50 million.\nPurpose: Isolate data for large African nations to analyze trends specific to highly populated areas.\n\nEconomic Data in Europe:\n\nFilter: Show data only for the years 2015-2020 for European countries with GDP per capita above $30,000.\nPurpose: Focus on wealthy European countries during a specific period to study economic outcomes.\n\nHigh Birth Rates in Asia:\n\nFilter: Data for Asian countries where the birth rate is above 2.5.\nPurpose: Analyze regions with high birth rates, possibly indicating population growth trends.\n\nCold Regions in Asia:\n\nFilter: Asian countries where the average temperature is below 10°C between 2010 and 2020.\nPurpose: Focus on colder regions in Asia to study how temperature may correlate with other demographic factors.\n\nGDP Data with Missing Values:\n\nFilter: Remove any entries with missing gdp_per_capita values for the years 2010-2020.\nPurpose: Ensure clean data for economic analysis, removing incomplete records that could skew results.\n\n\n\n\n\nSelecting\nSelecting allows you to focus on specific columns relevant to your analysis.\n\nExample 1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSelecting Challenges\n\nSelect columns related to economic indicators (e.g., country, gdp_per_capita, population) for further analysis.\nCreate a dataset with only the year, life_expectancy, and temperature columns for all countries and show the first 5 rows.\nChoose columns that exclude any geographical information and check the first 10 rows.\nSelect and rename the country and population columns to nation and pop_size, respectively.\nCreate a new dataset with only the year, population, and a newly created column, population_in_millions (which should be calculated as population / 1e6).\n\n\n\n\nMutating\nMutating helps create new columns based on existing data.\n\nExample 1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nMutating Challenges\n\nCreate a new column called gdp_total that multiplies gdp_per_capita by population.\nAdd a new column that indicates whether a country’s GDP per capita is above or below a certain threshold (e.g., $20,000).\nMutate the temperature column to create a new column, temperature_f, that converts Celsius to Fahrenheit.\nCreate a population_density column by dividing population by a given area (assuming you have area data).\nGenerate a column that calculates the ratio of birth rate to life expectancy for each country.\n\n\n\n\nSummarizing\nSummarizing aggregates data by country, year, or region to find totals and averages.\n\nExample 1\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualization\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSummarizing Challenges\n\nSummarize the dataset by finding the average temperature for each region.\nAggregate the data to find the total population and average life expectancy for each continent.\nGroup the data by country and summarize to find the maximum and minimum GDP per capita for each country.\nSummarize by year to find the total population and average birth rate each year.\nCreate a summary that calculates the total population and average GDP per capita for countries classified as “First World.”"
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#untidy-data-wrangling",
    "href": "content/blog/interactive-teaching-with-webr/index.html#untidy-data-wrangling",
    "title": "Interactive Learning with WebR",
    "section": "Untidy Data Wrangling",
    "text": "Untidy Data Wrangling\n\nHandling Wide vs. Long Formats\nUntidy data often comes in a “wide” format, where multiple variables are stored across columns rather than in a long format where each observation is a row.\n\nCreating an Untidy Version\nTo demonstrate this, let’s take our dataset and convert it into a wide format, then back to a long format.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, let’s convert this wide data back to a long format.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis process shows how data can be reshaped for different analytical needs. Wide format is useful for certain analyses but often needs to be converted to long format for modeling and visualization.\n\n\n\nHandling Misspelled Header Column Names\nSometimes datasets come with misspelled or inconsistent column names, which can lead to errors in data manipulation.\n\nCreating Misspelled Header Names\nLet’s create a dataset with intentionally misspelled column names and then fix them.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFixing the Misspelled Column Names\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis illustrates how to identify and correct misspelled column names, which is a crucial step in data cleaning.\n\n\n\nHandling Row Content Split and Reverse\nSometimes, data stored in a single column needs to be split into multiple columns or vice versa.\n\nMerging Two Columns into One\nLet’s take the country and year columns and merge them into a single column.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSplitting the Merged Column Back into Two\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis demonstrates how to handle situations where data needs to be recombined or separated for different purposes.\n\n\n\nHandling Dates\nIn some cases, it might be necessary to convert a year column from a numeric format (double) into a proper date format for time series analysis or plotting purposes. Here’s how you can do that in R using the lubridate package.\n\nExample 1\n\nConverting year to a Date\nLet’s convert the year column into a date format, setting it as January 1st of that year.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat did we do here?\n\npaste0(year, \"-01-01\"): Combines the year with the string “-01-01” to create a date string like “2010-01-01”.\nymd(): Converts the resulting string into a date object in the “Year-Month-Day” format.\n\nThis creates a new column, year_date, which is now in the proper date format.\n\n\n\nChallenges [Solutions]\n\nConvert year to end of year date:\n\nTask: Convert the year column to a date format, but set it as December 31st of that year.\nPurpose: Useful for representing data that summarizes annual results.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCreate a quarterly date:\n\nTask: Convert the year column into a date representing the first quarter (e.g., “2010-03-31”).\nPurpose: Useful for quarterly analysis.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMid-year date conversion:\n\nTask: Convert the year column to a date format, setting it as June 30th of each year.\nPurpose: Represents mid-year data points.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUse year as a dynamic time period:\n\nTask: Convert the year column to represent the last day of a chosen month (e.g., November).\nPurpose: Allows for flexibility depending on the analysis context.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConvert year to fiscal year start date:\n\nTask: Convert the year column into a date representing the start of the fiscal year (e.g., April 1st).\nPurpose: Useful for financial and budgetary analyses.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nHandling Missing Data\nDealing with missing data is a crucial aspect of data wrangling. Missing data can occur for various reasons, and understanding the nature of these missing values is essential for appropriate handling.\n\nTypes of Missing Data\n\nMCAR (Missing Completely at Random): Data is missing entirely at random, with no relationship between the missing data and any other observed or unobserved data. The analysis remains unbiased if this missing data is ignored.\nMAR (Missing at Random): The likelihood of missing data on a variable is related to other observed variables but not to the value of the variable itself .\nMNAR (Missing Not at Random): The missingness is related to the unobserved data itself, meaning the missing values are related to the actual value that is missing.\n\n\n\nSimulating MAR Data\nWe’ll create a scenario where gdp_per_capita is more likely to be missing if the population is below a certain threshold, making it “missing at random” based on population size.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSimulating MNAR Data\nWe’ll simulate a scenario where the likelihood of life_expectancy being missing is higher if life expectancy is lower than 60 years, making it “missing not at random.”\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nMissing Data | Before Wrangling\n\nBefore Handling Missing Data\nLet’s visualize the data before handling missing values, focusing on the relationship between GDP per capita and life expectancy.\n\n\nVisualization with MAR Data\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualization with MNAR Data\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nMissing Data | How to Wrangle\nTo handle the missing data, we’ll apply a simple imputation strategy, filling in missing values with the median of the respective variable.\n\nImputing Missing Values for MAR Data\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nMissing Data | After Wrangling\nLet’s visualize the data again after imputing the missing values.\n\nVisualization with MAR Data (Imputed)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nImputing Missing Values for MNAR Data\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualization with MNAR Data (Imputed)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSummary\nUnderstanding the different types of missing data—MCAR, MAR, and MNAR—is crucial for choosing the right approach to handle them. We explored how to simulate and visualize MAR and MNAR scenarios in our dataset, highlighting the importance of addressing missing data for accurate analysis. By comparing visualizations before and after imputation, students can grasp the significant impact missing data can have on their results and learn effective strategies to mitigate these issues."
  },
  {
    "objectID": "content/blog/interactive-teaching-with-webr/index.html#creating-a-pseudo-publication-ready-visualization",
    "href": "content/blog/interactive-teaching-with-webr/index.html#creating-a-pseudo-publication-ready-visualization",
    "title": "Interactive Learning with WebR",
    "section": "Creating a Pseudo-Publication Ready Visualization",
    "text": "Creating a Pseudo-Publication Ready Visualization\nWe’ll combine all the data wrangling techniques you’ve learned—filtering, selecting, mutating, summarizing—to perform a detailed analysis and produce a polished, publication-ready visualization.\n\nCreating a Professional-Quality Visualization\nHere’s a step-by-step guide to transform and visualize data from 10 countries in the dataset:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nReview & Scrutinize\nWhat is this visualization?: Write a brief paragraph describing the design, purpose, and key message of the plot. Explain what the visualization is intended to show and how it effectively communicates the data.\nWhy is this visualization nearly publication-ready?: In 2-3 sentences, discuss what makes the plot polished and professional, highlighting any elements that could make it suitable for publication."
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html",
    "href": "content/blog/health-insurance-premiums-analysis/index.html",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I analyzed a health insurance dataset by importing and cleaning the data, performing exploratory analysis, and creating detailed visualizations—such as histograms, boxplots, and correlation heatmaps—to compare insurance premiums between smokers and non-smokers. My analysis revealed that among smokers, male premiums increased by 52% (amounting to roughly 408.57% more than non-smokers) and female premiums increased by 49% (approximately 350.12% higher than non-smokers). Additionally, I examined the roles of BMI, age, and family size on premium costs, illustrating that these lifestyle factors further contribute to escalating health insurance expenses."
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html#abstract",
    "href": "content/blog/health-insurance-premiums-analysis/index.html#abstract",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Abstract",
    "text": "Abstract\nThis study confirms the significant impact of smoking on the escalation of health insurance premiums. Male and female smokers with a body mass index (BMI) of 30 or higher face additional charges, compounding their financial burden. Male smokers experience a 52% increase, while female smokers face a 49% rise in insurance charges, in addition to the base premium for smokers.\nMoreover, male smokers pay 408.57\\% more than non-smokers, while female smokers pay 350.12\\% more. The data unequivocally supports the idea that unhealthy lifestyle choices, such as smoking and high BMI, result in higher health insurance premiums. It is also evident that premiums increase gradually with age.\nWhile this project provides valuable insights, further exploration opportunities include applying machine learning techniques to assess the representativeness of the sample, which could enhance the accuracy of conclusions and drive advancements in health insurance research."
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html#data-setup-and-import",
    "href": "content/blog/health-insurance-premiums-analysis/index.html#data-setup-and-import",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Data Setup and Import",
    "text": "Data Setup and Import\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nsns.set_style('darkgrid')\nsns.set(font_scale=1.2)\n\n# Read the CSV file\ndf = pd.read_csv(\"../../../assets/datasets/insurance.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520"
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html#high-level-exploratory-analysis",
    "href": "content/blog/health-insurance-premiums-analysis/index.html#high-level-exploratory-analysis",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "High-Level Exploratory Analysis",
    "text": "High-Level Exploratory Analysis\nWe start by exploring the dataset at a high level: checking for missing data, data types, and general statistics.\n\n\nCode\nprint(\"Dataset Info:\")\nprint(df.info())\nprint(\"\\nBasic Description:\")\nprint(df.describe(include='all'))\n\n\nDataset Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\nNone\n\nBasic Description:\n                age   sex          bmi     children smoker     region  \\\ncount   1338.000000  1338  1338.000000  1338.000000   1338       1338   \nunique          NaN     2          NaN          NaN      2          4   \ntop             NaN  male          NaN          NaN     no  southeast   \nfreq            NaN   676          NaN          NaN   1064        364   \nmean      39.207025   NaN    30.663397     1.094918    NaN        NaN   \nstd       14.049960   NaN     6.098187     1.205493    NaN        NaN   \nmin       18.000000   NaN    15.960000     0.000000    NaN        NaN   \n25%       27.000000   NaN    26.296250     0.000000    NaN        NaN   \n50%       39.000000   NaN    30.400000     1.000000    NaN        NaN   \n75%       51.000000   NaN    34.693750     2.000000    NaN        NaN   \nmax       64.000000   NaN    53.130000     5.000000    NaN        NaN   \n\n             charges  \ncount    1338.000000  \nunique           NaN  \ntop              NaN  \nfreq             NaN  \nmean    13270.422265  \nstd     12110.011237  \nmin      1121.873900  \n25%      4740.287150  \n50%      9382.033000  \n75%     16639.912515  \nmax     63770.428010  \n\n\n\nDistribution of Smokers vs. Non-Smokers\n\n\nCode\n# Count the number of smokers vs. non-smokers\nnum_smokers = (df[\"smoker\"] == \"yes\").sum()\nnum_nonsmokers = (df[\"smoker\"] == \"no\").sum()\n\nprint(f\"Number of smokers: {num_smokers}\")\nprint(f\"Number of nonsmokers: {num_nonsmokers}\")\n\nplt.figure(figsize=(6, 4))\nsns.countplot(x=\"smoker\", data=df)\nplt.title(\"Count of Smokers vs. Non-smokers\")\nplt.xlabel(\"Smoking Status\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n\nNumber of smokers: 274\nNumber of nonsmokers: 1064"
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html#analysis-of-factors-affecting-insurance-costs",
    "href": "content/blog/health-insurance-premiums-analysis/index.html#analysis-of-factors-affecting-insurance-costs",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Analysis of Factors Affecting Insurance Costs",
    "text": "Analysis of Factors Affecting Insurance Costs\nWe now focus on identifying the relationship between smoking status and insurance charges. We also investigate gender differences, BMI (Body Mass Index), age, and other features.\n\n\nCode\nprint(\"Overall mean charges:\", round(df['charges'].mean(), 2))\nprint(\"Overall median charges:\", round(df['charges'].median(), 2))\nprint(\"Overall standard deviation:\", round(df['charges'].std(), 2))\n\n\nOverall mean charges: 13270.42\nOverall median charges: 9382.03\nOverall standard deviation: 12110.01\n\n\n\nInsurance Costs: Smokers Only\n\nBoxplot and Histogram\n\n\nCode\ndf_smokers = df[df[\"smoker\"] == \"yes\"]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\nsns.boxplot(y=\"charges\", data=df_smokers, ax=ax1)\nax1.set_title(\"Insurance Cost Boxplot (Smokers)\")\nax1.set_ylabel(\"Insurance Cost (USD)\")\n\nsns.histplot(df_smokers[\"charges\"], bins=40, kde=True, ax=ax2)\nax2.set_title(\"Insurance Cost Distribution (Smokers)\")\nax2.set_xlabel(\"Insurance Cost (USD)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics (Smokers)\n\n\nCode\nsmoker_mean = round(df_smokers[\"charges\"].mean(), 2)\nsmoker_median = round(df_smokers[\"charges\"].median(), 2)\nsmoker_std = round(df_smokers[\"charges\"].std(), 2)\nsmoker_var = round(df_smokers[\"charges\"].var(), 2)\nsmoker_max = round(df_smokers[\"charges\"].max(), 2)\nsmoker_min = round(df_smokers[\"charges\"].min(), 2)\n\nprint(f\"Smoker Mean: ${smoker_mean}\")\nprint(f\"Smoker Median: ${smoker_median}\")\nprint(f\"Smoker Max: ${smoker_max}\")\nprint(f\"Smoker Min: ${smoker_min}\")\nprint(f\"Smoker Std: ${smoker_std}\")\nprint(f\"Smoker Var: {smoker_var}\")\n\n\nSmoker Mean: $32050.23\nSmoker Median: $34456.35\nSmoker Max: $63770.43\nSmoker Min: $12829.46\nSmoker Std: $11541.55\nSmoker Var: 133207311.21\n\n\n\n\n\nMale vs. Female Smokers\nBelow we compare male smokers and female smokers, highlighting BMI and how it affects insurance cost.\n\n\nCode\ndf_male_smoker = df_smokers[df_smokers[\"sex\"] == \"male\"]\ndf_female_smoker = df_smokers[df_smokers[\"sex\"] == \"female\"]\n\n# Plot multiple subplots for male smokers\nfig, ax = plt.subplots(2, 3, figsize=(14, 8))\nsns.set(font_scale=0.9)\n\n# 1) Histogram of charges (male smokers)\nsns.histplot(df_male_smoker[\"charges\"], bins=40, ax=ax[0,0], color=\"navy\")\nax[0,0].set_title(\"Male Smoker Cost Dist.\")\nax[0,0].set_xlabel(\"Cost (USD)\")\n\n# 2) Bar plot of average charges by children\ndf_children_m = (\n    df_male_smoker\n    .groupby(\"children\")[\"charges\"]\n    .mean()\n    .reset_index()\n)\nsns.barplot(data=df_children_m, x=\"children\", y=\"charges\", ax=ax[0,1], color=\"teal\")\nax[0,1].set_title(\"Avg. Male Smoker Cost by # of Children\")\nax[0,1].set_xlabel(\"Number of Children\")\nax[0,1].set_ylabel(\"Cost (USD)\")\n\n# 3) Scatterplot charges vs BMI\nsns.scatterplot(\n    data=df_male_smoker, x=\"bmi\", y=\"charges\",\n    hue=\"region\", ax=ax[0,2]\n)\nax[0,2].axvline(x=30, color=\"gray\", linestyle=\"--\", label=\"BMI = 30\")\nax[0,2].set_title(\"Male Smoker: Cost vs. BMI\")\nax[0,2].set_xlabel(\"BMI\")\nax[0,2].set_ylabel(\"Cost (USD)\")\n\n# 4) Scatterplot charges vs age\nsns.scatterplot(\n    data=df_male_smoker, x=\"age\", y=\"charges\",\n    hue=\"children\", ax=ax[1,0]\n)\nax[1,0].set_title(\"Male Smoker: Cost vs. Age\")\nax[1,0].set_xlabel(\"Age\")\nax[1,0].set_ylabel(\"Cost (USD)\")\n\n# 5) Barplot average cost by region\ndf_region_m = (\n    df_male_smoker\n    .groupby(\"region\")[\"charges\"]\n    .mean()\n    .reset_index()\n)\nsns.barplot(data=df_region_m, x=\"region\", y=\"charges\", ax=ax[1,1], palette=\"viridis\")\nax[1,1].set_title(\"Male Smoker Cost by Region\")\nax[1,1].set_xlabel(\"Region\")\nax[1,1].set_ylabel(\"Cost (USD)\")\n\n# 6) Boxplot\nsns.boxplot(x=df_male_smoker[\"charges\"], ax=ax[1,2], color=\"lightblue\")\nax[1,2].set_title(\"Male Smoker: Boxplot of Charges\")\nax[1,2].set_xlabel(\"Cost (USD)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBMI &lt; 30 vs. BMI &gt;= 30 (Male Smokers)\n\n\nCode\nmale_bmi_under_30 = df_male_smoker[df_male_smoker[\"bmi\"] &lt; 30]\nmale_bmi_over_30 = df_male_smoker[df_male_smoker[\"bmi\"] &gt;= 30]\n\navg_under_30 = round(male_bmi_under_30[\"charges\"].mean(), 2)\navg_over_30 = round(male_bmi_over_30[\"charges\"].mean(), 2)\ndiff_male = round(avg_over_30 - avg_under_30, 2)\nrel_increase_male = 100 * round(diff_male / avg_under_30, 2)\n\nprint(f\"Male Smoker &lt;30 BMI Avg. Cost: ${avg_under_30}\")\nprint(f\"Male Smoker &gt;=30 BMI Avg. Cost: ${avg_over_30}\")\nprint(f\"Absolute Difference: ${diff_male}\")\nprint(f\"Relative Increase: ~{rel_increase_male}% higher cost\")\n\n\nMale Smoker &lt;30 BMI Avg. Cost: $21643.07\nMale Smoker &gt;=30 BMI Avg. Cost: $41131.57\nAbsolute Difference: $19488.5\nRelative Increase: ~90.0% higher cost\n\n\n\n\n\nFemale Smokers\n\n\nCode\nfig, ax = plt.subplots(2, 3, figsize=(14, 8))\nsns.set(font_scale=0.9)\n\n# 1) Histogram\nsns.histplot(df_female_smoker[\"charges\"], bins=40, ax=ax[0,0], color=\"purple\")\nax[0,0].set_title(\"Female Smoker Cost Dist.\")\nax[0,0].set_xlabel(\"Cost (USD)\")\n\n# 2) Barplot average cost by children\ndf_children_f = (\n    df_female_smoker\n    .groupby(\"children\")[\"charges\"]\n    .mean()\n    .reset_index()\n)\nsns.barplot(data=df_children_f, x=\"children\", y=\"charges\", ax=ax[0,1], color=\"violet\")\nax[0,1].set_title(\"Avg. Female Smoker Cost by # of Children\")\nax[0,1].set_xlabel(\"Number of Children\")\nax[0,1].set_ylabel(\"Cost (USD)\")\n\n# 3) Scatterplot cost vs BMI\nsns.scatterplot(\n    data=df_female_smoker, x=\"bmi\", y=\"charges\",\n    hue=\"region\", ax=ax[0,2]\n)\nax[0,2].axvline(x=30, color=\"gray\", linestyle=\"--\")\nax[0,2].set_title(\"Female Smoker: Cost vs. BMI\")\nax[0,2].set_xlabel(\"BMI\")\nax[0,2].set_ylabel(\"Cost (USD)\")\n\n# 4) Scatterplot cost vs age\nsns.scatterplot(\n    data=df_female_smoker, x=\"age\", y=\"charges\",\n    hue=\"children\", ax=ax[1,0]\n)\nax[1,0].set_title(\"Female Smoker: Cost vs. Age\")\nax[1,0].set_xlabel(\"Age\")\nax[1,0].set_ylabel(\"Cost (USD)\")\n\n# 5) Barplot by region\ndf_region_f = (\n    df_female_smoker\n    .groupby(\"region\")[\"charges\"]\n    .mean()\n    .reset_index()\n)\nsns.barplot(data=df_region_f, x=\"region\", y=\"charges\", ax=ax[1,1], palette=\"rocket\")\nax[1,1].set_title(\"Female Smoker Cost by Region\")\nax[1,1].set_xlabel(\"Region\")\nax[1,1].set_ylabel(\"Cost (USD)\")\n\n# 6) Boxplot\nsns.boxplot(x=df_female_smoker[\"charges\"], ax=ax[1,2], color=\"pink\")\nax[1,2].set_title(\"Female Smoker: Boxplot of Charges\")\nax[1,2].set_xlabel(\"Cost (USD)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfemale_bmi_under_30 = df_female_smoker[df_female_smoker[\"bmi\"] &lt; 30]\nfemale_bmi_over_30 = df_female_smoker[df_female_smoker[\"bmi\"] &gt;= 30]\n\navg_under_30_f = round(female_bmi_under_30[\"charges\"].mean(), 2)\navg_over_30_f = round(female_bmi_over_30[\"charges\"].mean(), 2)\ndiff_female = round(avg_over_30_f - avg_under_30_f, 2)\nrel_increase_female = 100 * round(diff_female / avg_under_30_f, 2)\n\nprint(f\"Female Smoker &lt;30 BMI Avg. Cost: ${avg_under_30_f}\")\nprint(f\"Female Smoker &gt;=30 BMI Avg. Cost: ${avg_over_30_f}\")\nprint(f\"Absolute Difference: ${diff_female}\")\nprint(f\"Relative Increase: ~{rel_increase_female}% higher cost\")\n\n\nFemale Smoker &lt;30 BMI Avg. Cost: $21070.04\nFemale Smoker &gt;=30 BMI Avg. Cost: $42320.62\nAbsolute Difference: $21250.58\nRelative Increase: ~101.0% higher cost\n\n\n\n\nSmokers vs. Non-Smokers\n\n\nCode\ndf_nonsmokers = df[df[\"smoker\"] == \"no\"]\n\nnonsmoker_mean = round(df_nonsmokers[\"charges\"].mean(), 2)\nnonsmoker_median = round(df_nonsmokers[\"charges\"].median(), 2)\n\nprint(f\"Non-smoker Mean: ${nonsmoker_mean}\")\nprint(f\"Non-smoker Median: ${nonsmoker_median}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\nsns.boxplot(y=\"charges\", data=df_nonsmokers, ax=ax1, color=\"green\")\nax1.set_title(\"Cost Boxplot (Non-smokers)\")\n\nsns.histplot(df_nonsmokers[\"charges\"], bins=40, kde=True, ax=ax2, color=\"green\")\nax2.set_title(\"Cost Distribution (Non-smokers)\")\nax2.set_xlabel(\"Insurance Cost (USD)\")\n\nplt.tight_layout()\nplt.show()\n\n\nNon-smoker Mean: $8434.27\nNon-smoker Median: $7345.41\n\n\n\n\n\n\n\n\n\n\nComparison by Gender (Non-Smokers)\n\n\nCode\ndf_male_nonsmoker = df_nonsmokers[df_nonsmokers[\"sex\"] == \"male\"]\ndf_female_nonsmoker = df_nonsmokers[df_nonsmokers[\"sex\"] == \"female\"]\n\navg_male_nonsmoker = round(df_male_nonsmoker[\"charges\"].mean(), 2)\navg_female_nonsmoker = round(df_female_nonsmoker[\"charges\"].mean(), 2)\nprint(f\"Male Non-smoker Avg: ${avg_male_nonsmoker}\")\nprint(f\"Female Non-smoker Avg: ${avg_female_nonsmoker}\")\n\ndf_both_means = pd.DataFrame({\n    'Sex': ['Male', 'Female'],\n    'AvgCharges': [avg_male_nonsmoker, avg_female_nonsmoker]\n})\n\nplt.figure(figsize=(5,4))\nsns.barplot(data=df_both_means, x='Sex', y='AvgCharges', palette='Set2')\nplt.title(\"Non-smoker Avg Charges by Sex\")\nplt.ylabel(\"Average Insurance Cost (USD)\")\nplt.show()\n\n\nMale Non-smoker Avg: $8087.2\nFemale Non-smoker Avg: $8762.3"
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html#correlation-heatmap",
    "href": "content/blog/health-insurance-premiums-analysis/index.html#correlation-heatmap",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Correlation Heatmap",
    "text": "Correlation Heatmap\nTo quickly see how numeric variables (like age, bmi, children, charges) relate to each other, we can look at a correlation heatmap.\n\n\nCode\n# Select only numeric columns\ndf_numeric = df[[\"age\", \"bmi\", \"children\", \"charges\"]].copy()\n\ncorr_matrix = df_numeric.corr()\nplt.figure(figsize=(6,5))\nsns.heatmap(corr_matrix, annot=True, cmap=\"magma\", fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numeric Features\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTakeaways: - There’s a moderate positive correlation between age and charges, as well as bmi and charges.\n- Children has a mild correlation with charges.\n- This quick analysis suggests that age and BMI might be strong predictors of insurance cost (which also aligns with the earlier analyses)."
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html#results",
    "href": "content/blog/health-insurance-premiums-analysis/index.html#results",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Results",
    "text": "Results\n\nSmoking Impact: Male and female smokers have significantly higher costs than non-smokers. Once BMI reaches 30 or higher, that cost inflates further.\n\nGender Differences: Among smokers with BMI &lt; 30, men pay slightly more on average. For those with BMI &gt;= 30, women pay marginally more.\n\nAge and Children: Costs generally rise with age; having more children is associated with higher costs among non-smokers, though there are some nuances.\n\nCorrelation: A heatmap reveals that age, BMI, and number of children all have positive associations with charges, with age and BMI showing the strongest correlation."
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html#conclusion",
    "href": "content/blog/health-insurance-premiums-analysis/index.html#conclusion",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "Conclusion",
    "text": "Conclusion\nOur analysis demonstrates the profound impact of smoking and high BMI on health insurance costs. Age is a steady contributor to increased premiums, while having multiple children also shows mild cost elevations for non-smokers. These findings highlight the importance of preventive healthcare and lifestyle interventions.\nMoving forward, applying machine learning methods or deeper statistical modeling (e.g., linear or logistic regression) could refine these conclusions and more accurately predict costs. This lays groundwork for broader health insurance strategies, encouraging healthier choices while clarifying how demographic and lifestyle factors compound to drive premiums upward."
  },
  {
    "objectID": "content/blog/health-insurance-premiums-analysis/index.html#references",
    "href": "content/blog/health-insurance-premiums-analysis/index.html#references",
    "title": "The Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs",
    "section": "References",
    "text": "References\n\nMedical News Today: BMI (body mass index): What is it and is it useful?\n\nHealthMarkets: Smoking and Health Insurance"
  },
  {
    "objectID": "content/blog/data-fest-2025-effective-data-visualizations/index.html",
    "href": "content/blog/data-fest-2025-effective-data-visualizations/index.html",
    "title": "Effective Data Visualizations for DataFest",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I created a professional guide on effective data visualization tailored for the ASA DataFest competition, emphasizing the appropriate selection of chart types based on data structures, and clearly differentiating static and interactive visualizations. I included practical examples demonstrating both strong and weak visuals, highlighting common pitfalls like misleading axes and unnecessary complexity. I provided guidance to participants, so they can enhance visualization effectiveness under tight deadlines."
  },
  {
    "objectID": "content/blog/data-fest-2025-effective-data-visualizations/index.html#introduction",
    "href": "content/blog/data-fest-2025-effective-data-visualizations/index.html#introduction",
    "title": "Effective Data Visualizations for DataFest",
    "section": "Introduction",
    "text": "Introduction\nThe American Statistical Association (ASA) DataFest is a 48-hour data analysis competition where undergraduate teams work on large, real-world datasets. Creating clear, purposeful, and persuasive visualizations is essential to stand out and communicate your results effectively.\nWhen you’re working under tight deadlines (as you will in DataFest), it pays off to plan your visuals carefully. The goal is to highlight insights quickly and engage both technical and non-technical audiences. Here we’ll discuss:\n\nChoosing the Right Visualization for your data type.\n\nThe differences between static and interactive charts, along with pros and cons.\n\nHelpful resources for further learning.\n\nExamples of good vs. poor visualizations."
  },
  {
    "objectID": "content/blog/data-fest-2025-effective-data-visualizations/index.html#choosing-the-right-visualization-data-type-matters",
    "href": "content/blog/data-fest-2025-effective-data-visualizations/index.html#choosing-the-right-visualization-data-type-matters",
    "title": "Effective Data Visualizations for DataFest",
    "section": "Choosing the Right Visualization: Data Type Matters",
    "text": "Choosing the Right Visualization: Data Type Matters\nData comes in many flavors—continuous, categorical, binary, ranked/ordinal, time series, etc. Each type often suggests a specific set of chart types:\n\nContinuous (e.g., height, weight, temperature): Histograms, boxplots, or scatter plots help show distribution or relationships between continuous variables.\nCategorical (e.g., gender, product category): Bar charts or mosaic plots make category comparisons clear.\nBinary (e.g., yes/no, 0/1): Similar to categorical, but often bars or pie charts if you only have two categories (though pie charts can be misleading with many segments).\nRanked/Ordinal (e.g., Likert scale responses): Bar plots or ordered boxplots help display the progression from one rank to another.\nTime Series (continuous or discrete time): Line charts, area charts, or spark lines for concise trend viewing.\n\nAlways match the question you’re answering to the chart type: a time-trend question typically calls for a line chart, while a comparison across categories suggests a bar chart. This deliberate pairing of data type and visualization style helps keep your visuals clean and intuitive."
  },
  {
    "objectID": "content/blog/data-fest-2025-effective-data-visualizations/index.html#static-vs.-interactive-visualizations",
    "href": "content/blog/data-fest-2025-effective-data-visualizations/index.html#static-vs.-interactive-visualizations",
    "title": "Effective Data Visualizations for DataFest",
    "section": "Static vs. Interactive Visualizations",
    "text": "Static vs. Interactive Visualizations\n\nStatic Visualizations\nStatic plots (like bar charts, line charts, scatter plots) are quick to produce and easy to embed in reports or presentations. Their major advantage is simplicity: you can share a single image (e.g., .png or .pdf), ensuring everyone sees the same information. However, viewers can’t probe or explore the data beyond what you explicitly present.\nPros\n- Easy to produce and share (e.g., in PDFs, slides).\n- Good for final, polished, and unchanging presentations.\n- Typically less prone to technical issues.\nCons\n- No user interaction or deeper data exploration.\n- You might need multiple static visuals to explore different angles.\n\n\nInteractive Visualizations\nInteractive charts allow users to hover, zoom, and filter data in real-time. Tools like Plotly, Altair, Bokeh, or JavaScript libraries like D3.js can bring your data to life. The main advantage is flexibility—audiences can explore aspects of the data that interest them, often uncovering deeper insights on their own.\nPros\n- Engaging for end users; encourages exploration.\n- Can handle richer datasets, with on-demand drilldowns.\n- Allows a single figure to convey multiple layers of detail (via tooltips, dynamic filters, etc.).\nCons\n- Requires more programming and environment setup (JavaScript or Python libraries, etc.).\n- Harder to embed in static documents.\n- Potential performance issues with large datasets or if viewers have older browsers/hardware."
  },
  {
    "objectID": "content/blog/data-fest-2025-effective-data-visualizations/index.html#good-visualizations",
    "href": "content/blog/data-fest-2025-effective-data-visualizations/index.html#good-visualizations",
    "title": "Effective Data Visualizations for DataFest",
    "section": "Good Visualizations",
    "text": "Good Visualizations\n\nExample 1: Good Static Visualization\n\n\n\n\n\nToyota Tacoma Sales in Mexico per Quarter\n\n\n\n\nWhy it’s effective:\n\nClear labels on both axes.\n\nMinimal “chartjunk.”\n\nFocuses on the key trend (sales vs. quarter).\n\n\n\nExample 2: Good Interactive Visualization\n\n\n\n\n\n\nWhy it’s effective:\n\nAllows viewers to hover on each point to see make/model and exact values.\n\nZoom/pan features let readers explore sub-regions of interest.\n\nClear legend and use of color to delineate categories."
  },
  {
    "objectID": "content/blog/data-fest-2025-effective-data-visualizations/index.html#bad-visualizations",
    "href": "content/blog/data-fest-2025-effective-data-visualizations/index.html#bad-visualizations",
    "title": "Effective Data Visualizations for DataFest",
    "section": "Bad Visualizations",
    "text": "Bad Visualizations\n\nExample 1: Poor Visualization (Misleading Axis)\n\n\n\n\n\n\n\n\n\nIssue: The y-axis doesn’t start at zero, so the 3-point increase from 100 to 103 looks massive. This visual can mislead viewers into thinking the change is more significant than it is.\n\n\nExample 2: Poor Visualization (Excessive Chartjunk)\n\n\n\n\n\n\n\n\n\nIssue:\n\nToo many slices, making comparisons difficult.\n\n3D perspective distorts slice sizes, making it harder to read actual proportions.\n\nColors are overwhelming, especially with no legend to clarify each slice."
  },
  {
    "objectID": "content/blog/data-fest-2025-effective-data-visualizations/index.html#conclusion",
    "href": "content/blog/data-fest-2025-effective-data-visualizations/index.html#conclusion",
    "title": "Effective Data Visualizations for DataFest",
    "section": "Conclusion",
    "text": "Conclusion\nData visualization is both an art and a science. Picking the right chart type begins with understanding your data structure (continuous, categorical, binary, etc.) and the question you need to answer. From there, weigh the trade-offs between static and interactive visuals:\n\nStatic plots are simpler to share and best for final, polished presentations.\n\nInteractive charts enable on-the-spot data exploration but can be more complex to build.\n\nAs you gear up for DataFest, remember:\n\nKeep your visuals clean and focused.\n\nMake sure they honestly represent the underlying data.\n\nEncourage engagement where possible (especially for complex insights).\n\nBy planning early, you can leverage visual storytelling to impress both the judges and your peers—helping you stand out in the competition. Good luck, and happy visualizing!"
  },
  {
    "objectID": "content/blog/data-fest-2025-effective-data-visualizations/index.html#recommended-resources",
    "href": "content/blog/data-fest-2025-effective-data-visualizations/index.html#recommended-resources",
    "title": "Effective Data Visualizations for DataFest",
    "section": "Recommended Resources",
    "text": "Recommended Resources\nBelow are three resources to help deepen your visualization skillset:\n\nStatic Visualizations\n“Top 10 Proven Data Visualization Best Practices” GoodData\nThis post emphasizes clarity, simplicity, and matching chart types to data.\n\n\nInteractive Visualizations\n“Interactive Python Plots: Getting Started and Best Packages” Fabi.ai\nA beginner-friendly guide covering Plotly, Altair, and Bokeh. Demonstrates tooltips, panning, and zooming.\n\n\nMisleading/Poor Visualizations\n“Misleading Data Visualization – What to Avoid” Coupler.io\nShowcases common pitfalls (truncated axes, chartjunk, misleading color scales) and how to avoid them."
  },
  {
    "objectID": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html",
    "href": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html",
    "title": "Parameterized Reports",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I automated the report generation process using parameterized R Markdown, which allowed me to update the year parameter dynamically and consistently produce reports without manual intervention. I developed custom themes and employed ggplotly to create interactive time series visualizations of monthly sales data, ensuring each report accurately reflected key trends and patterns. Additionally, I implemented a loop to render separate reports for multiple years, demonstrating efficiency and reproducibility in automated data reporting workflows."
  },
  {
    "objectID": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#objective",
    "href": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#objective",
    "title": "Parameterized Reports",
    "section": "Objective",
    "text": "Objective\nThis project was aimed at streamlining the report generation process by employing parameterized R Markdown. The core idea was to automate the update of report parameters, thus eliminating the manual task of adjusting values for daily or periodic reports. This innovation is designed to significantly reduce time and effort for data scientists, analysts, and engineers, facilitating the production of consistent and timely reports. The automation feature introduced through parameterized R Markdown enhances both productivity and accuracy, offering considerable benefits to business operations."
  },
  {
    "objectID": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#approach",
    "href": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#approach",
    "title": "Parameterized Reports",
    "section": "Approach",
    "text": "Approach\nThe process involved the following key steps:\n\nLibrary Utilization:\n\nEssential R libraries were loaded, including tidyverse for data manipulation, plotly and ggplotly for interactive visualizations, and rmarkdown for rendering parameterized reports.\n\nData Preparation:\n\nThe dataset was loaded and processed. This included renaming columns, converting dates, and filtering data based on the year specified through parameters.\n\nCustom Theme Development:\n\nA custom theme function was created to standardize the appearance of plots across all reports, ensuring a consistent and professional aesthetic.\n\nData Visualization:\n\nInteractive visualizations were crafted using ggplotly to display monthly sales data, highlighting key trends and insights.\n\nAutomated Report Generation:\n\nA loop was implemented to automate the generation of reports for each year, utilizing a function that leverages parameterized R Markdown to render individual reports dynamically.\n\n\n\nHighlights\n\nThe project underscores the efficiency and scalability of automated report generation.\nThe approach allows for the seamless creation of visually appealing and informative reports.\nAutomation minimizes errors and saves considerable time, enhancing decision-making processes with timely and accurate data insights.\n\n\n\nTechnical Notes\n\nThe script included the loading of required libraries, data wrangling steps, and the use of ggplotly for creating dynamic visualizations.\nThe custom theme function (myTheme) ensures a uniform look across all visualizations.\nThe renderReport function and subsequent loop for rendering reports underscore the automation aspect, showcasing the project’s capacity to produce multiple reports efficiently.\n\n\n\nConclusion\nThis project exemplifies the power of automation in report generation using parameterized R Markdown. It offers a scalable solution for producing detailed and aesthetically consistent reports, providing valuable time savings and accuracy for businesses and their data teams."
  },
  {
    "objectID": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#required-libraries",
    "href": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#required-libraries",
    "title": "Parameterized Reports",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(zoo)\nlibrary(rmarkdown)\nlibrary(purrr)"
  },
  {
    "objectID": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#load-dataset-wrangle",
    "href": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#load-dataset-wrangle",
    "title": "Parameterized Reports",
    "section": "Load Dataset & Wrangle",
    "text": "Load Dataset & Wrangle\n\n\nCode\nds &lt;- read_csv(\"../../../assets/datasets/retail.csv\")\n\n#head(ds)\n\nds &lt;- ds %&gt;% \n  rename(ID = ...1) %&gt;%\n  mutate(Month = lubridate::floor_date(Date, 'month')) %&gt;%\n  filter(year(Month) == params$year)\n\nglimpse(ds)\n\n\nRows: 10,042\nColumns: 9\n$ ID         &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ DocumentID &lt;dbl&gt; 716, 716, 716, 716, 716, 716, 716, 460, 461, 462, 463, 464,…\n$ Date       &lt;date&gt; 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23, 2019-09-23…\n$ SKU        &lt;dbl&gt; 1039, 853, 862, 868, 2313, 2355, 2529, 2361, 2723, 655, 254…\n$ Price      &lt;dbl&gt; 381.78, 593.22, 423.73, 201.70, 345.76, 406.78, 542.38, 139…\n$ Discount   &lt;dbl&gt; 67.37254, 0.00034, -0.00119, 35.58814, 61.01966, 101.69458,…\n$ Customer   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 460, 479, 26, 580, 311, 311, 311, 311,…\n$ Quantity   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 4,…\n$ Month      &lt;date&gt; 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01, 2019-09-01…"
  },
  {
    "objectID": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#visualize-the-report",
    "href": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#visualize-the-report",
    "title": "Parameterized Reports",
    "section": "Visualize The Report",
    "text": "Visualize The Report\nI utilized ggplotly, a graphical representation tool, to create an interactive visualization of monthly sales time series data for “CRM and Invoicing system,” which is a wholesale company owned by Sadi Evren. The data for this analysis was obtained from the following Kaggle dataset: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv.\nThe resulting plot provided an insightful representation of the monthly sales data, showcasing trends and patterns in the data that could potentially provide useful information for decision making in the business.\nIn addition to the initial plot, I implemented a for loop to automatically generate multiple reports based on the time series data for each year. This approach eliminated the need for manual report generation, thereby saving time and reducing the risk of errors. The loop enabled the automated generation of separate reports for each year, which provided a comprehensive view of the sales trends over time.\nOverall, the use of ggplotly for data visualization and automation of report generation using a for loop demonstrated an effective approach for efficiently analyzing and presenting data.\n\n\nCode\np &lt;- ds %&gt;%\n  group_by(Month) %&gt;%\n  summarize(AvgSales = round(mean(Price * Quantity),2) ) %&gt;%\n  ggplot(aes(x = Month, \n             y = AvgSales,\n             group = 1,                 #Necessary or else line plot disappears\n             text = paste0(\"Monthly Sales: $\", (round(AvgSales/1000,2)),\"K\" ))) +\n  geom_line(size = 1) + \n  scale_y_continuous(labels = scales::dollar_format(scale = .001, suffix = \"K\")) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%B\") + \n  labs(title = paste0(\"CRM and Invoicing System Sales For FY: \", params$year),\n       caption = \"Source: https://www.kaggle.com/datasets/shedai/retail-data-set?select=file_out.csv\",\n       x = NULL,\n       y = NULL) +\n  myTheme()\n\nggplotly(p, tooltip = c(\"text\")) %&gt;% \n  layout(hovermode = \"x unified\")"
  },
  {
    "objectID": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#function-to-run-parameterized-reports",
    "href": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#function-to-run-parameterized-reports",
    "title": "Parameterized Reports",
    "section": "Function To Run Parameterized Reports",
    "text": "Function To Run Parameterized Reports\n\n\nCode\nrenderReport &lt;- function(year) {\n  quarto::quarto_render(\n    input = \"index.qmd\",\n    output_file = paste0(year, '.html'),\n    execute_params = list(year = year)\n  )\n}"
  },
  {
    "objectID": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#render-all-reports",
    "href": "content/blog/automatic-reports-using-rmarkdown-parameters/index.html#render-all-reports",
    "title": "Parameterized Reports",
    "section": "Render All Reports",
    "text": "Render All Reports\n\n\nCode\n# Renders all 4 Reports (dates range from 2019-2022)\nfor (year in 2019:2022) {\n    renderReport(year)\n}"
  },
  {
    "objectID": "content/about/index.html",
    "href": "content/about/index.html",
    "title": "Brian Cervantes Alvarez",
    "section": "",
    "text": "About Me\nI turn data into stories and solutions that people can actually use. My journey started with a love for math, grew into a passion for data science, and led to completing a master’s degree in Statistics at Oregon State University. My time at OSU shaped me into a statistician who not only understands complex methods, but also values the ethical responsibility that comes with working in data.\nWhile at Oregon State, I worked to bring a modern data science approach to the Department of Statistics—helping faculty upgrade their tools while continuing to learn from their deep expertise. Whether I’m teaching statistical methods, building simulations to tackle real-world problems, or developing research that bridges theory with practice, I’m guided by a core belief: data work should be impactful, responsible, and rooted in integrity.\n\n\nEducational Background\n\nB.S. in Mathematics – Linfield University\nM.S. in Data Science – Willamette University\nM.S. in Statistics – Oregon State University\nCurrent Goal: Job Hunting!\nFuture Aspiration: Work in Industry for 6-7 years before returning for a Ph.D in Data Science.\n\n\n\nExpertise and What I Offer\nHere’s what I’m good at and what I’m always striving to get better at:\n\nPredictive Modeling & Statistical Methods: I love digging into advanced techniques like time-to-event analysis, spatial stats, and simulations—especially when they’re solving real-world problems.\n\nMachine Learning: I’ve trained and interpreted models like Random Forests, SVMs, and Gradient Boosting Machines, figuring out how they can work best for different challenges.\n\nData Science Tools: I know my way around R, Python, and a ton of tools for time series analysis, visualization, and reproducible research.\n\nExperimentation & Consultation: I’ve designed experiments and worked directly with clients, making sure they get honest, useful advice (no fluff).\n\nData Storytelling: Breaking down complicated numbers into visuals and stories people actually care about is one of my favorite parts of the process.\n\nI’m constantly learning new techniques, refining what I already know, and working on projects that combine interactive visualization, classification research, and meaningful insights.\n\n\nProfessional Aspirations\n\nContinue establishing a data science approach within the Department of Statistics, helping current and future faculty enhance their toolkits.\n\nCollaborate with faculty to exchange knowledge, learning from their expertise while contributing my skills.\n\nAdvance my work on detailed, meticulously crafted statistical projects, focusing on simulations to address complex real-world challenges.\n\n\n\nPersonal Interests\nWhen I’m not knee-deep in data or teaching, I’m usually diving into hobbies that keep me curious and entertained:\n\nTravel: I recently had the chance to roam around Europe—Madrid, Barcelona, Venice, Rome, and Paris were all on the itinerary. Every city had its own vibe, and I came back with more than just souvenirs (mostly ideas and a lot of photos).\n\nAudio Enthusiast: I’m a bit obsessed with good sound. Whether it’s tweaking my car audio setup or finding the perfect pair of headphones, I’m always chasing that crisp, balanced sound.\n\nGaming & Tech: Gaming is more of a side quest these days, but I still love building PCs and geeking out over new gadgets. If there’s a shiny, overengineered device out there, it’s probably on my wishlist.\n\nLearning & AI: Lately, I’ve been nerding out on artificial intelligence—seeing how it works and imagining all the cool (and maybe slightly terrifying) ways it could evolve.\n\nAnd yeah, sometimes you’ll catch me scrolling TikTok or YouTube Shorts a little too long—it’s my go-to for random but oddly inspiring rabbit holes.\n\n\nTravels\n\n\n  \n  \n    \n    \n    \n    \n    \n  \n  \n    \n    \n      \n        \n        \n          Spain, Madrid\n          Dec 12th - Dec 13th, 2024\n        \n      \n    \n    \n    \n      \n        \n        \n          Spain, Barcelona\n          Dec 14th, 2024\n        \n      \n    \n    \n    \n      \n        \n        \n          Italy, Venice\n          Dec 15th - Dec 17th, 2024\n        \n      \n    \n    \n    \n      \n        \n        \n          Italy, Rome\n          Dec 18th - Dec 20th, 2024\n        \n      \n    \n    \n    \n      \n        \n        \n          France, Paris\n          Dec 21st - Dec 22nd, 2024\n        \n      \n    \n  \n  \n  \n    \n    Previous\n  \n  \n    \n    Next"
  },
  {
    "objectID": "assets/scripts/blog/readme.html",
    "href": "assets/scripts/blog/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "assets/html/projects/readme.html",
    "href": "assets/html/projects/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "assets/html/about/readme.html",
    "href": "assets/html/about/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "assets/datasets/readme.html",
    "href": "assets/datasets/readme.html",
    "title": "List of Datasets",
    "section": "",
    "text": "Here is a list of datasets available in this repository. Click on each dataset name to access the file directly.\n\nall-ages.csv\namazonProducts.csv\nAppleInc_Stocks.csv\nCard-Transaction_log.csv\ncountries.csv\ncustomerReturnTest.csv\ncustomerReturnTrain.csv\nFinal_dev_pub.csv\nfraud.png\nhealthcareSpending.csv\ninsurance.csv\nmegaGymDataset.csv\nolympics.csv\nopenml_1590.csv\npinot.rds\nretail.csv\nsalary_and_stats.csv\nscuffed_pokedex.csv\nseverityLevels.csv\nsubmission.csv\ntuition_cost.csv\ntuition_income.csv\nwinequality-red.csv\nwinequality-white.csv\n\n\n\nUse read_csv(Assets/Datasets/data.csv)\n\n\n\nUse read_csv(\"../../../Assets/Datasets/data.csv\")\nReplace data.csv with the dataset of your choice"
  },
  {
    "objectID": "assets/datasets/readme.html#load-data-correctly-project-development-in-progress",
    "href": "assets/datasets/readme.html#load-data-correctly-project-development-in-progress",
    "title": "List of Datasets",
    "section": "",
    "text": "Use read_csv(Assets/Datasets/data.csv)"
  },
  {
    "objectID": "assets/datasets/readme.html#render-quarto-documents-deployment",
    "href": "assets/datasets/readme.html#render-quarto-documents-deployment",
    "title": "List of Datasets",
    "section": "",
    "text": "Use read_csv(\"../../../Assets/Datasets/data.csv\")\nReplace data.csv with the dataset of your choice"
  },
  {
    "objectID": "assets/extensions/readme.html",
    "href": "assets/extensions/readme.html",
    "title": "Extensions",
    "section": "",
    "text": "Extensions\nHere are all the extensions that I have sused in my entire website application."
  },
  {
    "objectID": "assets/html/blog/readme.html",
    "href": "assets/html/blog/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "assets/scripts/about/readme.html",
    "href": "assets/scripts/about/readme.html",
    "title": "Custom HTML Files go here",
    "section": "",
    "text": "Custom HTML Files go here"
  },
  {
    "objectID": "assets/scripts/resources/readme.html",
    "href": "assets/scripts/resources/readme.html",
    "title": "",
    "section": "",
    "text": "jdfkldjfl"
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I built a Shiny dashboard to analyze Amazon products in India by integrating multiple datasets, performing extensive data wrangling, and applying advanced CSS theming to enhance design and efficiency. I developed interactive visualizations using Plotly and DT to display average product ratings and review counts across categories, facilitating easy exploration and comparison. My work demonstrates strong skills in data processing, dynamic UI development, and effective dashboard design for clear, insightful data analysis."
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#abstract",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#abstract",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Abstract",
    "text": "Abstract\nThis project focuses on the development of a Shiny application to analyze Amazon products in India. Using a dataset sourced from Kaggle, the application aims to create an informative dashboard showcasing categories with the highest average ratings and reviews. Despite encountering challenges with product categorization, the project prioritizes efficiency and design improvements."
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#introduction",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#introduction",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Introduction",
    "text": "Introduction\nThe objective of this project is to create a Shiny application that explores the Amazon products dataset from India. The dataset, obtained from Kaggle, presented challenges during development, particularly regarding product categories. However, the project’s primary focus was on creating a concise and informative dashboard displaying categories with the highest average ratings and reviews.\nCompared to previous Shiny app development experiences, this project demonstrated improved efficiency, taking significantly less time to complete. Furthermore, advanced CSS theming was implemented to enhance the overall design of the application. While additional features and visualizations could be incorporated, the decision was made to leave them for future exploration."
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#questions",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#questions",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average rating for each category and subcategory?\nWhat is the average review for each category and subcategory?"
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#further-improvement-questions",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#further-improvement-questions",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Further Improvement Questions",
    "text": "Further Improvement Questions\n\nIs there a correlation between the number of ratings and the product rating?\nWhat is the average discount percentage for each main category and subcategory?\nWhat is the price range for each main category and subcategory?\nWhich products have the highest ratings and how do they compare in terms of price and number of ratings?"
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#conclusion",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#conclusion",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Conclusion",
    "text": "Conclusion\nThis project served as a rewarding experience, providing opportunities to enhance data analysis and Shiny app development skills. The Shiny application developed enables users to analyze Amazon products in India and offers insights into categories with the highest average ratings and reviews. Despite encountering challenges related to product categorization, the project prioritized efficiency and introduced improvements in design. Future exploration of additional features and visualizations remains open for further development and enhancement.\n\n\n\n\nFullscreen"
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#load-libraries",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#load-libraries",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\nlibrary(plotly)\nlibrary(DT)\noptions(scipen = 999)"
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#part-1-data-wrangling-multiple-datasets",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#part-1-data-wrangling-multiple-datasets",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 1: Data Wrangling Multiple Datasets",
    "text": "Part 1: Data Wrangling Multiple Datasets\n\n\nCode\n# ORIGINAL DATA WRANGLING \n\ndatasets &lt;- as.data.frame(list.files(path = \"~/Documents/ShinyApps/AmazonProducts/AmazonProductApp\", pattern = \"csv\"))\ncolnames(datasets) &lt;- \"Datasets\"\n\n# Combine all the datasets\nfor (i in length(nrow(datasets))){\n combinedDs &lt;- read_csv(datasets[[i]])\n}\n\namazonProducts &lt;- combinedDs %&gt;%\n mutate(Name = name,\n       MainCategory = factor(str_to_title((sort(main_category)))),\n       SubCategory = factor(sort(sub_category)),\n       ProductImage = image,\n       ProductRating = as.numeric(ratings),\n       NumberOfRatings = as.numeric(gsub(\"\\\\,\",\"\",no_of_ratings)),\n       DiscountPrice = round(as.numeric(gsub(\"[\\\\₹,]\", \"\", discount_price)) / 81.85, 2), # convert from Rupee to USD\n       Price = round(as.numeric(gsub(\"[\\\\₹,]\", \"\", actual_price)) / 81.85, 2),           # convert from Rupee to USD\n       ProductLink = link) %&gt;%\n select(-c(name, \n           main_category, \n           sub_category, \n           image, \n           ratings, \n           no_of_ratings, \n           discount_price, \n           actual_price,\n           link)) %&gt;% \n drop_na() %&gt;%\n filter(!str_detect(SubCategory, \"^All \"))\n\n# amazonProducts %&gt;%\n# write_csv(\"amazonProducts.csv\")\n\n\n\nReload Data\n\n\nCode\nproducts &lt;- read_csv(\"../../../Assets/Datasets/amazonProducts.csv\")"
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#part-2-data-wrangling-for-visualization",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#part-2-data-wrangling-for-visualization",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 2: Data Wrangling For Visualization",
    "text": "Part 2: Data Wrangling For Visualization\n\n\nCode\nproducts &lt;- products %&gt;% mutate(DiscountPrice = round(DiscountPrice82.04,2), Price = round(Price82.04,2))\n\n#For Plot 1\nratings &lt;- products %&gt;% select(-c(ProductImage, Name, ProductLink)) %&gt;% group_by(MainCategory,SubCategory) %&gt;% summarise(AverageRating = mean(ProductRating)) %&gt;% ungroup()\n\nreviews &lt;- products %&gt;% select(-c(ProductImage, Name, ProductLink)) %&gt;% group_by(MainCategory,SubCategory) %&gt;% summarise(AverageReview = mean(NumberOfRatings)) %&gt;% ungroup()\n\n#For Plot 2 \ntop10Products &lt;- products %&gt;% filter(ProductRating &gt; 4.5, NumberOfRatings &gt; 50) %&gt;% group_by(SubCategory) %&gt;% arrange(desc(ProductRating)) %&gt;% slice(1:10) %&gt;% select(-c(ProductImage, Name, ProductLink))\n\n#unique(products$MainCategory)\n\n\n\nColor Theming\n\n\nCode\n#Plot 1 \nnum_colors &lt;- 21 \ncolors &lt;- c(\"#f2f2f2\", \"#ff9900\") \npal1 &lt;- colorRampPalette(colors)(num_colors)\n\nprint(pal1)\n\n#Plot 2 \nnum_colors &lt;- 21 \ncolors &lt;- colors &lt;- c(\"#f2f2f2\",\"#00a8e1\") \npal2 &lt;- colorRampPalette(colors)(num_colors)"
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#part-1-shiny-ui",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#part-1-shiny-ui",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 1: Shiny UI",
    "text": "Part 1: Shiny UI\n\n\nCode\n# Define UI for application \nui &lt;- fluidPage(\n  \n  #Background CSS\n  tags$head(tags$style(HTML('\n  @import url(https://fonts.googleapis.com/css?family=Montserrat&display=swap);\n    body {\n      font-family: Montserrat, sans-serif;\n      background-color: #FF9900;\n    }\n    .dataTables_wrapper {\n      background-color: #fff;\n    }\n    .sidebar {\n      background-color: #fff;\n      width: 3/12;\n      height: 2/12;\n    }\n    .nav-tabs &gt; li &gt; a {\n      color: black;\n      background-color: #00a8e1;\n      border-color: #00a8e1;\n    }'))),\n  \n  # Application title\n  titlePanel(\"Amazon Inc. Product Dashboard (EN.)\"),\n  \n  # Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"MainCategoryChoice\", \n                  label = h3(\"Select Category:\"), \n                  choices = unique(products$MainCategory), \n                  selected = \"Accessories\"),\n      \n      uiOutput(\"SubCategoryChoice\")\n    ),\n    \n    # Tabs\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Plot\", \n                 plotlyOutput(\"RatingsPlot\"),\n                 plotlyOutput(\"ReviewsPlot\")),\n        tabPanel(\"Data\", dataTableOutput(\"myDataTable\"))\n      )\n    )\n  )\n)"
  },
  {
    "objectID": "content/blog/amazon-indian-products-shiny-app/index.html#part-2-shiny-server",
    "href": "content/blog/amazon-indian-products-shiny-app/index.html#part-2-shiny-server",
    "title": "Building A Quick Dashboard For Amazon Products (EN.)",
    "section": "Part 2: Shiny Server",
    "text": "Part 2: Shiny Server\n\n\nCode\n# Define server logic required to draw Dashboard\nserver &lt;- function(input, output) {\n  \n  # Subcategory choices\n  output$SubCategoryChoice &lt;- renderUI({\n    subcategories &lt;- unique(products$SubCategory[products$MainCategory == input$MainCategoryChoice])\n    checkboxGroupInput(\"SubCategoryChoice\", \n                       label = h3(\"Select Subcategories:\"), \n                       choices = subcategories, \n                       selected = subcategories)\n  })\n  \n  # Plot 1: Product Rating\n  output$RatingsPlot &lt;- renderPlotly({\n    ratings %&gt;%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %&gt;% \n      mutate(SubCategory_ordered = factor(SubCategory, \n                                          levels = unique(SubCategory[order(AverageRating)]))) %&gt;%\n      plot_ly(x = ~SubCategory_ordered, \n              y = ~round(AverageRating,2),\n              type = 'bar',\n              marker = list(color = ~pal1[SubCategory_ordered])) %&gt;%\n      add_annotations(x = ~SubCategory_ordered,\n                      y = ~round(AverageRating,2),\n                      text = ~paste0(round(AverageRating,2)),\n                      font = list(color = 'black', \n                                  size = 10),\n                      showarrow = FALSE,\n                      yshift = 5) %&gt;%\n      layout(title = paste0(\"Average Product Rating For \",input$MainCategoryChoice),\n             xaxis = list(title = \"\", tickangle = 45),\n             yaxis = list(title = \"\"),\n             showlegend = FALSE,\n             margin = list(t = 50,\n                           l = 50,\n                           r = 50,\n                           b = 50)) \n  })\n  \n  # Plot 2: Product Reviews\n  output$ReviewsPlot &lt;- renderPlotly({\n    reviews %&gt;%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %&gt;% \n      mutate(SubCategory_ordered = factor(SubCategory, \n                                          levels = unique(SubCategory[order(AverageReview)]))) %&gt;%\n      plot_ly(x = ~SubCategory_ordered, \n              y = ~round(AverageReview),\n              type = 'bar',\n              marker = list(color = ~pal2[SubCategory_ordered])) %&gt;%\n      add_annotations(x = ~SubCategory_ordered,\n                      y = ~round(AverageReview),\n                      text = ~paste0(round(AverageReview)),\n                      font = list(color = 'black', \n                                  size = 10),\n                      showarrow = FALSE,\n                      yshift = 5) %&gt;%\n      layout(title = paste0(\"Average Number of Reviews For \",input$MainCategoryChoice),\n             xaxis = list(title = \"\", tickangle = 45),\n             yaxis = list(title = \"\"),\n             showlegend = FALSE,\n             margin = list(t = 50,\n                           l = 50,\n                           r =50,\n                           b = 50)) \n  })\n  \n  output$myDataTable &lt;- DT::renderDataTable({\n    products %&gt;%\n      filter(MainCategory == input$MainCategoryChoice,\n             SubCategory %in% input$SubCategoryChoice) %&gt;%\n      mutate(ProductImage = sprintf('&lt;img src=\"%s\" width=\"75px\"/&gt;', ProductImage)) %&gt;%\n      DT::datatable(., escape = FALSE, options = list(\n        pageLength = 10,\n        lengthMenu = c(5, 10, 25),\n        scrollY = \"600px\",\n        scrollX = TRUE\n      )) %&gt;%\n      DT::formatStyle(columns = colnames(products), \n                      backgroundColor = styleEqual(c(\"green\", \"white\"), c(\"rgb(51, 102, 0)\", \"rgb(255, 255, 255)\")))\n  })\n  \n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "content/blog/college-student-debt-shiny-app/index.html",
    "href": "content/blog/college-student-debt-shiny-app/index.html",
    "title": "College Debt Shiny App",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I collaborated with Corey Cassell to build an interactive Shiny dashboard that empowers high school students to make informed decisions by estimating salaries, tuition costs, and student debt. I performed extensive data wrangling on multiple datasets, implemented responsive UI elements, and developed custom visualizations using ggplot2 and Plotly to dynamically showcase higher education financial metrics. This project demonstrates my proficiency in integrating complex data pipelines, crafting interactive interfaces, and delivering insightful, user-friendly visualizations that guide future college financial planning."
  },
  {
    "objectID": "content/blog/college-student-debt-shiny-app/index.html#overview",
    "href": "content/blog/college-student-debt-shiny-app/index.html#overview",
    "title": "College Debt Shiny App",
    "section": "Overview",
    "text": "Overview\nIn collaboration with Corey Cassell, our team developed an interactive tool tailored to support students in making informed decisions about their educational journey. This tool offers crucial financial insights, including salary and tuition estimators, as well as a debt calculator, empowering users to navigate their educational choices confidently. By exploring potential salaries, estimating tuition costs, and visualizing projected student debts, individuals can gain valuable perspectives on the financial aspects of their chosen career paths. Recognizing the significance of financial considerations in higher education, our tool comprises four essential components: salary estimator, tuition estimator, debt estimator, and debt calculator. Together, these components provide a comprehensive platform for prospective students to assess potential earnings, anticipate tuition expenses, calculate degree-related debt, and visualize future financial commitments aligned with their chosen majors."
  },
  {
    "objectID": "content/blog/college-student-debt-shiny-app/index.html#college-salary-tuition-debt-tool",
    "href": "content/blog/college-student-debt-shiny-app/index.html#college-salary-tuition-debt-tool",
    "title": "College Debt Shiny App",
    "section": "College Salary, Tuition, & Debt Tool",
    "text": "College Salary, Tuition, & Debt Tool"
  },
  {
    "objectID": "content/blog/college-student-debt-shiny-app/index.html#introduction-and-setup",
    "href": "content/blog/college-student-debt-shiny-app/index.html#introduction-and-setup",
    "title": "College Debt Shiny App",
    "section": "Introduction and Setup",
    "text": "Introduction and Setup\nAs students make crucial decisions about their higher education, it’s imperative to equip them with insights into the financial aspects of their chosen career paths. To address this need, we’ve developed an interactive tool comprising four components: the salary estimator, tuition estimator, debt estimator, and debt calculator. This setup section initializes the necessary libraries and performs data wrangling to prepare the datasets for visualization.\n\n\nSetup + Wrangling\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(RColorBrewer)\nlibrary(rsconnect)\nlibrary(colorspace)\nlibrary(plotly)\nlibrary(shinyWidgets)\nlibrary(scales)\nlibrary(ggplot2)\n\n# Reading necessary files from GitHub\nallAgesDf &lt;- read_csv(\"../../../assets/datasets/all-ages.csv\")\ntuition_cost &lt;- read_csv(\"../../../assets/datasets/tuition_income.csv\")\ntuition &lt;- read_csv(\"../../../assets/datasets/tuition_cost.csv\")\nds4 &lt;- read_csv(\"../../../assets/datasets/salary_and_stats.csv\")\n\n# Wrangling Salary Potential\n# This section prepares data related to salary estimates for various majors.\nsalary &lt;- allAgesDf %&gt;%\n    dplyr::select(Major, P25th, Median, P75th) %&gt;%\n    pivot_longer(c(P25th, Median, P75th),\n        names_to = \"Percentile_Range\", values_to = \"Salary\"\n    ) %&gt;%\n    arrange(Major) %&gt;%\n    mutate(\n        Percentile_Range = as.factor(Percentile_Range),\n        Major = as.factor(Major)\n    )\n\n# Wrangling Potential Tuition Burden\n# This part of the code prepares data related to tuition costs.\ntuition_cost &lt;- tuition_cost %&gt;%\n    filter(year == 2018 & net_cost &gt; 0) %&gt;%\n    arrange(name) %&gt;%\n    mutate(\n        income_lvl = as.factor(income_lvl),\n        name = as.factor(name)\n    )\n\n# Adjusting income levels for readability\ntuition_cost$income_lvl &lt;- recode(tuition_cost$income_lvl,\n    \"0 to 30,000\" = \"$0 to $30,000\",\n    \"30,001 to 48,000\" = \"$30,001 to $48,000\",\n    \"48_001 to 75,000\" = \"$48,001 to $75,000\",\n    \"75,001 to 110,000\" = \"$75,001 to $110,000\",\n    \"Over 110,000\" = \"Over $110,000\"\n)\n\n# Adjusting data for visualization\nsalary$Percentile_Range &lt;- factor(salary$Percentile_Range, levels = c(\"P25th\", \"Median\", \"P75th\"))\nsalary$Percentile_Range &lt;- recode(salary$Percentile_Range,\n    \"P25th\" = \"Early Career\",\n    \"Median\" = \"Middle Career\",\n    \"P75th\" = \"Late Career\"\n)\nsalary$Major &lt;- str_to_title(salary$Major)\nsalary$Major &lt;- gsub(\"And\", \"and\", salary$Major)\n\n# Further data preparation for visualization\ndf &lt;- tuition %&gt;%\n    group_by(state, degree_length, type) %&gt;%\n    filter(!is.na(state) & degree_length != \"Other\") %&gt;%\n    summarise(\n        room_expenses = mean(room_and_board, na.rm = TRUE),\n        inStateTotal = mean(in_state_total, na.rm = TRUE),\n        outOfStateTotal = mean(out_of_state_total, na.rm = TRUE)\n    )\n\ndf$degree_length &lt;- as.factor(df$degree_length)\ndf$type &lt;- as.factor(df$type)\n\ndf &lt;- df %&gt;% rename(\n    \"Room and Board\" = room_expenses,\n    \"In State Tuition\" = inStateTotal,\n    \"Out of State Tuition\" = outOfStateTotal\n)"
  },
  {
    "objectID": "content/blog/college-student-debt-shiny-app/index.html#color-theme",
    "href": "content/blog/college-student-debt-shiny-app/index.html#color-theme",
    "title": "College Debt Shiny App",
    "section": "Color Theme",
    "text": "Color Theme\nThis section defines the visual theme to maintain consistency across all plots and enhance readability.\n\n\nColor Theme\n# Definitions for visual theme\ntitle &lt;- 25\nsubtitle &lt;- 20\nfacet_title &lt;- 25\naxis_title &lt;- 18\ntick_numbers &lt;- 13\ntitle_color &lt;- \"black\"\nbackground &lt;- \"gainsboro\"\nplot_background &lt;- \"gainsboro\"\nfacet_header_background &lt;- \"gainsboro\"\nline_type &lt;- \"solid\"\n\n# Custom theme for plots\nCoreyPlotTheme &lt;- theme(\n    text = element_text(family = \"Futura\"),\n    plot.background = element_rect(fill = background),\n    panel.background = element_blank(),\n    panel.grid.major = element_line(size = .1, linetype = line_type, colour = \"gainsboro\"),\n    panel.grid.minor = element_line(size = .1, linetype = line_type, colour = \"black\"),\n    plot.title = element_text(color = title_color, size = title, family = \"Futura\", hjust = 0.5),\n    plot.subtitle = element_text(color = title_color, size = subtitle, family = \"Futura\", hjust = 0.5),\n    plot.caption = element_text(color = title_color, face = \"bold\", size = tick_numbers, family = \"Futura\", hjust = 0),\n    strip.text = element_text(color = title_color, size = facet_title, family = \"Futura\"),\n    strip.background = element_rect(fill = facet_header_background),\n    axis.text = element_text(color = title_color, size = tick_numbers, family = \"Futura\"),\n    axis.title = element_text(color = title_color, size = axis_title, family = \"Futura\"),\n    axis.ticks.x = element_blank(),\n    legend.title = element_text(color = title_color, size = subtitle, family = \"Futura\"),\n    legend.background = element_rect(fill = plot_background),\n    legend.text = element_text(size = tick_numbers, family = \"Futura\")\n)"
  },
  {
    "objectID": "content/blog/college-student-debt-shiny-app/index.html#interactive-inputs",
    "href": "content/blog/college-student-debt-shiny-app/index.html#interactive-inputs",
    "title": "College Debt Shiny App",
    "section": "Interactive Inputs",
    "text": "Interactive Inputs\nThis section presents the user interface elements allowing users to interact with the data and customize visualizations according to their preferences.\n\nSalary Estimator Selectors\n\n\nSalary Estimator Selectors\n# Inputs for Salary Estimator plot\ninput1 &lt;- inputPanel(\n    selectInput(\"selectInput1\",\n        label = \"Choose your major:\",\n        choices = unique(salary$Major),\n        selected = \"ART HISTORY AND CRITICISM\"\n    ),\n    checkboxGroupInput(\"percentile_choice\",\n        label = \"Pick your career level:\",\n        choices = list(\n            \"Early Career \" = \"Early Career\",\n            \"Middle Career \" = \"Middle Career\",\n            \"Late Career \" = \"Late Career\"\n        ),\n        selected = c(\"Early Career\", \"Middle Career\", \"Late Career\")\n    ),\n)\n\n\n\n\nTuition Estimator Options\n\n\nTuition Estimator Options\n# Inputs for Tuition Estimator plot\ninput2 &lt;- inputPanel(\n    selectInput(\"money\",\n        label = \"Select the type of expense:\",\n        choices = c(\n            \"Room and Board\" = \"Room and Board\",\n           \n\n \"In State Tuition\" = \"In State Tuition\",\n            \"Out of State Tuition\" = \"Out of State Tuition\"\n        ),\n        selected = \"In State Tuition\"\n    ),\n    selectInput(\"state\",\n        label = \"Pick your State:\",\n        choices = unique(df$state),\n        selected = \"Oregon\"\n    ),\n)\n\n\n\n\nDebt Estimator Levels\n\n\nDebt Estimator Levels\n# Inputs for Debt Estimator plot\ninput3 &lt;- inputPanel(\n    selectInput(\"selectInput2\",\n        label = \"Select your university:\",\n        choices = unique(tuition_cost$name),\n        selected = \"Willamette University\"\n    ),\n    checkboxGroupInput(\"checkGroup\",\n        label = \"Select your household income bracket:\",\n        choices = list(\n            \"$0 to $30,000\" = \"$0 to $30,000\",\n            \"$30,001 to $48,000\" = \"$30,001 to $48,000\",\n            \"$48,001 to $75,000\" = \"$48,001 to $75,000\",\n            \"$75,001 to $110,000\" = \"$75,001 to $110,000\",\n            \"Over $110,000\" = \"Over $110,000\"\n        ),\n        selected = c(\n            \"$0 to $30,000\",\n            \"$30,001 to $48,000\",\n            \"$48,001 to $75,000\",\n            \"$75,001 to $110,000\",\n            \"Over $110,000\"\n        )\n    ),\n)\n\n\n\n\nDebt Calculator Choices\n\n\nDebt Calculator Choices\n# Inputs for Debt Calculator plot\ninput4 &lt;- inputPanel(\n    selectInput(\"major_category\",\n        label = \"Pick a major category:\",\n        choices = unique(ds4$major_category),\n        selected = \"Computers & Mathematics\"\n    ),\n)"
  },
  {
    "objectID": "content/blog/college-student-debt-shiny-app/index.html#plots",
    "href": "content/blog/college-student-debt-shiny-app/index.html#plots",
    "title": "College Debt Shiny App",
    "section": "Plots",
    "text": "Plots\nThese plots dynamically visualize various aspects of higher education finances based on user-selected inputs.\n\nSalary Estimator\n\n\nSalary Estimator\n# Plot for Salary Estimator\nplot1 &lt;- renderPlot({\n    salary %&gt;%\n        filter((Major %in% input$selectInput1) & (Percentile_Range %in% input$percentile_choice)) %&gt;%\n        ggplot(aes(x = Percentile_Range, y = Salary, fill = Percentile_Range)) +\n        geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n        geom_label(\n            aes(\n                y = Salary,\n                label = print(paste0(\"$\", round(Salary / 1000, 2), \"K\"))\n            ),\n            show.legend = FALSE,\n            size = 7,\n            family = \"Futura\",\n            fill = \"white\"\n        ) +\n        scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Estimated Salary for \", input$selectInput1),\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nTuition Estimator\n\n\nTuition Estimator\n# Plot for Tuition Estimator\nplot2 &lt;- renderPlot({\n    df %&gt;%\n        filter(state == input$state) %&gt;%\n        ggplot(aes(x = degree_length, y = .data[[input$money]], fill = degree_length)) +\n        geom_col(width = 0.4, color = \"black\", show.legend = FALSE) +\n        facet_wrap(~type) +\n        geom_label(\n            aes(\n                y = .data[[input$money]],\n                label = print(paste0(\"$\", round(.data[[input$money]] / 1000, 2), \"K\"))\n            ),\n            family = \"Oswald\",\n            size = 7,\n            show.legend = FALSE,\n            fill = \"white\"\n        ) +\n        scale_y_continuous(\n            labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3),\n            limits = c(0, 55000)\n        ) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Average \", input$money, \" for \", input$state, \" Universities\"),\n            subtitle = \"For Undergraduate Degrees\",\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nDebt Estimator\n\n\nDebt Estimator\n# Plot for Debt Estimator\nplot3 &lt;- renderPlot({\n    tuition_cost %&gt;%\n        filter((income_lvl %in% input$checkGroup) & (name %in% input$selectInput2)) %&gt;%\n        ggplot(aes(x = income_lvl, y = net_cost, fill = income_lvl)) +\n        geom_col(color = \"black\", width = 0.4, position = \"dodge\", show.legend = FALSE) +\n        geom_label(\n            aes(\n                y = net_cost,\n                label = print(paste0(\"$\", round(net_cost / 1000, 2), \"K\"))\n            ),\n            family = \"Oswald\",\n            size = 7,\n            show.legend = FALSE,\n            fill = \"white\"\n        ) +\n        scale_y_continuous(labels = label_number(prefix = \"$\", suffix = \"K\", scale = 1e-3)) +\n        labs(\n            x = NULL,\n            y = NULL,\n            title = paste0(\"Median Student Loan Debt for \", input$selectInput2),\n            subtitle = \"After Completing Their Undergraduate Degree\",\n            caption = \"Source: TuitionTracker.org @ 2018\"\n        ) +\n        CoreyPlotTheme +\n        scale_fill_brewer(palette = \"PuBuGn\")\n})\n\n\n\n\nDebt Calculator\n\n\nDebt Calculator\n# Plot for Debt Calculator\nplot4 &lt;- renderPlot({\n    ds4 %&gt;%\n        filter(major_category == input$major_category) %&gt;%\n        ggplot(aes(perfect_payback_period, reorder(major, perfect_payback_period), fill = perfect_payback_period)) +\n        geom_col(show.legend = FALSE) +\n        geom_label(aes(label = paste(round(perfect_payback_period, 2), \" yrs.\")),\n            show.legend = FALSE,\n            fill = \"white\",\n            hjust = 1.1\n        ) +\n        theme(\n            axis.title.y = element_blank(),\n            axis.text.x = element_blank()\n        ) +\n        labs(\n            title = \"How Long Will You Be In Debt?\",\n            subtitle = \"Based on Your\n\n Major\",\n            x = \"Time to pay off loans\"\n        ) +\n        CoreyPlotTheme +\n        theme(plot.title = element_text(hjust = 0.5)) +\n        scale_fill_continuous_sequential(\"PuBuGn\")\n})"
  },
  {
    "objectID": "content/blog/college-student-debt-shiny-app/index.html#conclusion",
    "href": "content/blog/college-student-debt-shiny-app/index.html#conclusion",
    "title": "College Debt Shiny App",
    "section": "Conclusion",
    "text": "Conclusion\nThe interactive tool provides valuable resources for high school students considering higher education. By offering comprehensive tools for estimating salaries, tuition costs, and student debt accumulation, we empower students to make informed decisions about their future. This project showcases the power of interactive visualizations in providing crucial information to prospective college students, guiding them towards successful career paths and financial planning."
  },
  {
    "objectID": "content/blog/gt-tables-showcase/index.html",
    "href": "content/blog/gt-tables-showcase/index.html",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I transformed and aggregated data from a Pinot wines dataset to compute key metrics such as average and standard deviation of wine points and prices by province, and enhanced the presentation using advanced design principles. I developed interactive, aesthetically pleasing tables with DT and gt, incorporating sparklines and custom themes to clearly visualize trends and facilitate data comprehension. This project underscores my proficiency in data wrangling, innovative visualization, and the application of design techniques to improve the clarity and impact of data communication."
  },
  {
    "objectID": "content/blog/gt-tables-showcase/index.html#abstract",
    "href": "content/blog/gt-tables-showcase/index.html#abstract",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Abstract",
    "text": "Abstract\nThis project aims to explore alternative techniques for designing visually appealing and comprehensible data tables. Traditional Excel spreadsheets often lack readability and visual impact. By incorporating design principles such as color theory, typography, and layout, we aim to create visually striking data tables that effectively convey information. Additionally, we will evaluate innovative software tools and platforms that offer user-friendly options for creating functional and aesthetically pleasing data tables. Enhancing data presentation is crucial for improving interpretation and understanding."
  },
  {
    "objectID": "content/blog/gt-tables-showcase/index.html#introduction",
    "href": "content/blog/gt-tables-showcase/index.html#introduction",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Introduction",
    "text": "Introduction\nExcel spreadsheets are widely used for organizing and presenting data. However, their conventional format can be tedious and challenging to read, hindering data comprehension. This project seeks to address this limitation by exploring various techniques to design visually appealing and comprehensible data tables.\nThe primary focus is to create data tables that are not only aesthetically pleasing but also convey information effectively. By employing design principles such as color theory, typography, and layout, we aim to enhance the visual impact and readability of data tables. This will involve experimenting with different combinations of colors, fonts, and arrangement patterns to find the most optimal design choices.\nIt is crucial to recognize that the presentation of data plays a significant role in its interpretation and understanding. The traditional Excel format often lacks visual cues to highlight key data points or insights. Therefore, this project seeks to explore new and innovative methods of presenting data tables that not only serve their functional purpose but also captivate the audience with their visual appeal."
  },
  {
    "objectID": "content/blog/gt-tables-showcase/index.html#advanced-data-tables",
    "href": "content/blog/gt-tables-showcase/index.html#advanced-data-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Advanced Data Tables",
    "text": "Advanced Data Tables\n\n\nCode\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(DT)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(countrycode)\nlibrary(downloadthis)\nlibrary(svglite)\n\n\nds &lt;- read_rds(\"../../../assets/datasets/pinot.rds\")\n\n# head(ds)\n\n\n\n\nCode\nds_starter &lt;- ds %&gt;%\n    mutate(\n        province = as.factor(province),\n        price = price,\n        thetaPointMean = mean(points),\n        thetaPriceMean = mean(price)\n    )\n\nds_starter %&gt;%\n    arrange(province, year) %&gt;%\n    select(\n        Province = province,\n        Year = year,\n        Price = price,\n        Points = points,\n        Description = description\n    ) %&gt;%\n    datatable(.,\n        filter = \"bottom\",\n        extensions = \"Buttons\",\n        options = list(\n            dom = \"Bfrtip\",\n            buttons = c(\"copy\", \"csv\", \"excel\"),\n            initComplete = JS(\n                \"function(settings, json) {\",\n                \"$(this.api().table().header()).css({'background-color': '#131F4F', 'color': '#fff'});\",\n                \"}\"\n            )\n        )\n    )\n\n\n\n\n\n\n\n\nCode\nds_summary &lt;- ds_starter %&gt;%\n    group_by(province) %&gt;%\n    arrange(year) %&gt;%\n    summarise(\n        pointsMean = mean(points, na.rm = TRUE),\n        pointsSD = sd(points),\n        priceMean = mean(price, na.rm = TRUE),\n        priceSD = sd(price),\n        points = list(points),\n        price = list(price),\n        .groups = \"drop\"\n    )\n\n\n\n\nCode\nexcel_file_attachment &lt;- ds_summary %&gt;%\n    download_this(\n        output_name = \"Pinot_Noir_Summary\",\n        output_extension = \".xlsx\", # Excel file type\n        button_label = \"Download Excel\",\n        button_type = \"primary\", # change button type\n    )"
  },
  {
    "objectID": "content/blog/gt-tables-showcase/index.html#adding-trend-lines-to-summary-tables",
    "href": "content/blog/gt-tables-showcase/index.html#adding-trend-lines-to-summary-tables",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Adding Trend Lines To Summary Tables",
    "text": "Adding Trend Lines To Summary Tables\n\n\nGT Table Code\nfancyTbl &lt;- ds_summary %&gt;%\n    gt() %&gt;%\n    # format the numeric output to 3 digit rounding\n    fmt_number(\n        columns = c(pointsMean, pointsSD, priceMean, priceSD),\n        decimals = 3\n    ) %&gt;%\n    # create nice labels for a few ugly variable names\n    cols_label(\n        province = \"Province\",\n        pointsMean = \"Avg. Points\",\n        pointsSD = \"Std. Dev. Points\",\n        priceMean = \"Avg. Price\",\n        priceSD = \"Std. Dev. Price\",\n        points = \"Points Trend\",\n        price = \"Price Trend\",\n    ) %&gt;%\n    # Plot the sparklines from the list column\n    gt_plt_sparkline(points,\n        type = \"ref_median\",\n        same_limit = TRUE\n    ) %&gt;%\n    gt_plt_sparkline(price,\n        type = \"ref_median\",\n        same_limit = TRUE\n    ) %&gt;%\n    # use the guardian's table theme\n    gt_theme_guardian() %&gt;%\n    # give hulk coloring to the Mean Human Rights Score\n    gt_hulk_col_numeric(pointsMean) %&gt;%\n    gt_hulk_col_numeric(priceMean) %&gt;%\n    # create a header and subheader\n    tab_header(title = \"Province Pinot Wine Summary\", subtitle = \"Source: Dr. Hendrick\") %&gt;%\n    # attach excel file\n    tab_source_note(excel_file_attachment)\n# save the original as an image\n# gtsave(fancyTbl, \"table.png\")\n# show the table themed in accordance with the page\nfancyTbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProvince Pinot Wine Summary\n\n\nSource: Dr. Hendrick\n\n\nProvince\nAvg. Points\nStd. Dev. Points\nAvg. Price\nStd. Dev. Price\nPoints Trend\nPrice Trend\n\n\n\n\nBurgundy\n90.438\n2.989\n98.035\n132.856\n\n\n\n   89.0\n\n\n\n\n   83.0\n\n\n\nCalifornia\n90.517\n2.831\n47.465\n18.553\n\n\n\n   91.0\n\n\n\n\n   34.0\n\n\n\nCasablanca_Valley\n86.282\n2.428\n21.107\n11.953\n\n\n\n   87.0\n\n\n\n\n   30.0\n\n\n\nMarlborough\n87.550\n2.245\n27.668\n13.833\n\n\n\n   85.0\n\n\n\n\n   25.0\n\n\n\nNew_York\n87.748\n2.268\n25.679\n9.565\n\n\n\n   88.0\n\n\n\n\n   35.0\n\n\n\nOregon\n89.489\n2.663\n44.856\n20.209\n\n\n\n   90.0\n\n\n\n\n   22.0\n\n\n\n\n\n Download Excel"
  },
  {
    "objectID": "content/blog/gt-tables-showcase/index.html#conclusion",
    "href": "content/blog/gt-tables-showcase/index.html#conclusion",
    "title": "Rating Pinot Wines: Is More Expensive Better?",
    "section": "Conclusion",
    "text": "Conclusion\nThis project highlights the importance of visually appealing and comprehensible data tables as an alternative to Excel. By incorporating design principles and exploring innovative tools, we enhance data presentation and interpretation. It calls for adopting alternative techniques to design data tables. By embracing visually appealing formats, we improve data comprehension, communication, and unlock new possibilities for visualization and analysis."
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o1    \n  \n  My blog curates R, Python, and data science projects, highlighting advanced visualizations, real-world analyses, and innovative solutions. From ASA DataFest experiences to interactive dashboards and time series explorations, you’ll find actionable insights at every turn. Each post aims to demonstrate clear, impactful methods for tackling modern data challenges.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Data Visualizations for DataFest\n\n\n\n\n\n\n\n\nApril 3, 2025\n\n\nBrian Cervantes Alvarez\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nPre-Workshop Setup Guide for Quarto-based Portfolios on GitHub Pages\n\n\n\n\n\n\n\n\nJanuary 29, 2025\n\n\nBrian Cervantes Alvarez\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nInteractive Learning with WebR\n\n\n\n\n\n\n\n\nAugust 12, 2024\n\n\nBrian Cervantes Alvarez\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding A Quick Dashboard For Amazon Products (EN.)\n\n\n\nR\n\nShiny\n\nCSS\n\nData Tables\n\nData Visualization\n\nDashboard\n\n\n\nExplore Amazon products in India with our Shiny app. Discover categories with the highest ratings and reviews. Efficient, informative, and visually…\n\n\n\n\n\nApril 21, 2023\n\n\nBrian Cervantes Alvarez\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports\n\n\n\nR\n\nPlotly\n\nReports\n\nTime Series\n\nData Visualization\n\n\n\nAchieving peak productivity in data analysis with automated R Markdown reports, minimizing effort and errors.\n\n\n\n\n\nApril 3, 2023\n\n\nBrian Cervantes Alvarez\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nApple’s Journey In The Stock Market\n\n\n\nR\n\nTime Series\n\nData Visualization\n\n\n\nExplore the dynamic world of Time Series Graphs with R’s plotly. Unveil trends in Apple’s stock market and the impact of groundbreaking innovations\n\n\n\n\n\nMarch 29, 2023\n\n\nBrian Cervantes Alvarez\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nDo You Like Stretching? I Would Reconsider!\n\n\n\nR\n\nData Visualization\n\nPlotly\n\n\n\nStretching enhances flexibility but lacks muscle-building benefits. Weight training and resistance exercises stimulate muscle growth. Supplement…\n\n\n\n\n\nFebruary 24, 2023\n\n\nBrian Cervantes Alvarez\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nRating Pinot Wines: Is More Expensive Better?\n\n\n\nR\n\nData Tables\n\n\n\nRevolutionizing Data Tables: Visual Appeal & Comprehension. Explore techniques for aesthetically pleasing & informative tables. Enhance data…\n\n\n\n\n\nFebruary 20, 2023\n\n\nBrian Cervantes Alvarez\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nPokédex Database\n\n\n\nR\n\nPostgreSQL\n\nData Visualization\n\nData Engineering\n\n\n\nUnveil the power of PostgreSQL! Explore the intricate ETL process, advanced tools, and meticulous schema design that created a functional database.…\n\n\n\n\n\nDecember 5, 2022\n\n\nBrian Cervantes Alvarez\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nCollege Debt Shiny App\n\n\n\nR\n\nShiny\n\nData Visualization\n\nDashboard\n\n\n\nCollaborating with Corey Cassell, we’ve developed an interactive tool aiding students in career and financial planning, empowering informed…\n\n\n\n\n\nDecember 2, 2022\n\n\nBrian Cervantes Alvarez, Corey Cassell\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nThe Impact of Smoking on Health Insurance Premiums: Unveiling the True Costs\n\n\n\nPython\n\nStatistics\n\nJupyter Notebook\n\nData Visualization\n\n\n\nIn this investigation, we dive deep into the impact of smoking habits on medical insurance expenses, carefully examining nuanced differences between…\n\n\n\n\n\nJuly 14, 2022\n\n\nBrian Cervantes Alvarez\n\n6 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html",
    "href": "content/blog/plotly-apple-stocks/index.html",
    "title": "Apple’s Journey In The Stock Market",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I imported and cleaned Apple’s stock dataset spanning from 1981 to the present, and I performed data wrangling to aggregate and transform daily values into yearly averages using custom R functions. I built dynamic time series visualizations with R’s plotly—both in regular and log-normalized formats—applying a custom theme to ensure a consistent, polished appearance, which revealed a substantial upward trend in stock prices post-2010 driven by the success of the iPhone. This project showcased my ability to integrate data manipulation, transformation, and interactive visualization to tell an engaging and insightful story about market trends."
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html#abstract",
    "href": "content/blog/plotly-apple-stocks/index.html#abstract",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Abstract",
    "text": "Abstract\nThis project aims to utilize R’s plotly package to create dynamic and interactive visualizations of Time Series Graphs. The focus of the analysis is on the Apple stock prices from 1981 to the present, with a specific emphasis on identifying notable trends that have emerged over time. The study reveals a significant spike in the company’s stock prices following 2010, attributed to the success of Apple’s iPhone and related products. The research also highlights the impact of Apple’s marketing strategy in maintaining its market value and relevance to changing consumer needs. Overall, the project demonstrates the insightful use of R’s plotly for visualizing Time Series Graphs and providing meaningful explanations of stock market trends."
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html#introduction",
    "href": "content/blog/plotly-apple-stocks/index.html#introduction",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Introduction",
    "text": "Introduction\nVisualizing Time Series Graphs using R’s plotly package provides a powerful means to analyze and interpret complex data. In this project, the focus is on exploring the Apple stock prices over several decades and identifying key trends that have shaped the company’s market performance. The analysis uncovers a significant upsurge in Apple’s stock prices after 2010, which can be attributed to the tremendous success of the iPhone and Apple’s ability to stay in tune with evolving consumer demands.\nFurthermore, the research recognizes the role of Apple’s marketing strategy in maintaining the company’s market value and driving its growth. Apple’s innovative technology and ability to revolutionize the tech industry have contributed significantly to its economic impact. By employing R’s plotly package, this project aims to provide a more meaningful and interactive representation of the Time Series Graphs, enabling a comprehensive understanding of the trends in the stock market."
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html#setup",
    "href": "content/blog/plotly-apple-stocks/index.html#setup",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(scales)\n\n\n\n\nCode\nds &lt;- read_csv(\"../../../assets/datasets/AppleInc_Stocks.csv\")\n#head(ds)"
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html#apply-custom-theme",
    "href": "content/blog/plotly-apple-stocks/index.html#apply-custom-theme",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Apply Custom Theme",
    "text": "Apply Custom Theme\n\n\nCode\nmyTheme &lt;- function(){ \n    font &lt;- \"SF Mono\"   #assign font family up front\n    \n    theme_minimal() %+replace%    #replace elements we want to change\n    \n    theme(\n      \n      #grid elements\n      panel.grid.major.x = element_blank(),    #strip major gridlines\n      panel.grid.minor = element_blank(),    #strip minor gridlines\n      axis.ticks = element_blank(),          #strip axis ticks\n      \n      #since theme_minimal() already strips axis lines, \n      #we don't need to do that again\n      \n      #text elements\n      plot.title = element_text(             #title\n                   family = font,            #set font family\n                   size = 16,                #set font size\n                   face = 'bold',            #bold typeface\n                   hjust = 0,                #left align\n                   vjust = 2),               #raise slightly\n      \n      plot.subtitle = element_text(          #subtitle\n                   family = font,            #font family\n                   size = 12),               #font size\n      \n      plot.caption = element_text(           #caption\n                   family = font,            #font family\n                   size = 9,                 #font size\n                   hjust = 1),               #right align\n      \n      axis.title = element_text(             #axis titles\n                   family = font,            #font family\n                   size = 10),               #font size\n      \n      axis.text = element_text(              #axis text\n                   family = font,            #axis famuly\n                   size = 9),                #font size\n      \n      axis.text.x = element_text(            #margin for axis text\n                    margin=margin(5, b = 10))\n      \n      #since the legend often requires manual tweaking \n      #based on plot content, don't define it here\n    )\n}"
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html#data-wrangling",
    "href": "content/blog/plotly-apple-stocks/index.html#data-wrangling",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n\nCode\nyearlyDs &lt;- ds %&gt;% \n  drop_na() %&gt;%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %&gt;%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %&gt;%\n  summarize(Open = mean(Open),\n            High = mean(High),\n            Low = mean(Low),\n            Close = mean(Close),\n            AdjClose = mean(`Adj Close`),\n            Volume = mean(Volume))\n\n\nlog_yearlyDs &lt;- ds %&gt;% \n  drop_na() %&gt;%\n  mutate(Date = as.Date(Date, \"%m/%d/%Y\")) %&gt;%\n  group_by(Year = lubridate::floor_date(Date, \"year\")) %&gt;%\n  summarize(Open = log(mean(Open)),\n            High = log(mean(High)),\n            Low = log(mean(Low)),\n            Close = log(mean(Close)),\n            AdjClose = log(mean(`Adj Close`)),\n            Volume = log(mean(Volume)))"
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "href": "content/blog/plotly-apple-stocks/index.html#times-series-plots-of-apple-inc.-stock-prices",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Times Series Plots of Apple Inc. Stock Prices",
    "text": "Times Series Plots of Apple Inc. Stock Prices\n\n\nCode\np &lt;- yearlyDs %&gt;%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price Since 1981\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  myTheme()\n\nggplotly(p) %&gt;%\n  layout(hovermode = \"x unified\") %&gt;% \n  style(hovertext = paste0(\" High: $\", round(yearlyDs$High,2)),\n        traces = 1) %&gt;%\n  style(hovertext = paste0(\" Low: $\", round(yearlyDs$Low,2)),\n        traces = 2) %&gt;%\n  style(hovertext = paste0(\" AdjClose: $\", round(yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html#applying-log-norm",
    "href": "content/blog/plotly-apple-stocks/index.html#applying-log-norm",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Applying Log-norm",
    "text": "Applying Log-norm\n\n\nCode\np2 &lt;- log_yearlyDs %&gt;%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = High), color = \"green4\") +\n  geom_line(aes(y = Low), color = \"red4\") +\n  geom_line(aes(y = AdjClose), color = \"grey\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Apple Inc. Stock Price (Log-Normalized)\") +\n  myTheme()\n\nggplotly(p2, tooltip = \"text\") %&gt;%\n  layout(hovermode = \"x unified\", \n         hovertext = paste0(\" Year: \", log_yearlyDs$Year)) %&gt;% \n  style(hovertext = paste0(\" High: \", round(log_yearlyDs$High,2)),\n        traces = 1) %&gt;%\n  style(hovertext = paste0(\" Low: \", round(log_yearlyDs$Low,2)),\n        traces = 2) %&gt;%\n  style(hovertext = paste0(\" AdjClose: \", round(log_yearlyDs$AdjClose,2)),\n        traces = 3)"
  },
  {
    "objectID": "content/blog/plotly-apple-stocks/index.html#conclusion",
    "href": "content/blog/plotly-apple-stocks/index.html#conclusion",
    "title": "Apple’s Journey In The Stock Market",
    "section": "Conclusion",
    "text": "Conclusion\nThe utilization of R’s plotly package for visualizing Time Series Graphs in the context of Apple’s stock prices has yielded valuable insights. The analysis revealed a substantial increase in Apple’s stock prices following 2010, fueled by the success of the iPhone and the company’s adept marketing strategy. The study emphasizes the transformative impact of Apple’s technology and its contributions to the tech industry and economic growth.\nBy employing dynamic and interactive visualizations, this project has enhanced the interpretation of Time Series Graphs, enabling a deeper understanding of the trends in the stock market. The use of R’s plotly package has proven to be a valuable tool in visual data exploration and storytelling. This undertaking serves as a testament to the power of R’s plotly in uncovering meaningful patterns and explaining the dynamics of stock market trends in a more engaging and informative manner."
  },
  {
    "objectID": "content/blog/pokemon-database-data-engineering/index.html",
    "href": "content/blog/pokemon-database-data-engineering/index.html",
    "title": "Pokédex Database",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o3-mini-high    \n  \n  I built a fully operational PostgreSQL database for a comprehensive Pokémon dataset in under two weeks using the ETL methodology, which involved extracting data from multiple sources, cleansing it to remove inconsistencies, and standardizing it before loading. I designed and implemented a robust schema with primary and foreign key constraints for tables such as Pokémon, types, abilities, generations, and moves, then transformed and exported the refined data into a CSV file for further analysis. This project demonstrates my proficiency in data engineering, SQL, and complex data integration, enabling efficient and accurate analysis of detailed Pokémon insights."
  },
  {
    "objectID": "content/blog/pokemon-database-data-engineering/index.html#purpose",
    "href": "content/blog/pokemon-database-data-engineering/index.html#purpose",
    "title": "Pokédex Database",
    "section": "Purpose",
    "text": "Purpose\nThe main objective of this project was to construct a fully operational Postgresql database in a time frame of fewer than two weeks by employing the Extract, Transform, Load (ETL) methodology. The purpose of this approach was to extract data from various sources, transform it into a format that could be easily integrated into the database, and finally load the transformed data into the database.\nThe process involved several intricate steps, including identifying the relevant data sources, cleansing the extracted data to remove inconsistencies, standardizing the data to a uniform format, and applying data validation and verification techniques to ensure accuracy and completeness. Furthermore, it required careful consideration of the database schema, including the design of tables, relationships between tables, and the use of appropriate data types.\nThe successful implementation of this project was dependent on the utilization of cutting-edge technologies and tools, such as data integration software, data profiling tools, and scripting languages. The result was a functional database that can efficiently store and manage data, making it readily available for analysis, decision-making, and reporting purposes."
  },
  {
    "objectID": "content/blog/pokemon-database-data-engineering/index.html#summary",
    "href": "content/blog/pokemon-database-data-engineering/index.html#summary",
    "title": "Pokédex Database",
    "section": "Summary",
    "text": "Summary\nThe inquiry of identifying the optimal base stat Pokemon type sparked my interest, prompting me to delve into the realm of data engineering. In order to craft a well-informed response to this question, I began by utilizing the expansive and multifaceted “Pokémon of Kanto, Johto, and Hoenn Region” dataset to establish a structured and organized database."
  },
  {
    "objectID": "content/blog/pokemon-database-data-engineering/index.html#unleashing-the-power-of-postgresql-building-a-database",
    "href": "content/blog/pokemon-database-data-engineering/index.html#unleashing-the-power-of-postgresql-building-a-database",
    "title": "Pokédex Database",
    "section": "Unleashing the Power of PostgreSQL: Building a Database",
    "text": "Unleashing the Power of PostgreSQL: Building a Database\nThis SQL code creates several tables for storing Pokémon data. The tables include information about Pokémon, their types, abilities, generations, and moves. The code establishes primary keys, foreign key constraints, and defines the data types for each column. These tables form the foundation for a comprehensive Pokémon database, enabling efficient storage and retrieval of Pokémon-related information."
  },
  {
    "objectID": "content/blog/pokemon-database-data-engineering/index.html#data-transformation-and-csv-preparation-in-sql-a-step-by-step-guide",
    "href": "content/blog/pokemon-database-data-engineering/index.html#data-transformation-and-csv-preparation-in-sql-a-step-by-step-guide",
    "title": "Pokédex Database",
    "section": "Data Transformation and CSV Preparation in SQL: A Step-by-Step Guide",
    "text": "Data Transformation and CSV Preparation in SQL: A Step-by-Step Guide\nThis section of the SQL file focuses on transforming and preparing a CSV file for analysis. It involves multiple SELECT statements that extract relevant data from different tables and join them together. The extracted data is then inserted into temporary tables, including ‘temp1’, ‘temp2’, and ‘temp3’, with each step refining the data further. Finally, the transformed data in ‘temp3’ is selected and filtered based on specific conditions, ordered, and then exported to a CSV file named ‘scuffed_pokedex.csv’. This process prepares the data for further analysis and exploration in external tools or applications.\n\n\nSELECT \n    identifier AS pokemon_name, \n    pokemon_types.type_id,\n    pokemon_abilities.ability_id\nINTO temp1\nFROM pokemon\nLEFT JOIN pokemon_types\nON pokemon.id = pokemon_types.pokemon_id\nLEFT JOIN pokemon_abilities\nON pokemon.id  = pokemon_abilities.pokemon_id;\n\n\nSELECT \n    pokemon_name,\n    types.identifier AS pokemon_type,\n    abilities.identifier AS pokemon_ability,\n    types.generation_id AS gen_id,\n    types.id AS type_id\nINTO temp2\nFROM temp1\nLEFT JOIN types\nON temp1.type_id = types.id\nLEFT JOIN abilities\nON temp1.ability_id = abilities.id;\n\n\nDROP TABLE temp3;\nSELECT \n    pokemon_name,\n    pokemon_type,\n    pokemon_ability,\n    generations.identifier AS pokemon_generation,\n    moves.identifier AS pokemon_move,\n    moves.power AS pokemon_power,\n    moves.accuracy AS pokemon_accuracy,\n    moves.pp AS pokemon_pp\nINTO temp3\nFROM temp2\nLEFT JOIN generations\nON temp2.gen_id = generations.main_region_id\nLEFT JOIN moves\nON temp2.type_id = moves.type_id;\n\nSELECT *\nFROM temp3\nWHERE pokemon_power IS NOT NULL \n    AND pokemon_accuracy IS NOT NULL\nORDER BY pokemon_accuracy, pokemon_power;\n\n\nCOPY temp3\nTO '/Users/Shared/Data_503/Datasets/scuffed_pokedex.csv'\nWITH (FORMAT CSV, HEADER);"
  },
  {
    "objectID": "content/blog/pokemon-database-data-engineering/index.html#unveiling-pokémon-insights-analyzing-damage-output-and-accuracy",
    "href": "content/blog/pokemon-database-data-engineering/index.html#unveiling-pokémon-insights-analyzing-damage-output-and-accuracy",
    "title": "Pokédex Database",
    "section": "Unveiling Pokémon Insights: Analyzing Damage Output and Accuracy",
    "text": "Unveiling Pokémon Insights: Analyzing Damage Output and Accuracy\nThrough the power of data analysis and visualization, we have delved into the world of Pokémon to uncover insights about types, damage output, accuracy, and their relationships. Our exploration has shed light on the best Pokémon type for damage output, the most accurate contenders, and the interplay between power and accuracy. By combining the captivating nature of Pokémon with the analytical capabilities of R, we have gained valuable knowledge and set the stage for further investigations in the vast Pokémon universe.\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\n\npokemon &lt;- read_csv(\"../../../Assets/Datasets/scuffed_pokedex.csv\")\n\nnames(pokemon)\n\nnb.cols &lt;- 18\nmycolors &lt;- colorRampPalette(brewer.pal(8, \"YlOrRd\"))(nb.cols)\n\npokemon %&gt;% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %&gt;%\n  group_by(pokemon_type) %&gt;%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = avg_power, y = reorder(pokemon_type, avg_power), fill = reorder(pokemon_type, avg_power)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average power\",\n       y = \"Pokemon type\",\n       title = \"FIRE! The Best Pokemon Type For Damage Output Is...?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\nnb.cols &lt;- 18\nmycolors &lt;- colorRampPalette(brewer.pal(8, \"Blues\"))(nb.cols)\n\npokemon %&gt;% \n  mutate(pokemon_type = str_to_title(pokemon_type)) %&gt;%\n  group_by(pokemon_type) %&gt;%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuracy = mean(pokemon_accuracy, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = avg_accuracy, y = reorder(pokemon_type, avg_accuracy), fill = reorder(pokemon_type, avg_accuracy)))+\n  geom_col(show.legend = FALSE, color = \"black\") +\n  labs(x = \"Average accuracy\",\n       y = \"Pokemon type\",\n       title = \"Ouch! Who wins the bullseye competition?\",\n       subtitle = \"Based on an Average of All Moves Per Pokemon Type\",\n       caption = \"Source: Pokédex of Kanto, Johto, and Hoenn Regions @ Kaggle.com\") +\n  scale_fill_manual(values = mycolors) +\n  theme(plot.background = element_blank(),\n        panel.background = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"grey\"))\n\n\npokemon %&gt;% \n  group_by(pokemon_type) %&gt;%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = avg_power, y = avg_accuarcy, color = pokemon_type)) +\n  geom_point()\n\n\nstats &lt;- pokemon %&gt;% \n  group_by(pokemon_type) %&gt;%\n  summarise(avg_power = mean(pokemon_power, na.rm = TRUE),\n            avg_accuarcy = mean(pokemon_accuracy, na.rm = TRUE))\n            \nmodel &lt;- lm(data = stats, avg_accuracy ~ avg_power)\nplot(model)"
  },
  {
    "objectID": "content/blog/pokemon-database-data-engineering/index.html#presentation",
    "href": "content/blog/pokemon-database-data-engineering/index.html#presentation",
    "title": "Pokédex Database",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "content/forms/thank-you/index.html",
    "href": "content/forms/thank-you/index.html",
    "title": "\n    Thank You!\n  ",
    "section": "",
    "text": "I've received your submission and will get back to you soon!"
  },
  {
    "objectID": "content/privacy/index.html",
    "href": "content/privacy/index.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o1    \n  \n  This policy details how personal and non-personal data is gathered, stored, and utilized on this site under a transparent, user-focused approach. Anonymous usage data helps improve navigation and content, while ensuring minimal disruption to your browsing. AI-assisted elements reflect thoughtful final edits, so if you’re curious about the process or have concerns, you’re encouraged to reach out.\n\n\n\n\n  My Website Policy\n  \n    Thank you for visiting my portfolio. I want to be fully transparent about \n    how I collect, store, and use information to keep improving the site.\n  \n  \n    The goal of this policy is to inform you about the nature of the data gathered, \n    whether you choose to provide it directly (e.g., via contact forms) or it’s \n    collected automatically (e.g., analytics). By clarifying these points, I \n    hope you can navigate and enjoy the site with peace of mind.\n  \n\n\n\n\n  Transparency\n  \n    Your privacy matters to me. This policy outlines how my portfolio gathers data—both\n    automatically (e.g., usage metrics) and voluntarily (e.g., you submitting a form).\n    It also covers how that information is used to enhance your experience.\n  \n  \n  \n    Scope: This policy applies solely to this website and doesn’t extend \n    to external platforms or services you might access through hyperlinks. Once you leave \n    this site, any interaction is bound by that external platform’s privacy rules.\n  \n\n\n\n\n  What Data Is Collected?\n  \n    My data collection can be categorized into two types: anonymous usage data and optional \n    personal data you choose to provide.\n  \n  \n    \n      Pages Visited: The specific pages you explore, helping me see \n      which sections are popular or may need improvement.\n    \n    \n      Time Spent: Average duration on each page to gauge how engaging \n      or valuable the content is.\n    \n    \n      Browser & Device Info: Basic data about your device type, \n      operating system, and browser—this helps me optimize layout and performance.\n    \n    \n      Voluntary Information: Anything you might type into forms, such as \n      your name, email, or message content, if you contact me directly.\n    \n  \n  \n    Google Analytics is used to collect anonymous metrics. No personally identifiable \n    information (PII) is automatically gathered by my analytics unless you voluntarily \n    provide it (e.g., via contact forms).\n  \n\n\n\n\n  How Is This Data Used?\n  \n    I use anonymous usage data to understand which areas of the site resonate with\n    visitors. For example:\n  \n  \n    \n      Enhancing navigation so frequently visited sections are easier to access \n      and explore\n    \n    \n      Creating or refining content in areas where users show high engagement \n      or interest\n    \n    \n      Diagnosing performance bottlenecks or design issues on specific pages\n    \n  \n  \n    Your data is never sold or passed to third parties beyond the analytics \n    processors that supply aggregated metrics. Where personal data is shared \n    voluntarily, such as in a contact form, it’s strictly used to respond \n    to your inquiries or feedback.\n  \n\n\n\n\n  Cookie Usage\n  \n    Cookies are small data files stored on your device that enable better \n    functionality and anonymous statistical analysis. By default, this site \n    primarily employs cookies for analytics. If you choose to block cookies \n    within your browser, the core features of the site will remain accessible, \n    but certain analytics or performance enhancements may not function as intended.\n  \n  \n    Any cookies used here do not collect PII but rather facilitate session \n    continuity and usage metrics. You remain in control: you can delete \n    existing cookies or prevent new ones from being set at any time.\n  \n\n\n\n\n  Your Choices\n  \n    You have several ways to manage your data and privacy preferences:\n  \n  \n    \n      Cookie Consent Banner: If provided, allows you to opt in \n      or out of certain tracking.\n    \n    \n      Browser Extensions: \n      \n        Google Analytics Opt-out\n       \n      or similar tools can block analytics tracking.\n    \n    \n      Browser Settings: Adjust your settings to disable cookies \n      altogether.\n    \n    \n      Personal Data Requests: If you submit personal info (like \n      through a contact form), you can request its deletion or modification at \n      any time by contacting me directly.\n    \n  \n  \n    I encourage you to choose whichever methods align best with your comfort \n    level regarding data collection and personalization.\n  \n\n\n\n\n  Contact Me\n  \n    Got questions, concerns, or suggestions related to how data is managed \n    on this site? Fill out the form below, and I’ll do my best to respond.\n  \n  \n    Constructive feedback about improving this policy is always welcome!\n  \n\n  \n  \n\n  \n    \n    \n    \n    \n    \n\n\n    \n    \n      First Name\n      \n    \n\n    \n    \n      Last Name\n      \n    \n\n    \n    \n      Profession\n      \n        -- Select Your Profession --\n        Student\n        Instructor/Professor\n        Recruiter\n        Professional\n        Other\n      \n    \n\n    \n    \n      Email\n      \n    \n\n    \n    \n      Message\n      \n    \n\n    \n    \n      Send Message\n    \n    \n    Please wait..."
  },
  {
    "objectID": "content/projects/predicting-patient-severity-machine-learning/index.html",
    "href": "content/projects/predicting-patient-severity-machine-learning/index.html",
    "title": "Symptom Severity Analysis",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I conducted symptom-based severity prediction using a Random Forest model, performing exploratory analysis, strategic feature engineering, and comprehensive evaluations. I successfully classified patient severity into Mild, Moderate, and Severe categories, achieving strong model performance metrics (ROC AUC = 0.95, Recall = 90%, Precision = 91.3%, Kappa = 0.85). These results enable personalized healthcare strategies by providing clear classification rules based on symptoms."
  },
  {
    "objectID": "content/projects/predicting-patient-severity-machine-learning/index.html#exploratory-data-analysis",
    "href": "content/projects/predicting-patient-severity-machine-learning/index.html#exploratory-data-analysis",
    "title": "Symptom Severity Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n# Display basic statistics of the dataset\nds.describe()\n\n\n\n\n\n\n\n\nPatient\nFatigue\nWeakness\nDepression\nAnxiety\nDry Skin\nSpasms\nTingling\nHeadaches\nCramps\nNumber of Symptoms\n\n\n\n\ncount\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n300.000000\n\n\nmean\n150.500000\n0.466667\n0.396667\n0.356667\n0.363333\n0.366667\n0.200000\n0.323333\n0.333333\n0.213333\n3.020000\n\n\nstd\n86.746758\n0.499721\n0.490023\n0.479816\n0.481763\n0.482700\n0.400668\n0.468530\n0.472192\n0.410346\n1.703704\n\n\nmin\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n75.750000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2.000000\n\n\n50%\n150.500000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3.000000\n\n\n75%\n225.250000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n1.000000\n1.000000\n0.000000\n4.000000\n\n\nmax\n300.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n8.000000\n\n\n\n\n\n\n\n\n# Visualize the distribution of the target variable 'Final Category'\nplt.figure(figsize=(9, 6))\nds['Final Category'].value_counts().plot(kind='bar')\nplt.xlabel('Symptom Category')\nplt.ylabel('Number of Cases')\nplt.title('Distribution of Final Category')\nplt.xticks(rotation=0)  # Set rotation to 0 degrees for horizontal x-axis labels\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calculate the correlation matrix\n# Select only numeric columns\nnumeric_ds = ds.select_dtypes(include='number')\n\n# Calculate the correlation matrix\ncorr_matrix = numeric_ds.corr()\n\n# Visualize the correlation matrix\nplt.figure(figsize=(10, 7))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "content/projects/predicting-patient-severity-machine-learning/index.html#feature-engineering",
    "href": "content/projects/predicting-patient-severity-machine-learning/index.html#feature-engineering",
    "title": "Symptom Severity Analysis",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nThe code performs some data preprocessing and feature selection to prepare the data for a Random Forest model. Let’s go through each step:\n\nDrop the ID column: The code removes a column named “Patient” from the dataset. This column likely contains unique identifiers for each patient, and since it’s not relevant for our analysis, we can safely remove it.\nCreate dummy columns for “Number of Symptoms”: The code converts a column called “Number of Symptoms” into multiple binary columns, known as dummy variables. Each dummy variable represents a unique value in the original column. This transformation helps us to use categorical data in our machine learning model.\nConcatenate the dummy columns with the original dataframe: The code combines the newly created dummy columns with the original dataset. This ensures that we retain all the existing information while incorporating the transformed categorical data.\nDrop the original “Number of Symptoms” column: Since we have created the dummy columns, we no longer need the original “Number of Symptoms” column. Therefore, the code removes this column from the dataset.\nSeparate the features (X) and the target variable (y): The code splits the dataset into two parts. The features, represented by the variable X, contain all the columns except the “Final Category” column, which is the target variable we want to predict. The target variable, represented by the variable y, contains only the “Final Category” column. This is the severity cases of ‘Mild’, ‘Moderate’ and ‘Severe’\nPerform feature selection using Random Forest: The code utilizes a machine learning algorithm called Random Forest to identify the most important features for each category in the target variable. It trains a separate Random Forest model for each category and determines the top three features that contribute the most to predicting that category.\nStore the top features for each category: The code stores the top three features for each category in a dictionary called “top_features.” Each category is represented by a label, and the corresponding top features are stored as a list.\nPrint the top 3 features for each label: The code then prints the top three features for each category in the target variable. This information helps us understand which features are most influential in determining the predicted category.\n\nOverall, this code prepares the data by transforming categorical data into a suitable format and identifies the top features that contribute to predicting different categories. This sets the stage for further analysis and building the final machine learning model based on these selected features.\n\n# Drop the ID column\nds.drop('Patient', axis=1, inplace=True)\n\n# Create dummy columns from the \"Number of Symptoms\" column\ndummy_cols = pd.get_dummies(ds['Number of Symptoms'], prefix='Symptom')\n\n# Concatenate the dummy columns with the original dataframe\nds = pd.concat([ds, dummy_cols], axis=1)\n\n# Drop the original \"Number of Symptoms\" column\nds.drop('Number of Symptoms', axis=1, inplace=True)\n\nds.head(10)\n\n\n\n\n\n\n\n\nFinal Category\nFatigue\nWeakness\nDepression\nAnxiety\nDry Skin\nSpasms\nTingling\nHeadaches\nCramps\nSymptom_0\nSymptom_1\nSymptom_2\nSymptom_3\nSymptom_4\nSymptom_5\nSymptom_6\nSymptom_7\nSymptom_8\n\n\n\n\n0\nMild\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\nMild\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n2\nMild\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\nMild\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nMild\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nMild\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nMild\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\nMild\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n8\nMild\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n9\nMild\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n# Separate the features (X) and the target variable (y)\nX = ds.drop('Final Category', axis=1)\ny = ds['Final Category']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store top features per label\ntop_features = {}\n\n# Perform feature selection for each label\nfor label in y_train.unique():\n    # Encode the target variable for the current label\n    y_label_train = (y_train == label).astype(int)\n    y_label_test = (y_test == label).astype(int)\n    \n    # Create a Random Forest model\n    rf_model = RandomForestClassifier(random_state=60)\n    \n    # Train the model\n    rf_model.fit(X_train, y_label_train)\n    \n    # Get feature importances\n    feature_importances = rf_model.feature_importances_\n    \n    # Sort features by importance in descending order\n    sorted_features = sorted(zip(X_train.columns, feature_importances), key=lambda x: x[1], reverse=True)\n    \n    # Get the top 3 features for the current label\n    selected_features = [feature for feature, _ in sorted_features[:3]]\n    \n    # Store the top features for the current label\n    top_features[label] = selected_features\n\n# Print the top 3 features for each label\nfor label, features in top_features.items():\n    print(f\"Top 3 features for {label}:\")\n    print(features)\n    print()\n\nTop 3 features for Severe:\n['Cramps', 'Spasms', 'Symptom_4']\n\nTop 3 features for Mild:\n['Depression ', 'Symptom_1', 'Symptom_3']\n\nTop 3 features for Moderate:\n['Symptom_3', 'Depression ', 'Cramps']\n\n\n\n\nFeatures Explained\nThe feature selection process aims to identify the most important factors (features) that contribute to determining the severity of a patient’s condition. In this case, the severity levels are categorized as “Mild,” “Moderate,” and “Severe.” The top three features that were found to be most indicative for each severity level are as follows:\nFor patients rated as “Mild”:\n\nHas 1 symptom:\n\nThis feature indicates the presence of a specific symptom (let’s say, symptom X) that is associated with a mild severity rating. If a patient has symptom X, it suggests a higher likelihood of being classified as “Mild.”\n\nDepression:\n\nThis feature refers to the presence or absence of depression symptoms in the patient. The presence of depression symptoms is considered important in determining a mild severity rating.\n\nHas 3 symptoms:\n\nThis feature represents the presence of 3 symptoms (let’s call them symptoms A,B,C) that are associated with a mild severity rating. If a patient has symptoms A,B,C, it suggests a higher likelihood of being classified as “Mild.”\nGiven this information, it can be recommended that a threshold is established. It can be inferred that if a patient has 1-3 symptoms and/or has depression, they can be classified as “Mild.” This is addressed with the model later in the study.\nFor patients rated as “Moderate”:\n\nHas 3 symptoms:\n\nThis feature represents the presence of 3 symptoms (let’s call them symptoms A,B,C again) that are associated with a moderate severity rating. If a patient has symptoms A,B,C, it suggests a higher likelihood of being classified as “Moderate.”\n\nDepression:\n\nThe presence or absence of depression symptoms also plays a role in determining a moderate severity rating.\n\nCramps:\n\nThis feature represents the presence or absence of cramps in the patient. The presence of cramps is considered important in predicting a moderate severity rating.\nGiven this information, another threshold can be established. It can be inferred that if a patient has at least 3 symptoms and/or has depression and/or cramps, they can be classified as “Moderate.” This is addressed with the model later in the study.\nFor patients rated as “Severe”:\n\nCramps:\n\nThis feature indicates the presence or absence of cramps, which is associated with a severe severity rating. If a patient has cramps, it suggests a higher likelihood of being classified as “Severe.”\n\nSpasms:\n\nThis feature refers to the presence or absence of muscle spasms in the patient. The presence of spasms is considered important in predicting a severe severity rating.\n\nHas 4 symptoms:\n\nThis feature represents the presence of symptoms (let’s call it symptoms A,B,C,D) that are associated with a severe severity rating. If a patient has symptoms , it suggests a higher likelihood of being classified as “Severe.”\nGiven this information, a last threshold can be established. It can be inferred that if a patient has at least 4 symptoms and/or has cramps and/or spasms, they can be classified as “Severe.” This is addressed with the model later in the study.\nIn summary, the top features identified for each severity level provide insights into the specific symptoms and factors that contribute to determining the severity of a patient’s condition. By considering the presence or absence of these features, the model can make predictions about the severity rating of a patient’s condition, helping healthcare professionals assess the level of severity and provide appropriate care and treatment."
  },
  {
    "objectID": "content/projects/predicting-patient-severity-machine-learning/index.html#random-forest-model",
    "href": "content/projects/predicting-patient-severity-machine-learning/index.html#random-forest-model",
    "title": "Symptom Severity Analysis",
    "section": "Random Forest Model",
    "text": "Random Forest Model\nThe code performs machine learning tasks using a Random Forest model with the selected features from the earlier model. Let’s go through each step:\n\nSeparate the features (X) and the target variable (y) using only the top features: The code selects specific features from the dataset based on their importance in predicting the target variable. These features are obtained from the “top_features” dictionary, which contains the top three features for each category (Mild, Moderate, and Severe) in the target variable.\nEncode the target variable using label encoding: The target variable “Final Category” is a categorical variable. To use it in the machine learning model, we need to convert it into numeric form. The code uses label encoding, which assigns a unique numeric value to each category in the target variable.\nCreate a random forest model using the top features: The code initializes a Random Forest model with a specific random state. Random Forest is an ensemble learning algorithm that combines multiple decision trees to make predictions.\nPerform 10-fold cross-validation: The code evaluates the performance of the Random Forest model using a technique called cross-validation. Cross-validation helps estimate how well the model will generalize to new, unseen data. In this case, 10-fold cross-validation is performed, which means the dataset is divided into 10 equal parts (folds). The model is trained and tested 10 times, with each fold serving as the test set once.\nPrint the cross-validation scores: The code prints the cross-validation scores obtained from each fold. These scores indicate how well the model performed on each fold. Additionally, the mean cross-validation score is calculated, which provides an overall measure of the model’s performance.\nTrain the model: The code trains the Random Forest model using all the available data, as specified by the features (X) and target variable (y).\nMake predictions on the training set: The code uses the trained model to make predictions on the same dataset that was used for training. This helps evaluate how well the model can predict the target variable for the given features.\nPrint the confusion matrix: The code prints a confusion matrix, which is a table that shows the number of correct and incorrect predictions made by the model. It provides insights into the model’s performance for each category in the target variable.\nCalculate and print other evaluation metrics: The code calculates additional evaluation metrics such as ROC AUC score, recall, precision, and Kappa metric. These metrics help assess the model’s performance in terms of classification accuracy, sensitivity, precision, and agreement beyond chance.\n\nOverall, this code builds a Random Forest model using selected features, evaluates its performance through cross-validation, and provides insights into the model’s predictive capabilities using various evaluation metrics. The goal is to understand how well the model can predict the categories in the target variable based on the selected features.\n\n# Separate the features (X) and the target variable (y) using only the top features\nX = ds[top_features['Mild'] + top_features['Moderate'] + top_features['Severe']]\ny = ds['Final Category']\n\n# Encode the target variable using label encoding\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Create a random forest model\nrf_model = RandomForestClassifier(random_state=60)\n\n# Define the parameter grid for hyperparameter optimization\nparam_grid = {\n    'max_depth': [None],\n    'min_samples_leaf': [4],\n    'min_samples_split': [2],\n    'n_estimators': [100]\n}\n\n# Perform hyperparameter optimization using grid search and 10-fold cross-validation\ngrid_search = GridSearchCV(rf_model, param_grid, cv=10)\ngrid_search.fit(X, y)\n\n# Get the best random forest model with optimized hyperparameters\nrf_model = grid_search.best_estimator_\n\n# Perform 10-fold cross-validation with the optimized model\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Print the cross-validation scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())\nprint()\n\nCross-Validation Scores: [0.86666667 0.9        1.         0.9        0.96666667 0.9\n 0.9        0.83333333 0.86666667 0.8       ]\nMean CV Score: 0.8933333333333333\n\n\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=87)\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\n\n# Calculate and print ROC AUC score\nroc_auc = roc_auc_score(y_test, rf_model.predict_proba(X_test), multi_class='ovr')\nprint(\"ROC AUC:\", roc_auc)\nprint()\n\n# Calculate and print recall score\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint(\"Recall:\", recall)\nprint()\n\n# Calculate and print precision score\nprecision = precision_score(y_test, y_pred, average='weighted')\nprint(\"Precision:\", precision)\nprint()\n\n# Calculate and print Kappa metric\nkappa = cohen_kappa_score(y_test, y_pred)\nprint(\"Kappa:\", kappa)\nprint()\n\nConfusion Matrix:\n[[18  4  0]\n [ 0 20  1]\n [ 0  1 16]]\n\nROC AUC: 0.9480560359313329\n\nRecall: 0.9\n\nPrecision: 0.9133333333333333\n\nKappa: 0.8493723849372385"
  },
  {
    "objectID": "content/projects/predicting-patient-severity-machine-learning/index.html#metrics-explained",
    "href": "content/projects/predicting-patient-severity-machine-learning/index.html#metrics-explained",
    "title": "Symptom Severity Analysis",
    "section": "Metrics Explained",
    "text": "Metrics Explained\nLet’s break down the provided metrics based on the given confusion matrix:\n\nConfusion Matrix\nThe confusion matrix is a table that helps us understand the performance of a classification model. It shows the predicted labels versus the actual labels for each class. In this case, the confusion matrix has three rows and three columns, representing the three severity rating categories. In this case, since I set the test set to be 20% of the sample of 300, there is 60 total patients that were randomly tested.\n\n\\begin{array}{c|ccc}\n& {\\text{Actual}} \\\\\n\\text{Predicted} & \\text{Mild} & \\text{Moderate} & \\text{Severe} \\\\\n\\hline\n\\text{Mild} & 18 & 4 & 0 \\\\\n\\text{Moderate} & 0 & 20 & 1 \\\\\n\\text{Severe} & 0 & 1 & 16 \\\\\n\\end{array}\n\nFirst, the number 18 in the first row and column indicates that the model correctly predicted 18 patients as “mild” which coincides with the actual “mild” severity label. Next, the number 4 in the first row and second column indicates that the model incorrectly predicted 4 patients as “moderate” when their actual severity rating was “mild.” Lastly, the number 0 in the first row and third column indicates that the model correctly predicted 0 patients as being “severe”, meaning that those patients were labeled correctly as “mild”.\nSimilarly, the other numbers in the confusion matrix represent the model’s predictions for the other severity rating categories. Given that this was a dataset with only 300 patients, it is very intriguing that it can label each patient with high accuarcy.\n\n\nROC Curve\nNow, the ROC AUC (Receiver Operating Characteristic Area Under the Curve) is a measure of the model’s ability to distinguish between different severity ratings. It represents the overall performance of the model across all severity levels. The value of 0.948 indicates a high level of performance, close to 1, suggesting that the model has good predictive capability for distinguishing between severity ratings.\n\n# Calculate predicted probabilities for each class\ny_pred_proba = rf_model.predict_proba(X_test)\n\n# Compute the ROC curve for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor class_index in range(len(label_encoder.classes_)):\n    fpr[class_index], tpr[class_index], _ = roc_curve(\n        (y_test == class_index).astype(int), y_pred_proba[:, class_index]\n    )\n    roc_auc[class_index] = roc_auc_score(\n        (y_test == class_index).astype(int), y_pred_proba[:, class_index]\n    )\n\n# Plot the ROC curve for each class\nplt.figure(figsize=(9, 8))\nfor class_index in range(len(label_encoder.classes_)):\n    plt.plot(\n        fpr[class_index],\n        tpr[class_index],\n        label=f\"Class {label_encoder.classes_[class_index]} (AUC = {roc_auc[class_index]:.2f})\",\n    )\n\n# Plot random guessing line\nplt.plot([0, 1], [0, 1], \"k--\")\n\n# Set plot properties\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\n\n# Show the plot\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRecall vs. Precision Curve\nRecall, also known as sensitivity or true positive rate, measures the proportion of actual positive instances (patients with a particular severity rating) that the model correctly identified. A recall value of 0.9 means that the model identified nearly 90.0% of the patients with their correct severity rating.\nPrecision measures the proportion of instances that the model predicted correctly as positive (patients with a particular severity rating) out of all instances it predicted as positive. A precision value of 0.913 indicates that out of all the patients the model identified as having a specific severity rating, 91.3% of them were correct.\n\n# Calculate precision and recall values for each class\nprecision, recall, thresholds = precision_recall_curve(y_test, rf_model.predict_proba(X_test)[:, 1], pos_label=1)\n\n# Plot the recall vs. precision curve\nplt.figure(figsize=(9, 8))\nplt.plot(recall, precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Recall vs. Precision Curve')\nplt.grid(True)\nplt.show()\nplt.clf()\nplt.close()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nKappa\nThe Kappa statistic is a measure of agreement between the model’s predictions and the actual severity ratings, taking into account the possibility of agreement occurring by chance. A Kappa value of 0.849 indicates a substantial level of agreement between the model’s predictions and the actual severity ratings, suggesting a reliable performance of the model.\nOverall, these metrics indicate that the model has performed well in predicting the severity ratings of the patients, with high accuracy, good distinction between severity levels, and substantial agreement with the actual severity ratings provided by physicians."
  },
  {
    "objectID": "content/projects/predicting-patient-severity-machine-learning/index.html#visualizing-random-forests-best-decision-tree",
    "href": "content/projects/predicting-patient-severity-machine-learning/index.html#visualizing-random-forests-best-decision-tree",
    "title": "Symptom Severity Analysis",
    "section": "Visualizing Random Forest’s Best Decision Tree",
    "text": "Visualizing Random Forest’s Best Decision Tree\n\n# Perform 10-fold cross-validation\ncv_scores = cross_val_score(rf_model, X, y, cv=10)\n\n# Find the index of the best decision tree\nbest_tree_index = np.argmax(cv_scores)\n\n# Get the best decision tree from the random forest\nbest_tree = rf_model.estimators_[best_tree_index]\n\n# Visualize the best decision tree using matplotlib\nplt.figure(figsize=(12, 12))\ntree.plot_tree(best_tree, feature_names=X.columns, class_names=label_encoder.classes_, filled=True)\n\nplt.show()"
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html",
    "title": "Pinot Province Prediction",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I developed a predictive Random Forest model to classify the province of wine origin using critic-provided descriptions, utilizing advanced text preprocessing and feature engineering techniques such as tokenization, stemming, and stop-word removal. Through hyperparameter tuning and cross-validation, I achieved a strong Kappa score of 82%, indicating high accuracy and reliability in predicting wine origins based on sensory descriptors. The project demonstrates my proficiency in NLP, machine learning model building, and rigorous evaluation methodologies."
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html#purpose",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html#purpose",
    "title": "Pinot Province Prediction",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of this project was to develop a predictive model for identifying the province of origin for wines based on descriptions provided by critics. To achieve this goal, a random forest model was built and evaluated for its performance, achieving a kappa score of 0.82. This project aimed to provide a useful tool for wine connoisseurs and industry professionals in identifying the origin of wines based on their sensory characteristics."
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html#setup",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html#setup",
    "title": "Pinot Province Prediction",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(tidytext)\nlibrary(SnowballC)"
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html#feature-engineering",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html#feature-engineering",
    "title": "Pinot Province Prediction",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nwine = read_rds(\"../../../assets/datasets/pinot.rds\") \n\nwine_words &lt;- function(df, j, stem = T){ \n  data(stop_words)\n  words &lt;- df %&gt;%\n    unnest_tokens(word, description) %&gt;%\n    anti_join(stop_words) %&gt;%\n    filter(str_detect(string = word, pattern = \"[a-z+]\")) %&gt;% # get rid weird non alphas \n    filter(str_length(word) &gt;= 3) %&gt;% # get rid of strings shorter than 3 characters \n    filter(!(word %in% c(\"wine\",\"pinot\", \"vineyard\"))) %&gt;%\n    group_by(word) %&gt;%\n    mutate(total=n()) %&gt;%\n    ungroup()\n  \n  if(stem){\n    words &lt;- words %&gt;% \n      mutate(word = wordStem(word))\n  }\n  \n  words &lt;- words %&gt;% \n    count(id, word) %&gt;% \n    group_by(id) %&gt;% \n    mutate(exists = (n&gt;0)) %&gt;% \n    ungroup %&gt;% \n    group_by(word) %&gt;% \n    mutate(total = sum(n)) %&gt;% \n    filter(total &gt; j) %&gt;% \n    pivot_wider(id_cols = id,\n                names_from = word,\n                values_from = exists,\n                values_fill = list(exists=0)) %&gt;% \n    right_join(select(df,id,province)) %&gt;% \n    select(-id) %&gt;% \n    mutate(across(-province, ~replace_na(.x, F)))\n}\n\nwino &lt;- wine_words(wine, j = 190, stem = T)"
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html#specification",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html#specification",
    "title": "Pinot Province Prediction",
    "section": "Specification",
    "text": "Specification\n\nset.seed(504) \n\nctrl &lt;- trainControl(method = \"cv\", number = 3)\n\n\nwine_index &lt;- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain &lt;- wino[ wine_index, ]\ntest &lt;- wino[-wine_index, ]\n\nfit &lt;- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             ntree = 100,\n             tuneLength = 15,\n             nodesize = 10,\n             verbose = TRUE,\n             trControl = ctrl,\n             metric = \"Kappa\")"
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html#model-performance",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html#model-performance",
    "title": "Pinot Province Prediction",
    "section": "Model Performance",
    "text": "Model Performance\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               216          5                 0           0        1\n  California              13        758                14          23       19\n  Casablanca_Valley        0          0                10           0        0\n  Marlborough              0          0                 0          12        0\n  New_York                 0          0                 0           0        0\n  Oregon                   9         28                 2          10        6\n                   Reference\nPrediction          Oregon\n  Burgundy              10\n  California            62\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               475\n\nOverall Statistics\n                                          \n               Accuracy : 0.8793          \n                 95% CI : (0.8627, 0.8945)\n    No Information Rate : 0.4728          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8069          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9076            0.9583                 0.384615\nSpecificity                   0.9889            0.8515                 1.000000\nPos Pred Value                0.9310            0.8526                 1.000000\nNeg Pred Value                0.9847            0.9579                 0.990379\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1291            0.4531                 0.005977\nDetection Prevalence          0.1387            0.5314                 0.005977\nBalanced Accuracy             0.9482            0.9049                 0.692308\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.266667         0.00000        0.8684\nSpecificity                    1.000000         1.00000        0.9512\nPos Pred Value                 1.000000             NaN        0.8962\nNeg Pred Value                 0.980132         0.98446        0.9370\nPrevalence                     0.026898         0.01554        0.3270\nDetection Rate                 0.007173         0.00000        0.2839\nDetection Prevalence           0.007173         0.00000        0.3168\nBalanced Accuracy              0.633333         0.50000        0.9098"
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html#re-fit-and-evaluation",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html#re-fit-and-evaluation",
    "title": "Pinot Province Prediction",
    "section": "Re-fit and evaluation",
    "text": "Re-fit and evaluation\n\nset.seed(1504)\n\nwine_index &lt;- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain &lt;- wino[ wine_index, ]\ntest &lt;- wino[-wine_index, ]\n\n# example spec for knn\nfit_final &lt;- train(province ~ .,\n             data = train, \n             method = \"rf\",\n             tuneGrid = fit$bestTune) \n# The last line means we will fit a model using the best tune parameters your CV found above."
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html#final-model-performance",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html#final-model-performance",
    "title": "Pinot Province Prediction",
    "section": "Final Model Performance",
    "text": "Final Model Performance\n\nconfusionMatrix(predict(fit_final, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               219          7                 0           0        0\n  California               7        752                12          13       20\n  Casablanca_Valley        0          0                12           0        0\n  Marlborough              0          1                 0          22        0\n  New_York                 0          0                 0           0        1\n  Oregon                  12         31                 2          10        5\n                   Reference\nPrediction          Oregon\n  Burgundy               7\n  California            55\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon               485\n\nOverall Statistics\n                                          \n               Accuracy : 0.8912          \n                 95% CI : (0.8753, 0.9057)\n    No Information Rate : 0.4728          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8274          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.9202            0.9507                 0.461538\nSpecificity                   0.9902            0.8787                 1.000000\nPos Pred Value                0.9399            0.8754                 1.000000\nNeg Pred Value                0.9868            0.9521                 0.991571\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1309            0.4495                 0.007173\nDetection Prevalence          0.1393            0.5134                 0.007173\nBalanced Accuracy             0.9552            0.9147                 0.730769\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                     0.48889       0.0384615        0.8867\nSpecificity                     0.99939       1.0000000        0.9467\nPos Pred Value                  0.95652       1.0000000        0.8899\nNeg Pred Value                  0.98606       0.9850478        0.9450\nPrevalence                      0.02690       0.0155409        0.3270\nDetection Rate                  0.01315       0.0005977        0.2899\nDetection Prevalence            0.01375       0.0005977        0.3258\nBalanced Accuracy               0.74414       0.5192308        0.9167"
  },
  {
    "objectID": "content/projects/predicting-wine-province-machine-learning/index.html#conclusion",
    "href": "content/projects/predicting-wine-province-machine-learning/index.html#conclusion",
    "title": "Pinot Province Prediction",
    "section": "Conclusion",
    "text": "Conclusion\nThe kappa value of 0.82 for our random forest model signifies a high level of precision and accuracy, reflecting a very good agreement with the actual outcomes. This statistical measure, important for assessing classification model performance, confirms the model’s efficacy in predicting the correct class labels. Thus, with a kappa value of 0.82, the model demonstrates reliable predictive performance."
  },
  {
    "objectID": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html",
    "href": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html",
    "title": "Healthcare Spending Analysis",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I conducted a comprehensive statistical analysis of U.S. healthcare spending from 1980 to 2014, performing extensive data wrangling, visualization, and regional comparisons to uncover expenditure trends. My analysis revealed healthcare costs increased by over 500% nationally, with Personal Healthcare expenses alone rising from approximately $10,000 to nearly $80,000 per capita. Notably, significant regional disparities were identified, with spending in traditionally cheaper regions like New England, Plains, and Rocky Mountains still growing by 18%, highlighting persistent nationwide increases."
  },
  {
    "objectID": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#abstract",
    "href": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#abstract",
    "title": "Healthcare Spending Analysis",
    "section": "Abstract",
    "text": "Abstract\nSince 1980, healthcare costs in the United States have been consistently increasing across all categories. Various factors contribute to this rise, such as population growth and higher wages for doctors. This report examines expenditure trends from 1980 to 2005, extending up to 2014. The findings reveal an unprecedented surge in healthcare costs across every sector in the United States. Consequently, this report sheds light on the reasons behind the country’s reputation as one of the world’s most expensive nations in terms of healthcare."
  },
  {
    "objectID": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#introduction",
    "href": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#introduction",
    "title": "Healthcare Spending Analysis",
    "section": "Introduction",
    "text": "Introduction\nHealthcare plays a crucial role in our lives, providing essential support for our well-being and longevity. However, healthcare spending continues to soar annually. This report uncovers the alarming reality of escalating healthcare expenditure, presenting a visual representation of each component. It explores overall national spending and delves into individual categories, demonstrating the persistent upward trend in healthcare costs."
  },
  {
    "objectID": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#background",
    "href": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#background",
    "title": "Healthcare Spending Analysis",
    "section": "Background",
    "text": "Background\nThe dataset utilized for this report is titled “US Healthcare Spending Per Capita” and was obtained from Kaggle. The dataset’s format posed a challenge, as it followed a wide format with numerous columns and few rows. Notably, the years were presented in the format “Y####,” initially impeding analysis. However, by employing pivoting techniques and manipulating the strings, the dataset was transformed, enabling comprehensive analysis. The subsequent section outlines the complete step-by-step process."
  },
  {
    "objectID": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#methodology",
    "href": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#methodology",
    "title": "Healthcare Spending Analysis",
    "section": "Methodology",
    "text": "Methodology\nTo begin, it is essential to assess whether the data is in a “wide” or “long” format. This involves examining the number of rows and columns to facilitate necessary data wrangling.\n\n\n\n\n\n\nVersion Control\n\n\n\nMake sure to use RStudio’s version 2023.12.1 or higher\n\n\n\n\n# A tibble: 5 × 42\n   Code Item     Group Region_Number Region_Name State_Name  Y1980  Y1981  Y1982\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 Persona… Unit…             0 United Sta… &lt;NA&gt;       216977 251789 283073\n2     1 Persona… Regi…             1 New England &lt;NA&gt;        12960  14845  16759\n3     1 Persona… Regi…             2 Mideast     &lt;NA&gt;        43479  49604  55406\n4     1 Persona… Regi…             3 Great Lakes &lt;NA&gt;        40658  46668  51440\n5     1 Persona… Regi…             4 Plains      &lt;NA&gt;        16980  19682  21919\n# ℹ 33 more variables: Y1983 &lt;dbl&gt;, Y1984 &lt;dbl&gt;, Y1985 &lt;dbl&gt;, Y1986 &lt;dbl&gt;,\n#   Y1987 &lt;dbl&gt;, Y1988 &lt;dbl&gt;, Y1989 &lt;dbl&gt;, Y1990 &lt;dbl&gt;, Y1991 &lt;dbl&gt;,\n#   Y1992 &lt;dbl&gt;, Y1993 &lt;dbl&gt;, Y1994 &lt;dbl&gt;, Y1995 &lt;dbl&gt;, Y1996 &lt;dbl&gt;,\n#   Y1997 &lt;dbl&gt;, Y1998 &lt;dbl&gt;, Y1999 &lt;dbl&gt;, Y2000 &lt;dbl&gt;, Y2001 &lt;dbl&gt;,\n#   Y2002 &lt;dbl&gt;, Y2003 &lt;dbl&gt;, Y2004 &lt;dbl&gt;, Y2005 &lt;dbl&gt;, Y2006 &lt;dbl&gt;,\n#   Y2007 &lt;dbl&gt;, Y2008 &lt;dbl&gt;, Y2009 &lt;dbl&gt;, Y2010 &lt;dbl&gt;, Y2011 &lt;dbl&gt;,\n#   Y2012 &lt;dbl&gt;, Y2013 &lt;dbl&gt;, Y2014 &lt;dbl&gt;, …\n\n\n [1] \"Code\"                          \"Item\"                         \n [3] \"Group\"                         \"Region_Number\"                \n [5] \"Region_Name\"                   \"State_Name\"                   \n [7] \"Y1980\"                         \"Y1981\"                        \n [9] \"Y1982\"                         \"Y1983\"                        \n[11] \"Y1984\"                         \"Y1985\"                        \n[13] \"Y1986\"                         \"Y1987\"                        \n[15] \"Y1988\"                         \"Y1989\"                        \n[17] \"Y1990\"                         \"Y1991\"                        \n[19] \"Y1992\"                         \"Y1993\"                        \n[21] \"Y1994\"                         \"Y1995\"                        \n[23] \"Y1996\"                         \"Y1997\"                        \n[25] \"Y1998\"                         \"Y1999\"                        \n[27] \"Y2000\"                         \"Y2001\"                        \n[29] \"Y2002\"                         \"Y2003\"                        \n[31] \"Y2004\"                         \"Y2005\"                        \n[33] \"Y2006\"                         \"Y2007\"                        \n[35] \"Y2008\"                         \"Y2009\"                        \n[37] \"Y2010\"                         \"Y2011\"                        \n[39] \"Y2012\"                         \"Y2013\"                        \n[41] \"Y2014\"                         \"Average_Annual_Percent_Growth\"\n\n\nEarlier, we noticed that the dataset had a wide format, which means the years were in separate columns. To make it easier to analyze, we rearranged the data using a special technique. We combined the year columns into a single “Year” column and placed their corresponding values in a new column called “Cost.”\nWe also made some adjustments to the “Year” column by removing a specific symbol and converting it to numbers. This way, we can work with the years as numeric values instead of text.\nAdditionally, we transformed certain columns into categories, which help us group and analyze the data more effectively. These categories include “Item,” “Region_Name,” “Group,” and “State_Name.”\nFinally, we selected specific columns, including “Item,” “Region_Name,” “State_Name,” “Year,” and “Cost,” to focus on for further analysis. This will provide us with a clearer understanding of the data.\n\n\n# A tibble: 6 × 5\n  Item                 Region_Name   State_Name  Year   Cost\n  &lt;fct&gt;                &lt;fct&gt;         &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Personal Health Care United States &lt;NA&gt;        1980 216977\n2 Personal Health Care United States &lt;NA&gt;        1981 251789\n3 Personal Health Care United States &lt;NA&gt;        1982 283073\n4 Personal Health Care United States &lt;NA&gt;        1983 311677\n5 Personal Health Care United States &lt;NA&gt;        1984 341645\n6 Personal Health Care United States &lt;NA&gt;        1985 376376"
  },
  {
    "objectID": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#rising-health-care-costs",
    "href": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#rising-health-care-costs",
    "title": "Healthcare Spending Analysis",
    "section": "Rising Health Care Costs",
    "text": "Rising Health Care Costs\nLet’s jump right into the first visualization. It’s evident that healthcare spending has been consistently increasing and shows no signs of slowing down. This graph focuses on the years 1980 to 2005, highlighting the era of escalating healthcare costs.\n\n\n\n\n\n\n\n\n\n\nDominant Spending Categories: Personal, Hospital, and Physician & Clinical Care\nWow! Personal health care expenses skyrocketed from around $10K to nearly $80K within a relatively short period. Both Hospital and Clinical Care play significant roles in healthcare spending. Surprisingly, all three categories follow a similar upward trend, which reveals some unsettling information.\n\n\n\n\n\n\n\n\n\n\n\nConsistent Trends Across Regions\nIt’s disheartening to report that every region has been experiencing relentless growth in healthcare spending. The trend lines for each region are strikingly similar and proportionate to one another. Notably, the Mideast stands out as the most expensive region, while the Rocky Mountains region appears to have comparatively lower healthcare costs. It’s important to note that this analysis considers spending up to 2005, and we can hope for potential changes by 2014.\n\n\n\n\n\n\n\n\n\n\n\nPersistent Trends: Rising Healthcare Spending Across U.S. Regions\nThe bar chart vividly demonstrates the ongoing trends in healthcare spending across different regions. Notably, the Plains, New England, and the Rocky Mountains regions emerge as some of the lowest in terms of medical funding. Surprisingly, their costs can be as low as one-third compared to the most expensive regions. This stark contrast highlights the significant disparities in healthcare expenditure throughout the United States.\n\n\n\n\n\n\n\n\n\n\n\nFar West: A Surprising 3rd Place in Healthcare Spending\nIn 2014, the Far West region experienced a significant surge in healthcare spending, landing them in the 3rd position. This unexpected leap challenges the assumption that states within this region are heavy spenders. However, the subsequent graphic reveals an intriguing revelation that contradicts this perception.\n\n\n\n\n\n\n\n\n\n[1] \"$21.81M\"\n\n\n\n\nOregon: 3rd Place, but Don’t Be Deceived!\nSurprisingly, Oregon ranks 3rd in healthcare spending. However, let’s not overlook the undeniable fact that California claims the top spot. The massive population size of California is a significant contributing factor to its high expenditure. Although this report doesn’t delve into the specific reasons, it’s plausible that further analysis would align the Far West region more closely with the spending patterns observed in the Plains or New England regions.\n\n\n\n\n\n\n\n\n\n[1] \"$1.41M\"  \"$53.64M\" \"$1.9M\"   \"$3.41M\"  \"$5.81M\"  \"$10.16M\"\n[1] \"$5.81M\"\n\n\n\n\nInappropriate Model: Linear Fit Inadequate for the Data\nAt first glance, the model may seem impressive with an adjusted R-squared value of 0.8572. However, this is deceptive. It’s crucial to note that this model is highly inaccurate and strongly discouraged. The analysis reveals no correlation between Cost and Region_Name per Year, a finding consistent with the filtered dataset covering the years 1980 to 2014.\nThe residual plots provide clear evidence against a linear fit. The Residuals vs Fitted plot indicates a clear quadratic relationship rather than a linear one. The Q-Q plot deviates from linearity, exhibiting multiple curves along the fitted line. Additionally, the scale-location plot highlights that this model is fundamentally unsuitable for the data.\nIt is evident that a linear fit is not the appropriate choice for accurately modeling this dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      1 \n11072.4 \n\n\n\nCall:\nlm(formula = Cost ~ Region_Name + Year, data = regionHealthCareSince2005)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2683.8  -782.9  -279.8   597.7  4139.3 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                -680650.41   24479.29 -27.805  &lt; 2e-16 ***\nRegion_NameGreat Lakes        1438.65     368.55   3.904 0.000130 ***\nRegion_NameMideast            1657.50     368.55   4.497 1.17e-05 ***\nRegion_NameNew England       -3909.68     368.55 -10.608  &lt; 2e-16 ***\nRegion_NamePlains            -3830.75     368.55 -10.394  &lt; 2e-16 ***\nRegion_NameRocky Mountains   -5106.26     368.55 -13.855  &lt; 2e-16 ***\nRegion_NameSoutheast         -1231.18     368.55  -3.341 0.000998 ***\nRegion_NameSouthwest          -849.93     368.55  -2.306 0.022133 *  \nYear                           344.83      12.29  28.069  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1329 on 199 degrees of freedom\nMultiple R-squared:   0.88, Adjusted R-squared:  0.8752 \nF-statistic: 182.4 on 8 and 199 DF,  p-value: &lt; 2.2e-16\n\n\n             Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \nRegion_Name   7 1.185e+09 1.693e+08    95.9 &lt;2e-16 ***\nYear          1 1.391e+09 1.391e+09   787.9 &lt;2e-16 ***\nResiduals   199 3.514e+08 1.766e+06                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nSignificant Differences in Means among Regions\nMy objective was to investigate whether there were significant differences in the means of each region’s spending over the years. To start, I utilized the LeveneTest to examine the importance of variance (i.e., the spread of spending) across regions. Both tests yielded remarkably small p-values, indicating that three regions had substantially different variances compared to the others.\nBuilding on this, I employed the TukeyHsd test to confirm if these differing variances were reflected in the means. As anticipated from the LeveneTest results, the means of these regions indeed exhibited significant differences. Notably, New England, Plains, and Rocky Mountains had considerably lower average spending. However, despite their comparatively lower spending, these regions still followed the overall growth trend, with an increase of 18% since 2005.\n\n\n[1] \"The Average Spending In The Expensive Regions since 2005 = $5333.29\"\n\n\n[1] \"The Average Spending In The Expensive Regions since 2014= $7724.45\"\n\n\n[1] \"Difference: +$2391.16 | Percentage Increase: +18.31%\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2005 = $2173.22\"\n\n\n[1] \"The Average Spending In The Cheap Regions since 2014= $3142.21\"\n\n\n[1] \"Difference: +$968.99 | Percentage Increase: 18.23%\"\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(&gt;F)    \ngroup   7   12.03 8.727e-13 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   7  11.462 3.292e-12 ***\n      200                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value    Pr(&gt;F)    \ngroup   7  19.546 &lt; 2.2e-16 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   7  12.828 3.075e-14 ***\n      272                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n             Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \nRegion_Name   7 1.185e+09 169337440   19.43 &lt;2e-16 ***\nResiduals   200 1.743e+09   8712933                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Cost ~ Region_Name, data = regionHealthCareSince2005)\n\n$Region_Name\n                                   diff         lwr           upr     p adj\nGreat Lakes-Far West         1438.64777 -1069.19906  3946.4945990 0.6495584\nMideast-Far West             1657.50100  -850.34583  4165.3478291 0.4679930\nNew England-Far West        -3909.68132 -6417.52815 -1401.8344885 0.0000930\nPlains-Far West             -3830.75287 -6338.59970 -1322.9060420 0.0001417\nRocky Mountains-Far West    -5106.25783 -7614.10466 -2598.4109954 0.0000001\nSoutheast-Far West          -1231.18113 -3739.02796  1276.6657036 0.8046614\nSouthwest-Far West           -849.92577 -3357.77260  1657.9210559 0.9679983\nMideast-Great Lakes           218.85323 -2288.99360  2726.7000602 0.9999950\nNew England-Great Lakes     -5348.32909 -7856.17592 -2840.4822574 0.0000000\nPlains-Great Lakes          -5269.40064 -7777.24747 -2761.5538109 0.0000000\nRocky Mountains-Great Lakes -6544.90559 -9052.75242 -4037.0587643 0.0000000\nSoutheast-Great Lakes       -2669.82890 -5177.67573  -161.9820653 0.0279871\nSouthwest-Great Lakes       -2288.57354 -4796.42037   219.2732870 0.1019616\nNew England-Mideast         -5567.18232 -8075.02915 -3059.3354875 0.0000000\nPlains-Mideast              -5488.25387 -7996.10070 -2980.4070410 0.0000000\nRocky Mountains-Mideast     -6763.75882 -9271.60565 -4255.9119944 0.0000000\nSoutheast-Mideast           -2888.68213 -5396.52896  -380.8352954 0.0119419\nSouthwest-Mideast           -2507.42677 -5015.27360     0.4200569 0.0500724\nPlains-New England             78.92845 -2428.91838  2586.7752767 1.0000000\nRocky Mountains-New England -1196.57651 -3704.42334  1311.2703233 0.8267302\nSoutheast-New England        2678.50019   170.65336  5186.3470223 0.0270977\nSouthwest-New England        3059.75554   551.90871  5567.6023746 0.0058329\nRocky Mountains-Plains      -1275.50495 -3783.35178  1232.3418768 0.7745369\nSoutheast-Plains             2599.57175    91.72492  5107.4185757 0.0361922\nSouthwest-Plains             2980.82710   472.98027  5488.6739280 0.0081616\nSoutheast-Rocky Mountains    3875.07670  1367.22987  6382.9235291 0.0001119\nSouthwest-Rocky Mountains    4256.33205  1748.48522  6764.1788814 0.0000135\nSouthwest-Southeast           381.25535 -2126.59148  2889.1021825 0.9997829"
  },
  {
    "objectID": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#results",
    "href": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#results",
    "title": "Healthcare Spending Analysis",
    "section": "Results",
    "text": "Results\nThe cost of healthcare in the United States has increased more than fivefold between 1980 and 2014. Across regions, there is a consistent upward trend in healthcare spending with no clear indications of a decrease. Although some regions are less expensive than others, their growth rates align with the national average. Personal health care spending, which averaged around $10,000 in 1980, has significantly risen to nearly $80,000 in 2014.\nAs of 2014, the Mideast, Great Lakes, and Far West regions rank as the top three most expensive regions, while the Rocky Mountains, New England, and Plains regions are the least expensive.\nWithin the Far West region, Oregon stands out as the third most expensive state."
  },
  {
    "objectID": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#conclusion",
    "href": "content/projects/united-states-healthcare-spending-statistical-analysis/index.html#conclusion",
    "title": "Healthcare Spending Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThe United States continues to experience escalating healthcare expenditures, raising concerns about the affordability of personal health care. The substantial increase of approximately $70,000 over a span of 35 years far exceeds inflation expectations. It would have been beneficial to have inflation-adjusted values in the dataset, allowing for a more comprehensive analysis and deeper insights.\nFurther exploration can be done to investigate potential statistical significance between individual states and their spending patterns. This avenue remains open for future researchers to delve into for a more in-depth understanding of healthcare expenditure variations."
  },
  {
    "objectID": "content/projects/yelp-business-reviews-shiny-app/index.html",
    "href": "content/projects/yelp-business-reviews-shiny-app/index.html",
    "title": "Yelp Reviews Spatial Analysis",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I developed an interactive Shiny dashboard for spatial analysis and visualization of Yelp business reviews, enabling users to explore geographic patterns, star ratings, and business attributes through interactive mapping and dynamic plots. The dashboard incorporates advanced geostatistical analysis features, including variogram modeling, allowing users to assess spatial autocorrelation effects on Yelp star ratings through customizable parameters such as Partial Sill, Range, Nugget, and Kappa. This interactive tool leverages spatial analytics and visualization to reveal hidden patterns and provide insightful spatial interpretations of Yelp review data.\n\n\n\nWelcome to the Yelp Review Dashboard – an interactive Shiny application designed to help you explore and analyze Yelp business reviews and spatial patterns. This dashboard allows you to:\n\nVisualize Spatial Patterns: Navigate an interactive map that displays Yelp business locations filtered by city and star ratings. Markers reveal essential business information, while clusters provide an aggregated view that you can zoom in on.\nExplore Data Visualizations: Switch between multiple interactive plots to examine relationships between credit card acceptance, open hours, and star ratings. Use dynamic filters to refine your view and uncover hidden trends.\nEngage in Spatial Modeling: Dive into geostatistical analysis with variogram modeling. Adjust parameters like Partial Sill, Range, Nugget, and Kappa to see how spatial autocorrelation influences Yelp star ratings.\n\nExplore the Dashboard! Access the full application here: Yelp Review Dashboard\nFeel free to interact with the filters and explore each visualization. The dashboard is designed with user-friendly controls and clear, informative visualizations to help you gain insights into Yelp review data. Enjoy exploring the spatial dynamics and trends across Yelp businesses!"
  },
  {
    "objectID": "content/talks/building-professional-portfolio-via-quarto/index.html",
    "href": "content/talks/building-professional-portfolio-via-quarto/index.html",
    "title": "Building a Professional Portfolio with Quarto and GitHub Pages",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I built a professional portfolio using Quarto and GitHub Pages to effectively showcase my data science and analytics projects, enhance professional branding, and increase visibility through interactive online presentations. I established a structured workflow, implemented best practices for version control using Git, and created an engaging, customizable website featuring clear sections for projects, blogs, and talks. This approach not only improved immediate visibility and recruiter engagement but also provided long-term career advantages through demonstrated expertise in HTML, CSS, JavaScript, and modern web development frameworks."
  },
  {
    "objectID": "content/talks/loan-application-statistical-analysis/index.html",
    "href": "content/talks/loan-application-statistical-analysis/index.html",
    "title": "Loan Approval Logistic Regression Analysis",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I conducted logistic regression analysis and ANOVA tests on 32,581 simulated loan applications to determine the key demographic and financial predictors influencing loan approvals. My logistic regression model achieved strong interpretability, revealing that higher income and renting significantly increased approval odds, while higher credit scores unexpectedly decreased approval likelihood; notably, multicollinearity was minimal with all VIF values below 5. ANOVA results highlighted statistically significant differences in loan interest rates across loan intents and loan-to-income percentages across home ownership statuses, providing actionable insights for refining lending practices."
  },
  {
    "objectID": "content/talks/teaching-with-webr/index.html",
    "href": "content/talks/teaching-with-webr/index.html",
    "title": "Interactive Teaching with webR",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT 4.5    \n  \n  I introduced Quarto, highlighting its capabilities for creating dynamic presentations, interactive Shiny applications, and professional portfolio websites using multiple languages like R, Python, Julia, and ObservableJS. I demonstrated practical uses, including generating simulated datasets, implementing cross-language coding examples with tabsets, and deploying interactive visualizations and Shiny apps directly within Quarto. Additionally, I emphasized mastering GitHub, RMarkdown, and Shiny as foundational skills before fully leveraging Quarto’s extensive features."
  },
  {
    "objectID": "content/talks/water-parasites-survival-analysis/index.html",
    "href": "content/talks/water-parasites-survival-analysis/index.html",
    "title": "Investigating the Impact of Temperature on Pesticide Toxicity: A Survival Analysis Approach",
    "section": "",
    "text": "Yapper Labs | AI Summary\n    \n      \n      Model: ChatGPT o1    \n  \n  A study review examined 1,076 cercariae tadpoles exposed to three pesticides (Atrazine, Azoxystrobin, and Carbaryl) under varying temperature conditions to explore whether higher temperatures influence toxicity. The analysis focused on time to knockdown (TTKD) and observed status, revealing that elevated water temperatures markedly increased the pesticides’ toxic effects. These findings highlight the need to consider multiple temperature settings in pesticide toxicity studies, reflecting real-world environmental scenarios."
  }
]