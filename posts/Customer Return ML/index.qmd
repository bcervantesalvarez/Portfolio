---
title: "Predicting Customer Returns"
author: Brian Cervantes Alvarez
date: "3-11-2023"
image: return.jpeg
format:
  html:
    toc: true
    toc-location: right
    html-math-method: katex
    page-layout: full
execute: 
  warning: false
  message: false
categories: [R, machine learning, logistical model, data visualization]
---

![](return.jpeg)

## Approach and Methodology

To ensure proper analysis, I began by loading the necessary libraries and examining the train and test datasets to identify the variables available for analysis. Exploration of the data revealed trends in the number of returns across various variables such as ProductDepartment, ProductSize, and Returns Per State. Given the project objective of predicting the probability of item returns, I selected logistic regression, a well-suited model for binary dependent variables. To prepare the data, I developed a function to transform both the train and test data, ensuring dimensionality was kept in check by removing unnecessary columns such as dates and IDs. Additionally, I updated character data types to factors. Further exploration of the data led to the inclusion of additional features, such as "Season", "CustomerAge", "MSRP", and "PriceRange", that may play a significant role in predicting a customer's probability of returning their product. I then ran the logistic model and examined the coefficients, noting the highest ratios for ProductDepartment, with a significant influence from men's and women's products. Noting that for future exploration, I concluded my model and wrote the submssion file.

Note: I had explored the idea of using rpart, rf and gbm, however I was not accustomed to using those models. There was much more refinement to be done, but the 3 hour constraint kept me focused on building a draft of the model. Would this go into production? No, but it would be a step in the right direction.

## Load the required packages

```{r}
library(tidyverse)
library(lubridate)
library(caret)
library(glmnet)
```

## Load the training and test data

```{r}
train <- read_csv("train.csv") 
test <- read_csv("test.csv") 

glimpse(train)
```

## Data Exploration

```{r}
# Look at Product Department
train %>% 
  filter(Returned == 1) %>%
  ggplot(aes(x = ProductDepartment)) +
  geom_bar(fill = "#e67838") +
  labs(title = "Number of Returns Per Department") + 
  theme_minimal()


# Look at Product Size
train %>% 
  filter(Returned == 1) %>%
  ggplot(aes(x = ProductSize)) +
  geom_bar(fill = "#e67838") +
  labs(title = "Number of Returns Per Product Size") + 
  theme_minimal()


#This won't be that valuable
train %>% 
  mutate(CustomerState = factor(CustomerState)) %>%
  filter(Returned == 1) %>%
  ggplot(aes(x = CustomerState)) +
  coord_flip() +
  geom_bar(fill = "#e67838") +
  labs(title = "Number of Returns Per State") + 
  theme_minimal()


```

## Feature Engineering

```{r}
# Creates the features
buildFeatures <- function(ds){
  CurrentDate <- Sys.Date()
  ds %>%
  mutate(Returned = factor(Returned, levels = c(0, 1), labels = c("No","Yes")),
         Season = factor(case_when(months(OrderDate) %in% month.name[1:3] ~ "Winter",
                            months(OrderDate) %in% month.name[4:6] ~ "Spring",
                            months(OrderDate) %in% month.name[7:9] ~ "Summer",
                            months(OrderDate) %in% month.name[10:12] ~ "Fall")), 
         CustomerAge = year(as.period(interval(CustomerBirthDate,CurrentDate))),
         MSRP = round(PurchasePrice / (1 - DiscountPct)),
         PriceRange = factor(case_when(MSRP >= 13 & MSRP <= 30 ~ "$13-$30",
                             MSRP > 30 & MSRP <= 60 ~ "$31-$60",
                             MSRP > 60 & MSRP <= 100 ~ "$61-$100",
                             MSRP > 100 ~ ">$100")),
         ProductDepartment = as.factor(ProductDepartment),
         ProductSize = as.factor(ProductSize),
         CustomerState = as.factor(CustomerState)
  ) %>%
  select(-OrderDate, 
         -CustomerBirthDate, 
         -ID, 
         -OrderID, 
         -CustomerID)
}

IDCols <- test$ID

#Removes and adds columns for train and test sets
train <- buildFeatures(train)
test <- buildFeatures(test)


#Inspect the dataset before training the model
glimpse(train)
summary(train)
table(train$Returned)

```

## Fit a Logistical Regression Model

```{r}
set.seed(345)

#Model using Logistical Regression
logModel <- glm(Returned ~ .,
                data = train,
                family = "binomial")
summary(logModel)

odds_ratio <- exp(logModel$coefficients)
data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  
  arrange(desc(odds_ratio)) %>% 
  head()

```

## Make prediction on the test data

```{r}
testPredictions <- predict(logModel, newdata = test, type = "response")
```

## Writing the Submission File

```{r}
submission <- data.frame(ID = IDCols, Prediction = testPredictions)
write.csv(submission, "submission.csv", row.names = FALSE)
```

## Leftout Features that were considered

```{r}

#odds_ratio <- exp(coef(fit$finalModel))
#data.frame(name = names(odds_ratio), odds_ratio = odds_ratio) %>%  
#  arrange(desc(odds_ratio)) %>% 
#  head()

#Sampling
#train_index <- createDataPartition(returns_train$Returned, times = 1, p = 0.7, list = FALSE)
#train <- returns_train[train_index, ]
#test <- returns_train[-train_index, ]


#Returned = factor(Returned, levels = c(0, 1), labels = c("No","Yes"))
#AgeGroup = factor(case_when(CustomerAge >= 18 & CustomerAge <= 30 ~ "18-30",
#                              CustomerAge > 30 & CustomerAge <= 45 ~ "31-45",
#                              CustomerAge > 45 & CustomerAge <= 60 ~ "46-60",
#                              CustomerAge > 60 ~ ">61"),
#                           levels = c("18-30", "31-45", "46-60", ">61"))


#select(-OrderDate, 
#         -CustomerBirthDate, 
#         -ID, 
#         -OrderID, 
#         -CustomerID,
#         -PurchasePrice,
#         -DiscountPct,
#         -MSRP,
#         -ProductCost,
#         -CustomerState)
```
