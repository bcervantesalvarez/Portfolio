---
title: "Predicting $50K+ Incomes"
description: "Expertly predicting $50K+ incomes through ML, our project highlights the synergy of data science skills and team collaboration."
author:  
  - "Brian Cervantes Alvarez"
  - "Willa Van Liew"
date: "04-11-2023"
image: /Assets/Images/income.jpeg
bibliography: "bibliography.bib"
nocite: |
     @*
format:
  html:
    page-layout: article
    toc: true
    toc-location: right
    html-math-method: katex
output: html_document
execute:
  message: false
  warning: false
categories: [R, Machine Learning, Gradient Boosting Machines, Logistic Regression, Classification]
---

![](/Assets/Images/income.jpeg)

## Project Overview

In this project, our goal was to predict whether individuals earn more than $50K using machine learning techniques. We engaged in a thorough data science process, from data preprocessing and feature engineering to selecting principal components through PCA and optimizing hyperparameters for our model. Our collaborative efforts led to a high-performing model with a Kappa score of 0.9543, enhancing our data science expertise and setting the stage for future projects.

## Team Contribution

Willa played a pivotal role in developing the foundational models and pinpointing the key PCA features. Her outstanding efforts laid the groundwork for our high-quality model that accurately predicts incomes over $50,000. Willa's expertise not only provided a solid base for our project but also enabled us to fine-tune our models for peak performance. I'm thankful for her invaluable input and dedication, which significantly contributed to the success of our work.

Explore more of Willa's data science work on her [website](https://willavanliew.netlify.app/).

## Predicting Income \>50K

### Load Libraries

```{r setup, message=FALSE, warning=FALSE}
#knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(caret)
library(tidymodels)
library(fastDummies)
library(randomForest)
```

### Load The Data

```{r}
income = read_csv("../../../Assets/Datasets/openml_1590.csv", na = c("?")) %>%
  drop_na() %>%
  mutate(income_above_50K = ifelse(class == ">50K",1,0)) %>%
  select(-class) %>%
  dummy_cols(remove_selected_columns = T)

```

## Run Random Forest & Obtain Importance Features

```{r}
set.seed(504)
raw_index <- createDataPartition(income$income_above_50K, p = 0.8, list = FALSE)
train <- income[raw_index,]
test  <- income[-raw_index, ]
ctrl <- trainControl(method = "cv", number = 3)

fit <- train(income_above_50K ~ .,
            data = train, 
            method = "rf",
            ntree = 50,
            tuneLength = 3,
            trControl = ctrl,
            metric = "kappa")
fit

print(varImp(fit), 10)
```

## PCA

### Chose Top 8 Features

```{r}
inc <- income %>%
  select(-c(fnlwgt,
            `marital-status_Married-civ-spouse`,
            age,
            `capital-gain`,
            `education-num`,
            `hours-per-week`,
            relationship_Husband,
            `capital-loss`))

#Remained unchanged
pr_income = prcomp(x = inc, scale=T, center = T)
screeplot(pr_income, type="lines")

rownames_to_column(as.data.frame(pr_income$rotation)) %>%
  select(1:11) %>%
  filter(abs(PC1) >= 0.35 | abs(PC2) >= 0.35 | abs(PC3) >= 0.35 | abs(PC4) >= 0.35 | abs(PC5) >= 0.35 | abs(PC6) >= 0.35 | abs(PC7) >= 0.35 | abs(PC8) >= 0.35 | abs(PC9) >= 0.35 | abs(PC10) >= 0.35)
```

### Chose First 10 PCA Features

```{r}
# IMPORTANT: Since I used 8 features, I updated the prc dataframe to include
# the features + PCA 1-10
prc <- 
  bind_cols(select(income, 
                   c(fnlwgt, 
                    `marital-status_Married-civ-spouse`, 
                    age, 
                    `capital-gain`, 
                    age, 
                    `hours-per-week`, 
                    relationship_Husband,
                    `capital-loss`,
                    income_above_50K)
                   ), 
            as.data.frame(pr_income$x)
            ) %>%
  select(1:18) %>%
  ungroup() %>%
  rename("NonBlack_Men" = PC1,
         "US_Women" = PC2,
         "PrivateSec_Men" = PC3,
         "NonUS_NonBlack" = PC4,
         "NonPrivateSec_Black" = PC5,
         "PrivateSec" = PC6,
         "NonBlack_SelfEmploy" = PC7,
         "Wives" = PC8,
         "NonFamily_SomeCollege" = PC9,
         "NotSelfEmployes_NonBlack" = PC10)

head(prc)
```

## Gradient Boosting Machine

```{r}

#IMPORTANT: I took a while and messed around with the hyperparameters
# Went From 0.2 Kappa to 0.6 Kappa BEFORE updating the features.
# After updating to the top 8 features + PCA 1-5, it jumped to 
# 0.88 Kappa. Then I added PCA 1-10 and it jumped to 0.95 for the Kappa!
set.seed(504)
raw_index <- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)
train <- prc[raw_index,]
test  <- prc[-raw_index, ]
ctrl <- trainControl(method = "cv", number = 5)
weights <- ifelse(income$income_above_50K == 1, 75, 25)

hyperparameters <- expand.grid(interaction.depth = 9, 
                    n.trees = 300, 
                    shrinkage = 0.1, 
                    n.minobsinnode = 4)
fit <- train(factor(income_above_50K) ~ .,
            data = train, 
            method = "gbm",
            verbose = FALSE,
            tuneGrid = hyperparameters,
            trControl = ctrl,
            metric = "kappa")
fit

```

### Confusion Matrix For GBM

```{r}
confusionMatrix(predict(fit, test), factor(test$income_above_50K))
```

## Logistical Model

```{r}
#I messed around with using a logistical model
#It turns out that it's pretty good too! Not as great as the GBM
#But a great and easy model to explain!

set.seed(504)
raw_index <- createDataPartition(prc$income_above_50K, p = 0.8, list = FALSE)
train <- prc[raw_index,]
test  <- prc[-raw_index, ]
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, verboseIter = FALSE)
hyperparameters <- expand.grid(alpha = 1, 
                               lambda = 0.001)

fit <- train(factor(income_above_50K)  ~ .,
            data = train, 
            method = "glmnet",
            family = "binomial",
            tuneGrid = hyperparameters,
            trControl = ctrl,
            metric = "kappa",
            importance = TRUE)
fit
```

### Confusion Matrix For Logistical Regression

```{r}
confusionMatrix(predict(fit, test), factor(test$income_above_50K))
```

## KMeans Clustering

```{r}
kclust <- kmeans(na.omit(prc), centers = 4)
kclust$centers

kclusts <- tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(prc, .x)),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, prc)
  )

clusterings <- kclusts %>%
  unnest(glanced, .drop = TRUE)

ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line()
```

## Augumenting The GBM Model with KMeans Clustering

```{r}
prc2 <- augment(kclust, prc)

set.seed(504)
raw_index <- createDataPartition(prc2$income_above_50K, p = 0.8, list = FALSE)

train <- prc2[raw_index,]
test  <- prc2[-raw_index, ]
ctrl <- trainControl(method = "cv", number = 5)

hyperparameters <- expand.grid(
  n.trees = 500,
  interaction.depth = 5,
  shrinkage = 0.1,
  n.minobsinnode = 10
)



fit <- train(factor(income_above_50K)  ~ .,
            data = train, 
            method = "gbm",
            trControl = ctrl,
            tuneGrid = hyperparameters,
            verbose = FALSE)
fit


```

### Confusion Matrix For KMeans + GBM

```{r}
#We should be getting a Kappa of 0.9543!
#Sensitivity = 0.9930, Specificity = 0.9533
#Excellent Numbers!
confusionMatrix(predict(fit, test), factor(test$income_above_50K))
```

## Results

Our analysis began with a random forest model to pinpoint critical factors, focusing on the top 8 features for in-depth examination. We then applied principal component analysis for further insight. The culmination of our work was a gradient boosting machine model, finely tuned for peak performance, which boasted a remarkable accuracy rate of 98.3% and a Kappa score of 95.4%. To validate our model's reliability and guard against overfitting, we compared it with a basic logistic regression model, which showed a Kappa score of 89.5% and an accuracy of 96.1%. Enhancements were made by integrating Kmeans clustering, an unsupervised learning technique, pushing the Kappa score slightly higher to 95.4% and accuracy to 98.3%. Our methodology, combining feature selection and PCA, proved highly effective, offering a solid foundation for future projects with potential for even finer adjustments.

## Data References

```{r, echo=FALSE, eval=TRUE}
knitr::write_bib(names(sessionInfo()$otherPkgs), file = "bibliography.bib")
```